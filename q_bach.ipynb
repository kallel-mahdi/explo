{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "\n",
    "import torch\n",
    "### gpytorch \n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "\n",
    "# logging.config.fileConfig('logging.conf')\n",
    "# logger = logging.getLogger(\"ShapeLog.\"+__name__)\n",
    "\n",
    "from gpytorch.settings import debug\n",
    "from copy import deepcopy\n",
    "\n",
    "debug._set_state(False) ##hotfix to allow input dim and ard_dim to have different dimensions\n",
    "\n",
    "class MyRBFKernel(ScaleKernel):\n",
    "        \n",
    "    \n",
    "    def __init__(self,ard_num_dims,use_ard,\n",
    "                lengthscale_constraint=None,\n",
    "                lengthscale_hyperprior=None,\n",
    "                outputscale_constraint=None,\n",
    "                outputscale_hyperprior=None,\n",
    "                mlp = None, ## unused\n",
    "                \n",
    "                ):\n",
    "        \n",
    "        if use_ard == False :\n",
    "            ard_num_dims = None\n",
    "            \n",
    "        rbf = RBFKernel(\n",
    "            ard_num_dims=ard_num_dims,\n",
    "            lengthscale_prior=lengthscale_hyperprior,\n",
    "            lengthscale_constraint=lengthscale_constraint,\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        ScaleKernel.__init__(self,base_kernel=rbf,\n",
    "            outputscale_prior=outputscale_hyperprior,\n",
    "            outputscale_constraint=outputscale_constraint,\n",
    "                                                            ) \n",
    "            \n",
    "        # Initialize lengthscale and outputscale to mean of priors.\n",
    "        if lengthscale_hyperprior is not None:\n",
    "            self.base_kernel.lengthscale = lengthscale_hyperprior.mean\n",
    "        if outputscale_hyperprior is not None:\n",
    "            self.outputscale = outputscale_hyperprior.mean  \n",
    "\n",
    "        \n",
    "        ## Used only for logging purpouses\n",
    "        self.states = None \n",
    "\n",
    "            \n",
    "        \n",
    "    def forward(self,x1,x2,**params):\n",
    "        \n",
    "        rslt = super().forward(x1,x2,**params)\n",
    "        \n",
    "        return rslt\n",
    "\n",
    "    \n",
    "        \n",
    "    def set_train_data(self,new_s):\n",
    "        \n",
    "        self.states = new_s\n",
    "    \n",
    "    def append_train_data(self,new_s):\n",
    "\n",
    "        pass\n",
    "        \n",
    "\n",
    "class StateKernel:\n",
    "    \n",
    "    \"\"\"Abstract class for a kernel that uses state action pairs metric\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,train_s,\n",
    "                **kernel_config\n",
    "                ):\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        use_ard : whether to give more weight to certain states.\n",
    "        ard_num_dims : in this kernel it's the number of states to take.\n",
    "        \n",
    "        \"\"\"\n",
    "      \n",
    "        self.kernel_config = kernel_config\n",
    "        self.set_train_data(train_s)\n",
    "        \n",
    "        ## intialised by set_train_data\n",
    "        \n",
    "        self.mlp = None \n",
    "        self.states = None\n",
    "        self.n_actions = None \n",
    "  \n",
    "\n",
    "    def build_kernel(self,**kwargs):\n",
    "        \n",
    "        raise NotImplementedError\n",
    "     \n",
    "    def forward(self,x1,x2,**params):\n",
    "            \n",
    "        raise NotImplementedError\n",
    " \n",
    "    def run_parameters(self,params_batch,states):\n",
    "        \n",
    "        actions = self.mlp(states,params_batch) ##[params_batch[:2],n_actions,n_states]    \n",
    "        \n",
    "        return actions\n",
    "    \n",
    "            \n",
    "class RBFStateKernel(MyRBFKernel,StateKernel):\n",
    "    \n",
    "        def __init__(self,**kwargs):\n",
    "            \n",
    "            StateKernel.__init__(self,**kwargs)\n",
    "    \n",
    "        def build_kernel(self,use_ard,\n",
    "                         mlp,train_s,train_ls=None,**kwargs):\n",
    "            \n",
    "    \n",
    "            ard_num_dims  = train_s.shape[0]\n",
    "            MyRBFKernel.__init__(self,ard_num_dims,use_ard,**kwargs)\n",
    "            \n",
    "            #self.base_kernel.lengthscale = 0.1 * torch.ones((ard_num_dims,),requires_grad=True)\n",
    "            # self.outputscale = torch.Tensor([0.1])\n",
    "            self.ard_num_dims = ard_num_dims\n",
    "            self.states = train_s\n",
    "            self.mlp = mlp\n",
    "        \n",
    "        def forward(self,x1,x2,**params):\n",
    "                \n",
    "            #Evaluate current parameters\n",
    "            a1 = self.run_parameters(x1,self.states)\n",
    "            a2 = self.run_parameters(x2,self.states) \n",
    "\n",
    "            n_actions,n_states = a1.shape[-2],a1.shape[-1]\n",
    "\n",
    "            # Compute pairwise kernel \n",
    "            norm = torch.sqrt(torch.Tensor([self.ard_num_dims]))\n",
    "            \n",
    "            kernel = super().forward(a1[:,0,:]/norm, a2[:,0,:]/norm, **params)\n",
    "\n",
    "            for i in range(1,n_actions):\n",
    "\n",
    "                kernel *= super().forward(a1[:,i,:]/norm, a2[:,i,:]/norm, **params)\n",
    "\n",
    "        \n",
    "            return kernel\n",
    "        \n",
    "\n",
    "        def set_train_data(self,train_s):\n",
    "\n",
    "            \"\"\" sometimes we need to reset the states used by the kernel\n",
    "            This usually requires re insantiating the base kernel (RBF or Linear ..) \"\"\"\n",
    "                \n",
    "            self.build_kernel(**self.kernel_config,train_s=train_s)\n",
    "        \n",
    "        def get_mini_kernel(self,batch_size):\n",
    "            \n",
    "            \"\"\"Used for acquisition function to minmize optimization overhead \"\"\"\n",
    "            \n",
    "            perm = torch.randperm(self.states.size(0))\n",
    "            idx = perm[:batch_size]\n",
    "            state_batch = self.states[idx]\n",
    "            ls_batch = self.base_kernel.lengthscale[0,idx]\n",
    "            \n",
    "            new_kernel = deepcopy(self)\n",
    "\n",
    "            new_kernel.build_kernel(**self.kernel_config,train_s=state_batch,train_ls=ls_batch)\n",
    "            \n",
    "            return new_kernel\n",
    "        \n",
    "       \n",
    "            \n",
    "        def append_train_data(self,new_s):\n",
    "            \n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MathLog.src.helpers : WARNING : MLP dimensions : [8, 2]\n",
      "MLPPPPPPPPP SETUP 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gpytorch\n",
    "from src.environments.gym_env import Gym\n",
    "from src.helpers import setup_policy\n",
    "import torch\n",
    "\n",
    "kernel_config = {\n",
    "                        \"use_ard\":True,\n",
    "                        \"lengthscale_hyperprior\":gpytorch.priors.torch_priors.UniformPrior(a=0.01,b=0.5),\n",
    "                        \"lengthscale_constraint\":gpytorch.constraints.constraints.Interval(0.01,0.5), ## constraints are loose to avoid crash\n",
    "                        \"outputscale_hyperprior\":gpytorch.priors.torch_priors.NormalPrior(loc=2.0,scale=1.0),\n",
    "                        \"outputscale_constraint\":gpytorch.constraints.constraints.GreaterThan(0.01),\n",
    "                }\n",
    "\n",
    "policy_config = {\n",
    "                \"add_layer\":[],### can be empty or [8,7] for adding 2 layers with width 8,7  neurons respectively\n",
    "                \"add_bias\":False, ### newwww\n",
    "        }\n",
    "\n",
    "env = Gym(\"Swimmer-v4\")\n",
    "states = torch.rand((10,3))\n",
    "mlp = setup_policy(env,policy_config)\n",
    "kernel_config[\"mlp\"]= mlp\n",
    "\n",
    "kernel = RBFStateKernel(**kernel_config,train_s=states)\n",
    "kernel.set_train_data(states)\n",
    "kernel.base_kernel.lengthscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "new_kernel = get_mini_kernel(kernel)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kernel.base_kernel.lengthscale.shape\n",
    "kernel.base_kernel.lengthscale.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boptim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
