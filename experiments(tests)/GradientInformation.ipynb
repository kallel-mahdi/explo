{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RBFKernel\n",
    "\n",
    "covar_module = RBFKernel()\n",
    "theta_t = torch.rand(1,2,requires_grad=False)\n",
    "X_hat = torch.rand(5,2,requires_grad=False)\n",
    "\n",
    "def get_KxX_dx( x, X) :\n",
    "    '''Computes the analytic derivative of the kernel K(x,X) w.r.t. x.\n",
    "\n",
    "    Args:\n",
    "        x: (n x D) Test points.\n",
    "\n",
    "    Returns:\n",
    "        (n x D) The derivative of K(x,X) w.r.t. x.\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    n = x.shape[0]\n",
    "    D = x.shape[-1]\n",
    "    \n",
    "    \n",
    "    K_xX = covar_module(x, X).evaluate()\n",
    "    lengthscale = covar_module.lengthscale.detach()\n",
    "    return (\n",
    "        -torch.eye(D, device=X.device)\n",
    "        / lengthscale**2\n",
    "        @ (\n",
    "            (x.view(n, 1, D) - X.view(1, N, D))\n",
    "            * K_xX.view(n, N, 1)\n",
    "        ).transpose(1, 2)\n",
    "    )\n",
    "\n",
    "def get_Kxx_dx2(x):\n",
    "        \"\"\"Computes the analytic second derivative of the kernel K(x,x) w.r.t. x.\n",
    "\n",
    "        Args:\n",
    "            x: (n x D) Test points.\n",
    "\n",
    "        Returns:\n",
    "            (n x D x D) The second derivative of K(x,x) w.r.t. x.\n",
    "        \"\"\"\n",
    "        \n",
    "        D = x.shape[-1]\n",
    "        lengthscale = covar_module.lengthscale.detach()\n",
    "        sigma_f = 1\n",
    "        return (\n",
    "            torch.eye(D, device=lengthscale.device) / lengthscale ** 2\n",
    "        ) * sigma_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_θX(theta_t,X_hat):\n",
    "    \n",
    "    rslt = covar_module(theta_t,X_hat).evaluate()\n",
    "    \n",
    "    return rslt\n",
    "\n",
    "def get_K_θX_dθ(theta_t,X_hat):\n",
    "        \n",
    "    jacobs = torch.autograd.functional.jacobian(func=lambda theta : K_θX(theta,X_hat),inputs=(theta_t))\n",
    "    K_θX_dθ = jacobs.sum(dim=2).transpose(1,2)\n",
    "    print(f'K_θX_dθ{K_θX_dθ.shape}')\n",
    "    \n",
    "    \n",
    "    return K_θX_dθ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_θX_dθtorch.Size([1, 2, 5])\n",
      "torch.Size([1, 2, 5]) torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "a = get_K_θX_dθ(theta_t,X_hat)\n",
    "b = get_KxX_dx(theta_t,X_hat)\n",
    "\n",
    "print(a.shape,b.shape)\n",
    "\n",
    "assert ( (a-b) < 1e-5).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_θX_dθtorch.Size([1, 2, 1])\n",
      "torch.Size([1, 2, 1]) torch.Size([1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = get_K_θX_dθ(theta_t,theta_t)\n",
    "b = get_KxX_dx(theta_t,theta_t)\n",
    "\n",
    "print(a.shape,b.shape)\n",
    "\n",
    "assert ( (a-b) < 1e-5).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_θX_dθtorch.Size([1, 2, 5])\n",
      "jacobs[0]torch.Size([2, 5, 1, 2])\n",
      "K_θθ_dθ2 torch.Size([5, 2, 2])\n",
      "torch.Size([5, 2, 2]) torch.Size([2, 2])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]])\n",
      "tensor([[2.0814, 0.0000],\n",
      "        [0.0000, 2.0814]])\n"
     ]
    }
   ],
   "source": [
    "def get_K_θX_dθ2(theta_t,X_hat):\n",
    "    \n",
    "    jacobs = torch.autograd.functional.jacobian(func= lambda theta_t: get_K_θX_dθ(theta_t,X_hat),inputs=(theta_t))\n",
    "    print(f'jacobs[0]{jacobs[0].shape}')\n",
    "    ### we must put it in the right shape\n",
    "    K_θθ_dθ2 = jacobs[0].sum(dim=2).transpose(1,0) \n",
    "    print(f'K_θθ_dθ2 {K_θθ_dθ2.shape}')\n",
    "    return K_θθ_dθ2\n",
    "\n",
    "a = get_K_θX_dθ2(theta_t,X_hat)\n",
    "b = get_Kxx_dx2(theta_t)\n",
    "\n",
    "\n",
    "print(a.shape,b.shape)\n",
    "\n",
    "print(a.squeeze())\n",
    "print(b.squeeze())\n",
    "\n",
    "#assert ( (a-b) < 1e-5).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0814,  0.0000],\n",
       "        [ 0.0000, -2.0814]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def last_hope(theta_x,X_hat):\n",
    "    \n",
    "    hessian = torch.autograd.functional.hessian(func=lambda theta : K_θX(theta,X_hat),inputs=(theta_t))\n",
    "    print(hessian.shape)\n",
    "    return hessian\n",
    "    \n",
    "\n",
    "hessian = last_hope(theta_t,theta_t)\n",
    "hessian.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0814]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/ covar_module.lengthscale**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], grad_fn=<RBFCovarianceBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar_module(theta_t,theta_t).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_K_θX_dθ(theta_t,theta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def K_θθ(theta_t):\n",
    "    \n",
    "#     rslt = covar_module(theta_t,theta_t).evaluate()\n",
    "    \n",
    "#     return rslt\n",
    "\n",
    "# def get_K_θθ_dθ(theta_t):\n",
    "    \n",
    "#     jacobs = torch.autograd.functional.jacobian(func=K_θθ,inputs=(theta_t))\n",
    "#     print(f'jacobs[0]{jacobs.shape}')\n",
    "#     ### we must put it in the right shape\n",
    "#     K_θθ_dθ = jacobs.sum(dim=2).transpose(1,2)\n",
    "#     print(f'K_θθ_dθ {K_θθ_dθ.shape}')\n",
    "    \n",
    "#     return K_θθ_dθ\n",
    "    \n",
    "# a = get_K_θX_dθ(theta_t,theta_t)\n",
    "# b = get_KxX_dx(theta_t,theta_t)\n",
    "# c = get_K_θθ_dθ(theta_t)\n",
    "\n",
    "# print(a.shape,b.shape)\n",
    "\n",
    "# assert ( (a-b) < 1e-5).all()\n",
    "# assert ( (c-b) < 1e-5).all()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
