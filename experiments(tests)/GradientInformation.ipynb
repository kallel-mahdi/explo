{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RBFKernel\n",
    "\n",
    "covar_module = RBFKernel()\n",
    "theta_t = torch.rand(1,2,requires_grad=False)\n",
    "X_hat = torch.rand(5,2,requires_grad=False)\n",
    "\n",
    "def get_KxX_dx( x, X) :\n",
    "    '''Computes the analytic derivative of the kernel K(x,X) w.r.t. x.\n",
    "\n",
    "    Args:\n",
    "        x: (n x D) Test points.\n",
    "\n",
    "    Returns:\n",
    "        (n x D) The derivative of K(x,X) w.r.t. x.\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    n = x.shape[0]\n",
    "    D = x.shape[-1]\n",
    "    \n",
    "    \n",
    "    K_xX = covar_module(x, X).evaluate()\n",
    "    lengthscale = covar_module.lengthscale.detach()\n",
    "    return (\n",
    "        -torch.eye(D, device=X.device)\n",
    "        / lengthscale**2\n",
    "        @ (\n",
    "            (x.view(n, 1, D) - X.view(1, N, D))\n",
    "            * K_xX.view(n, N, 1)\n",
    "        ).transpose(1, 2)\n",
    "    )\n",
    "\n",
    "def get_Kxx_dx2(x):\n",
    "        \"\"\"Computes the analytic second derivative of the kernel K(x,x) w.r.t. x.\n",
    "\n",
    "        Args:\n",
    "            x: (n x D) Test points.\n",
    "\n",
    "        Returns:\n",
    "            (n x D x D) The second derivative of K(x,x) w.r.t. x.\n",
    "        \"\"\"\n",
    "        \n",
    "        D = x.shape[-1]\n",
    "        lengthscale = covar_module.lengthscale.detach()\n",
    "        sigma_f = 1\n",
    "        return (\n",
    "            torch.eye(D, device=lengthscale.device) / lengthscale ** 2\n",
    "        ) * sigma_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_θX(theta_t,X_hat):\n",
    "    \n",
    "    rslt = covar_module(theta_t,X_hat).evaluate()\n",
    "    \n",
    "    return rslt\n",
    "\n",
    "def get_K_θX_dθ(theta_t,X_hat):\n",
    "        \n",
    "    jacobs = torch.autograd.functional.jacobian(func=lambda theta : K_θX(theta,X_hat),inputs=(theta_t))\n",
    "    K_θX_dθ = jacobs.sum(dim=2).transpose(1,2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return K_θX_dθ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_θX_dθtorch.Size([1, 2, 5])\n",
      "torch.Size([1, 2, 5]) torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "a = get_K_θX_dθ(theta_t,X_hat)\n",
    "b = get_KxX_dx(theta_t,X_hat)\n",
    "\n",
    "print(a.shape,b.shape)\n",
    "\n",
    "assert ( (a-b) < 1e-5).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_θX_dθtorch.Size([1, 2, 1])\n",
      "torch.Size([1, 2, 1]) torch.Size([1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = get_K_θX_dθ(theta_t,theta_t)\n",
    "b = get_KxX_dx(theta_t,theta_t)\n",
    "\n",
    "print(a.shape,b.shape)\n",
    "\n",
    "assert ( (a-b) < 1e-5).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_θX_dθtorch.Size([1, 2, 5])\n",
      "jacobs[0]torch.Size([2, 5, 1, 2])\n",
      "K_θθ_dθ2 torch.Size([5, 2, 2])\n",
      "torch.Size([5, 2, 2]) torch.Size([2, 2])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]])\n",
      "tensor([[2.0814, 0.0000],\n",
      "        [0.0000, 2.0814]])\n"
     ]
    }
   ],
   "source": [
    "def get_K_θX_dθ2(theta_t,X_hat):\n",
    "    \n",
    "    jacobs = torch.autograd.functional.jacobian(func= lambda theta_t: get_K_θX_dθ(theta_t,X_hat),inputs=(theta_t))\n",
    "    print(f'jacobs[0]{jacobs[0].shape}')\n",
    "    ### we must put it in the right shape\n",
    "    K_θθ_dθ2 = jacobs[0].sum(dim=2).transpose(1,0) \n",
    "    print(f'K_θθ_dθ2 {K_θθ_dθ2.shape}')\n",
    "    return K_θθ_dθ2\n",
    "\n",
    "a = get_K_θX_dθ2(theta_t,X_hat)\n",
    "b = get_Kxx_dx2(theta_t)\n",
    "\n",
    "\n",
    "print(a.shape,b.shape)\n",
    "\n",
    "print(a.squeeze())\n",
    "print(b.squeeze())\n",
    "\n",
    "#assert ( (a-b) < 1e-5).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0814, -0.0000],\n",
       "        [-0.0000, 2.0814]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def last_hope(theta_t,X_hat):\n",
    "    \n",
    "    hessian = torch.autograd.functional.hessian(func=lambda theta : K_θX(theta,X_hat),inputs=(theta_t))\n",
    "    \n",
    "    return -hessian.squeeze()\n",
    "    \n",
    "\n",
    "hessian = last_hope(theta_t,theta_t)\n",
    "hessian.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.0814, -0.0000]],\n",
       "\n",
       "         [[-0.0000, 2.0814]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_Kxx_dx2(theta_t,X_hat):\n",
    "    \n",
    "    hessian = torch.autograd.functional.hessian(func=lambda theta : K_θX(theta,X_hat),inputs=(theta_t))\n",
    "    print(hessian.shape)\n",
    "    return -hessian\n",
    "\n",
    "\n",
    "a = last_hope(theta_t,theta_t)\n",
    "b = get_Kxx_dx2(theta_t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0814, 0.0000],\n",
       "        [0.0000, 2.0814]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments(tests)/GradientInformation.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments%28tests%29/GradientInformation.ipynb#ch0000013?line=3'>4</a>\u001b[0m g \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdiag(delta)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments%28tests%29/GradientInformation.ipynb#ch0000013?line=4'>5</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m3\u001b[39m,\u001b[39m10\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments%28tests%29/GradientInformation.ipynb#ch0000013?line=6'>7</a>\u001b[0m rslt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(a\u001b[39m@g\u001b[39;49m\u001b[39m@a\u001b[39;49m\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments%28tests%29/GradientInformation.ipynb#ch0000013?line=8'>9</a>\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(outputs\u001b[39m=\u001b[39mrslt,inputs\u001b[39m=\u001b[39mdelta)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "delta = torch.tensor([1.0,2,3],requires_grad=True)\n",
    "g = torch.diag(delta)\n",
    "a = torch.ones(3,10,3)\n",
    "\n",
    "rslt = torch.sum(a@g@a.T)\n",
    "\n",
    "torch.autograd.grad(outputs=rslt,inputs=delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 3])\n",
      "torch.Size([3, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "delta = torch.tensor([1.0,2,3],requires_grad=True)\n",
    "g = torch.diag(delta)\n",
    "a = torch.ones(1,10,3)\n",
    "\n",
    "tmp = a@g\n",
    "print(tmp.shape)\n",
    "print(a.T.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments(tests)/GradientInformation.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments%28tests%29/GradientInformation.ipynb#ch0000015?line=0'>1</a>\u001b[0m a\u001b[39m@g\u001b[39;49m\u001b[39m@a\u001b[39;49m\u001b[39m.\u001b[39;49mT\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "a@g@a.T"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
