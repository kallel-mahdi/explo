{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 1000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Linear,Sequential,Identity\n",
    "from typing import Tuple, List, Callable, Union, Optional\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"MLP that is differentiable w.r.t to parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "                self,\n",
    "                Ls: List[int],\n",
    "                add_bias: bool = False,\n",
    "                nonlinearity: Optional[Callable] = None,\n",
    "                ):\n",
    "        \n",
    "        \"\"\"Inits MLP with the provided weights \n",
    "        Note the MLP can support batches of weights \"\"\"\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        #print(f'MyMLP received params with shape',params.shape)\n",
    "            \n",
    "        weight_sizes  = [(in_size,out_size)\n",
    "                                for in_size, out_size in zip(Ls[:-1], Ls[1:])]\n",
    "        n_layers = len(weight_sizes)\n",
    "        \n",
    "        len_params = sum(\n",
    "            [\n",
    "                (in_size + 1 * add_bias) * out_size\n",
    "                for in_size, out_size in zip(Ls[:-1], Ls[1:])\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if nonlinearity is None:\n",
    "            nonlinearity = torch.nn.ReLU()\n",
    "        \n",
    "        self.__dict__.update(locals())\n",
    "        \n",
    "    \n",
    "    def create_weights(self,params):\n",
    "        \n",
    "        weights,biases = [],[]\n",
    "                \n",
    "        start,end = (0,0)\n",
    "        \n",
    "        for (in_size,out_size) in self.weight_sizes:\n",
    "            \n",
    "            start = deepcopy(end)\n",
    "            end   = deepcopy(start)  + (in_size * out_size)\n",
    "            weight = params[...,start:end].reshape(*params.shape[:-1],out_size,in_size)\n",
    "            \n",
    "            if self.add_bias:\n",
    "                \n",
    "                bias = params[...,end:end+out_size].reshape(*params.shape[:-1],out_size)\n",
    "                end = deepcopy(end) + out_size\n",
    "            \n",
    "            else :\n",
    "                \n",
    "                bias = torch.zeros(*params.shape[:-1],out_size)\n",
    "                \n",
    "            weights.append(weight) ## add transpose or dim error\n",
    "            biases.append(bias)\n",
    "                \n",
    "            #print(f'weight.shape {weight.shape} bias.shape {bias.shape}')\n",
    "        \n",
    "        return weights,biases\n",
    "        \n",
    "        \n",
    "    def forward(self,params,states):\n",
    "        \n",
    "        weights,biases = self.create_weights(params)\n",
    "        outputs = states.T\n",
    "        \n",
    "        for i,(w,b) in enumerate(\n",
    "                            zip(weights,biases)\n",
    "                            ):\n",
    "            \n",
    "            w_tmp = w @ outputs\n",
    "            b_tmp = b.unsqueeze(-1).expand_as(w_tmp)\n",
    "            outputs =  w_tmp + b_tmp\n",
    "            \n",
    "            ## no nonlinearity for last layer\n",
    "            if i != self.n_layers :\n",
    "                outputs = self.nonlinearity(outputs)\n",
    "                \n",
    "            #print(f'forward : output_tmp.shape{output_tmp.shape} b_tmp.shape{b_tmp.shape}')\n",
    "            #print(f'forward: output.shape {output.shape}')\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "mlp = MLP([4,2],add_bias=True)\n",
    "params = torch.rand(10,mlp.len_params)\n",
    "states = torch.rand(1000,4)\n",
    "mlp(params,states).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP([4,2],add_bias=True)\n",
    "\n",
    "print(mlp.len_params)\n",
    "params = torch.rand(mlp.len_params,requires_grad=True) \n",
    "states = torch.rand(100,4)\n",
    "outputs = torch.sum(mlp(params,states))\n",
    "gradient = torch.autograd.grad(outputs=outputs, inputs=params)\n",
    "assert (gradient[0].shape == params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 51.0128,  53.7022,  47.2828,  47.0763,  51.0128,  53.7022,  47.2828,\n",
       "          47.0763, 100.0000, 100.0000]),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
