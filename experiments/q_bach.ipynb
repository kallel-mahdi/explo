{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkallel/miniconda3/envs/bopt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import botorch\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.test_functions import Hartmann\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.utils.transforms import (\n",
    "    concatenate_pending_points,\n",
    "    match_batch_shape,\n",
    "    t_batch_mode_transform,\n",
    ")\n",
    "\n",
    "neg_hartmann6 = Hartmann(dim=6, negate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkallel/miniconda3/envs/bopt/lib/python3.9/site-packages/botorch/fit.py:148: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  warnings.warn(w.message, w.category)\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.zeros(1,6)\n",
    "theta_i = train_x[0]\n",
    "train_obj = neg_hartmann6(train_x).unsqueeze(-1)\n",
    "model = SingleTaskGP(train_X=train_x, train_Y=train_obj)\n",
    "model.D = 6\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "fit_gpytorch_model(mll);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GradientInformation(botorch.acquisition.AnalyticAcquisitionFunction):\n",
    "    '''Acquisition function to sample points for gradient information.\n",
    "\n",
    "    Attributes:\n",
    "        model: Gaussian process model that supplies the Jacobian (e.g. DerivativeExactGPSEModel).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model):\n",
    "        '''Inits acquisition function with model.'''\n",
    "        super().__init__(model)\n",
    "        self.call_count = 0\n",
    "        \n",
    "\n",
    "    def update_theta_i(self, theta_i):\n",
    "        '''Updates the current parameters.\n",
    "\n",
    "        This leads to an update of K_xX_dx.\n",
    "\n",
    "        Args:\n",
    "            theta_i: New parameters.\n",
    "        '''\n",
    "        if not torch.is_tensor(theta_i):\n",
    "            theta_i = torch.tensor(theta_i)\n",
    "        self.theta_i = theta_i\n",
    "        self.update_K_xX_dx()\n",
    "    \n",
    "    def K_xX(self,theta_t,X_hat):\n",
    "            \n",
    "        rslt = self.model.covar_module(theta_t,X_hat).evaluate()\n",
    "        \n",
    "        return rslt\n",
    "\n",
    "    def update_K_xX_dx(self):\n",
    "        \n",
    "        '''When new x is given update K_xX_dx.'''\n",
    "        # Pre-compute large part of K_xX_dx.\n",
    "        X = self.model.train_inputs[0]\n",
    "        x = self.theta_i.view(-1, self.model.D)\n",
    "        self.K_xX_dx_part = self._get_KθX_dθ(x, X)\n",
    "\n",
    "  \n",
    "\n",
    "    def _get_KθX_dθ(self, theta_t, X_hat) :\n",
    "        '''Computes the analytic derivative of the kernel K(x,X) w.r.t. x.\n",
    "\n",
    "        Args:\n",
    "            x: (n x D) Test points.\n",
    "\n",
    "        Returns:\n",
    "            (n x D) The derivative of K(x,X) w.r.t. x.\n",
    "        '''\n",
    "        \n",
    "        jacobs = torch.autograd.functional.jacobian(func=lambda theta : self.K_xX(theta,X_hat),inputs=(theta_t))\n",
    "        KθX_dθ = jacobs.sum(dim=2).transpose(1,2)\n",
    "\n",
    "        return KθX_dθ\n",
    "\n",
    "    # TODO: nicer batch-update for batch of thetas.\n",
    "    #@botorch.utils.transforms.t_batch_mode_transform(expected_q=1)\n",
    "    #@concatenate_pending_points\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, thetas) :\n",
    "        \n",
    "        '''Evaluate the acquisition function on the candidate set thetas.\n",
    "\n",
    "        Args:\n",
    "            thetas: A (q) x D-dim Tensor of (q) batches with a d-dim theta points each.\n",
    "\n",
    "        Returns:\n",
    "            A (q)-dim Tensor of acquisition function values at the given theta points.\n",
    "        '''\n",
    "\n",
    "        #print(f'ACQ received thetas {thetas.shape}')\n",
    "        self.call_count+=1\n",
    "        sigma_n = self.model.likelihood.noise_covar.noise\n",
    "        D = self.model.D\n",
    "        ## does this include theta_i???\n",
    "        X = self.model.train_inputs[0] \n",
    "        x = self.theta_i.view(-1, D)\n",
    "        variances = []\n",
    "        \n",
    "        for theta in thetas:\n",
    "            \n",
    "            theta = theta.view(-1, D)\n",
    "\n",
    "            X_hat = torch.cat([X,theta])\n",
    "            K_XX = self.model.covar_module(X_hat,X_hat).evaluate() + sigma_n * torch.eye(X_hat.shape[0])\n",
    "            K_XX_inv = torch.linalg.inv(K_XX)\n",
    "\n",
    "            # get K_xX_dx\n",
    "            K_xθ_dx = self._get_KθX_dθ(x, theta)\n",
    "            K_xX_dx = torch.cat([self.K_xX_dx_part, K_xθ_dx], dim=-1)\n",
    "\n",
    "            # Compute_variance.\n",
    "            variance_d = -K_xX_dx @ K_XX_inv @ K_xX_dx.transpose(1, 2)\n",
    "            variance_d = variance_d.squeeze()\n",
    "            variances.append(torch.trace(variance_d).view(1))\n",
    "\n",
    "        return -torch.cat(variances, dim=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_calls 67\n",
      "[tensor(0.5829), tensor(0.0744), tensor(0.5760), tensor(0.6410), tensor(0.3222)]\n"
     ]
    }
   ],
   "source": [
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "gradInfo = GradientInformation(model)\n",
    "gradInfo.update_theta_i(theta_i)\n",
    "acq_points,acq_value  = optimize_acqf(\n",
    "    acq_function=gradInfo,\n",
    "    bounds=torch.tensor([[0.0] * 6, [1.0] * 6]),\n",
    "    q=5,\n",
    "    num_restarts=100,\n",
    "    raw_samples=128,\n",
    "    \n",
    "    options={'nonnegative': True},\n",
    ")\n",
    "\n",
    "print(\"n_calls\",gradInfo.call_count)\n",
    "\n",
    "\n",
    "dist = [torch.mean(torch.abs(pt-theta_i)) for pt in acq_points]\n",
    "print(dist)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bopt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6c7e76b21fbf8268359659a13a1687ca07cc6ddf0d10c2b26cf47d2a8edd420"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
