{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10225/3784993782.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ins = torch.tensor(torch.zeros(1,4),requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/debug_grad.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000001?line=12'>13</a>\u001b[0m out \u001b[39m=\u001b[39m w(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000001?line=15'>16</a>\u001b[0m \u001b[39m#out = nn.functional.linear(ins[:,:4],x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000001?line=16'>17</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000001?line=17'>18</a>\u001b[0m \u001b[39m#torch.autograd.grad(out,w.parameters())\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000001?line=18'>19</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(out,ins)\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/autograd/__init__.py:275\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs)\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m         outputs, grad_outputs_, retain_graph, create_graph, inputs,\n\u001b[1;32m    277\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Failed experiment : nn.Parameter() breaks computation graph\"\"\"\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# #ins = nn.Parameter(torch.zeros(1,4),requires_grad=True)\n",
    "# ins = torch.tensor(torch.zeros(1,4),requires_grad=True)\n",
    "# w = torch.nn.Linear(4,1,bias=False)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     #w.weight = nn.Parameter(ins[:,:4])\n",
    "#     w.weight.data = ins\n",
    "#     #w.weight = ins\n",
    "    \n",
    "# x = torch.rand(4)\n",
    "# out = w(x)\n",
    "\n",
    "\n",
    "# #out = nn.functional.linear(ins[:,:4],x)\n",
    "\n",
    "# #torch.autograd.grad(out,w.parameters())\n",
    "# torch.autograd.grad(out,ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77382/992542170.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w.weight = torch.tensor(ins[:,:4],requires_grad=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/debug_grad.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000020?line=2'>3</a>\u001b[0m ins \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m,\u001b[39m8\u001b[39m)),requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000020?line=3'>4</a>\u001b[0m \u001b[39m#a = nn.Parameter(ins,requires_grad=True)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000020?line=4'>5</a>\u001b[0m w\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ins[:,:\u001b[39m4\u001b[39m],requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000020?line=5'>6</a>\u001b[0m \u001b[39m#ins = nn.Parameter(ins)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000020?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m w(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py:1206\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[39melif\u001b[39;00m params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m name \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot assign \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m as parameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1207\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39m(torch.nn.Parameter or None expected)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m                         \u001b[39m.\u001b[39mformat(torch\u001b[39m.\u001b[39mtypename(value), name))\n\u001b[1;32m   1209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(name, value)\n\u001b[1;32m   1210\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "# init = torch.zeros((1,8),requires_grad=True)\n",
    "# w = torch.nn.Linear(4,1,bias=False)\n",
    "# ins = torch.nn.Parameter(torch.zeros((1,8)),requires_grad=True)\n",
    "# #a = nn.Parameter(ins,requires_grad=True)\n",
    "# w.weight = torch.tensor(ins[:,:4],requires_grad=True)\n",
    "# #ins = nn.Parameter(ins)\n",
    "# out = w(x)\n",
    "# a= torch.autograd.grad(out,ins)\n",
    "# a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in w.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[0., 0., 0., 0.]], requires_grad=True)),\n",
       "             ('bias', None)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 21, 21]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "\n",
    "for i in range(len(a)):\n",
    "    a[i] = 21\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_params_batch(params_batch,actor,states):\n",
    "    \n",
    "    action_batch = []\n",
    "    \n",
    "    for p in params_batch:\n",
    "        \n",
    "        actor.set_params(p)\n",
    "        action_batch.append(actor(states))\n",
    "    \n",
    "    return torch.stack(action_batch)\n",
    "        \n",
    "params_batch = torch.rand(5,16)\n",
    "last_param = torch.zeros(1,16,requires_grad=True)\n",
    "#final_batch  = torch.vstack([params_batch,last_param])\n",
    "final_batch = last_param\n",
    "states = torch.rand(10,3)\n",
    "actions = run_params_batch(final_batch,actr,states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (layer0): Linear(in_features=3, out_features=4, bias=False)\n",
       "  (layer1): Linear(in_features=4, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/debug_grad.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000014?line=5'>6</a>\u001b[0m actions \u001b[39m=\u001b[39m actr\u001b[39m.\u001b[39msuper_forward(last_param,states)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000014?line=6'>7</a>\u001b[0m \u001b[39m#torch.autograd.grad(torch.sum(actions),actr.parameters())\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/debug_grad.ipynb#ch0000014?line=7'>8</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(torch\u001b[39m.\u001b[39;49msum(actions),last_param)\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/autograd/__init__.py:275\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs)\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m         outputs, grad_outputs_, retain_graph, create_graph, inputs,\n\u001b[1;32m    277\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "with torch.enable_grad():\n",
    "    \n",
    "    actr = ActorNetwork([3,4,1])\n",
    "    states = torch.rand(10,3)\n",
    "    last_param = torch.zeros(16,requires_grad=True)\n",
    "    actions = actr.super_forward(last_param,states)\n",
    "    #torch.autograd.grad(torch.sum(actions),actr.parameters())\n",
    "    torch.autograd.grad(torch.sum(actions),last_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
