{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/registration.py:415: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "pybullet build time: Jun 23 2022 12:25:14\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/\n",
    "\n",
    "from mushroom_rl.environments.dm_control_env import DMControl\n",
    "from mushroom_rl.policy import DeterministicPolicy\n",
    "from src.ddpg import DDPG\n",
    "from src.helpers import setup_experiment\n",
    "from src.config import get_configs\n",
    "import torch\n",
    "from src.approximators.actor import ActorNetwork\n",
    "from src.approximators.critic import CriticNetwork\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# MDP\n",
    "horizon = 500\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "#mdp = DMControl('walker', 'stand', horizon, gamma)\n",
    "\n",
    "\n",
    "# Settings\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 5000\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "tau = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "\n",
    "class ESQOptimizer(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,critic,actor,\n",
    "                        sigma,params_per_step,n_workers=None):\n",
    "        \n",
    "        if n_workers is None : n_workers = mp.cpu_count()\n",
    "        \n",
    "        self.args = locals()\n",
    "        self.args.pop(\"self\")\n",
    "        \n",
    "        \n",
    "        self.actor = actor\n",
    "        self.optimizer = torch.optim.Adam(actor.model.network.parameters())\n",
    "        #self.__dict__.update(locals())\n",
    "        \n",
    "\n",
    "    def run_noisy_advantage(self,states,critic,actor,\n",
    "                            sigma,seed):\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        n_params = actor.model.network.n_params\n",
    "        eps = torch.randn(n_params) * sigma\n",
    "        actor.model.network.add_noise(eps)\n",
    "        #actor2 = actor.model.network.add_noise(-eps)\n",
    "    \n",
    "        noisy_actions = actor(states,output_tensor=True)\n",
    "        noisy_q = critic(states,noisy_actions,output_tensor=True) ##  add absorbing flag\n",
    "        noisy_q = torch.sum(noisy_q)\n",
    "        noisy_grad = torch.autograd.grad(noisy_q,actor.model.network.parameters()) ## hotfix\n",
    "        \n",
    "        #return grad1+grad2\n",
    "        \n",
    "        return noisy_grad\n",
    "        \n",
    "    def run_parallel_advantage(self,states,critic,actor,\n",
    "                               sigma,params_per_step,n_workers):\n",
    "        \n",
    "        args = [(states,critic,actor,sigma,seed) for seed in range(params_per_step)]\n",
    "        \n",
    "        ctx = mp.get_context('spawn')\n",
    "        \n",
    "        # Step 1: Init multiprocessing.Pool()\n",
    "\n",
    "        with ctx.Pool(n_workers) as pool:\n",
    "        \n",
    "            # Step 2:  Run processes (we might need to use mapreduce to avoid big memory usage)\n",
    "            grads = pool.starmap(self.run_noisy_advantage,args) ## list of [(reward*eps)]\n",
    "\n",
    "            # Step 3: Wait for workers to run then close pool\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def compute_grads(self,states):\n",
    "        \n",
    "        grads = self.run_parallel_advantage(**self.args,states=states)  \n",
    "        \n",
    "        # Step 4: Aggregate gradients\n",
    "        \n",
    "        grad_stack = []\n",
    "        \n",
    "        for i in range(len(grads[0])):\n",
    "            \n",
    "            grad_stack.append(torch.stack([grad[i] for grad in grads]))\n",
    "            \n",
    "        grads = tuple(torch.sum(stack,dim=0) for stack in grad_stack)\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def step(self,states):\n",
    "        \n",
    "        self.optimizer.zero_grad()  \n",
    "        actor_grad = self.compute_grads(states)      \n",
    "        self.actor.model.network.grad = actor_grad ## optimizer usually minimizes (add -)\n",
    "        self.optimizer.step()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_step(agent,transitions):\n",
    "    \n",
    "    agent._replay_memory.add(transitions)\n",
    "\n",
    "    state, action, reward, next_state, absorbing, _ =\\\n",
    "        agent._replay_memory.get(agent._batch_size())\n",
    "\n",
    "    q_next = agent._next_q(next_state, absorbing)\n",
    "    q_target = reward + agent.mdp_info.gamma * q_next\n",
    "\n",
    "    agent._critic_approximator.fit(state, action, q_target,\n",
    "                                    **agent._critic_fit_params)\n",
    "    \n",
    "    agent._update_target(agent._critic_approximator,\n",
    "                        agent._target_critic_approximator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo/src\n",
      "MathLog.src.helpers : WARNING : MLP dimensions : [8, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo/src/environments/objective.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rslt = torch.tensor(state, dtype=torch.float32)\n",
      "/home/q123/Desktop/explo/src/environments/objective.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states.append(torch.tensor(self.manipulate_state(state)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ard_num_dims = 18\n",
      "done collecting states\n",
      "done training critic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/mushroom_rl/approximators/parametric/torch_approximator.py:166: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.int)\n",
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'ESQOptimizer' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-2:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000004?line=53'>54</a>\u001b[0m critic_step(agent,transitions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000004?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone training critic\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000004?line=55'>56</a>\u001b[0m grads \u001b[39m=\u001b[39m esq_optimizer\u001b[39m.\u001b[39;49mstep(states[:\u001b[39m10\u001b[39;49m])\n",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 3'\u001b[0m in \u001b[0;36mESQOptimizer.step\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=77'>78</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m,states):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=79'>80</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()  \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=80'>81</a>\u001b[0m     actor_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_grads(states)      \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=81'>82</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m actor_grad \u001b[39m## optimizer usually minimizes (add -)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=82'>83</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 3'\u001b[0m in \u001b[0;36mESQOptimizer.compute_grads\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=61'>62</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_grads\u001b[39m(\u001b[39mself\u001b[39m,states):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=63'>64</a>\u001b[0m     grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_parallel_advantage(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs,states\u001b[39m=\u001b[39;49mstates)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=65'>66</a>\u001b[0m     \u001b[39m# Step 4: Aggregate gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=67'>68</a>\u001b[0m     grad_stack \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 3'\u001b[0m in \u001b[0;36mESQOptimizer.run_parallel_advantage\u001b[0;34m(self, states, critic, actor, sigma, params_per_step, n_workers)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=47'>48</a>\u001b[0m \u001b[39m# Step 1: Init multiprocessing.Pool()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=49'>50</a>\u001b[0m \u001b[39mwith\u001b[39;00m ctx\u001b[39m.\u001b[39mPool(n_workers) \u001b[39mas\u001b[39;00m pool:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=50'>51</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=51'>52</a>\u001b[0m     \u001b[39m# Step 2:  Run processes (we might need to use mapreduce to avoid big memory usage)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=52'>53</a>\u001b[0m     grads \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mstarmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_noisy_advantage,args) \u001b[39m## list of [(reward*eps)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=54'>55</a>\u001b[0m     \u001b[39m# Step 3: Wait for workers to run then close pool\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000002?line=55'>56</a>\u001b[0m     pool\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/src\n",
    "\n",
    "env_name = \"Swimmer-v4\"\n",
    "kernel_name = \"rbf\"\n",
    "\n",
    "\n",
    "## Setup environment\n",
    "env_config,likelihood_config,kernel_config,optimizer_config,trainer_config = get_configs(env_name,kernel_name)\n",
    "_,env = setup_experiment(env_config,kernel_config,likelihood_config,additional_layers=[])\n",
    "\n",
    "\n",
    "# Setup DDPG\n",
    "mdp = env.env\n",
    "\n",
    "policy_class = DeterministicPolicy\n",
    "policy_params = dict()\n",
    "\n",
    "actor_input_shape = mdp.info.observation_space.shape\n",
    "actor_params = dict(network=ActorNetwork,\n",
    "                    n_features=n_features,\n",
    "                    input_shape=actor_input_shape,\n",
    "                    output_shape=mdp.info.action_space.shape)\n",
    "\n",
    "actor_optimizer = {'class': torch.optim.Adam,\n",
    "                   'params': {'lr': 1e-5}}\n",
    "\n",
    "critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n",
    "critic_params = dict(network=CriticNetwork,\n",
    "                     optimizer={'class': torch.optim.Adam,\n",
    "                                'params': {'lr': 1e-3}},\n",
    "                     loss=F.mse_loss,\n",
    "                     n_features=n_features,\n",
    "                     input_shape=critic_input_shape,\n",
    "                     output_shape=(1,))\n",
    "\n",
    "agent = DDPG(mdp.info, policy_class,policy_params,\n",
    "             actor_params, actor_optimizer, \n",
    "             critic_params,\n",
    "             batch_size, initial_replay_size, max_replay_size,\n",
    "             tau)\n",
    "\n",
    "esq_optimizer = ESQOptimizer(\n",
    "                         critic=agent._critic_approximator,\n",
    "                         actor = agent._actor_approximator,\n",
    "                         sigma=1,\n",
    "                         params_per_step=1,\n",
    "                         n_workers=1)\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    \n",
    "    _,states,transitions = env.run_many(agent._actor_approximator,1)\n",
    "    print(\"done collecting states\")\n",
    "    critic_step(agent,transitions)\n",
    "    print(\"done training critic\")\n",
    "    grads = esq_optimizer.step(states[:10])\n",
    "    \n",
    "    #print(optimizer.actor.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mushroom_rl.approximators import Regressor\n",
    "# from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "# from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "\n",
    "# replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "# critic_approximator = Regressor(TorchApproximator,\n",
    "#                                               **critic_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = ESOptimizer(env,torch.zeros(env.mlp.len_params),sigma=1e-2,\n",
    "#                 params_per_step=50,episodes_per_param=1,n_workers=8)\n",
    "# # for i in range(1):\n",
    "    \n",
    "# #     optimizer.step()\n",
    "    \n",
    "# #     if i % 3 == 0:\n",
    "# #         avg_reward,_,transitions = env.run_many(optimizer.policy_params,5)\n",
    "# #         # print(f'avg_rewarad {avg_reward} ')\n",
    "# #         # print(f'policy_params : {optimizer.policy_params}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
