{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/registration.py:415: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "pybullet build time: Jun 23 2022 12:25:14\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/\n",
    "\n",
    "from mushroom_rl.environments.dm_control_env import DMControl\n",
    "from mushroom_rl.policy import DeterministicPolicy\n",
    "from src.ddpg import DDPG\n",
    "from src.helpers import setup_experiment\n",
    "from src.config import get_configs\n",
    "import torch\n",
    "from src.approximators.actor import ActorNetwork\n",
    "from src.approximators.critic import CriticNetwork\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# MDP\n",
    "horizon = 500\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "#mdp = DMControl('walker', 'stand', horizon, gamma)\n",
    "\n",
    "\n",
    "# Policy \n",
    "\n",
    "policy_class = DeterministicPolicy\n",
    "policy_params = dict()\n",
    "\n",
    "# Settings\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 5000\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "tau = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MathLog.src.helpers : WARNING : MLP dimensions : [8, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo/src/environments/objective.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rslt = torch.tensor(state, dtype=torch.float32)\n",
      "/home/q123/Desktop/explo/src/environments/objective.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states.append(torch.tensor(self.manipulate_state(state)))\n",
      "/home/q123/Desktop/explo/src/environments/objective.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions.append(torch.tensor(action))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ard_num_dims = 18\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Swimmer-v4\"\n",
    "kernel_name = \"rbf\"\n",
    "\n",
    "env_config,likelihood_config,kernel_config,optimizer_config,trainer_config = get_configs(env_name,kernel_name)\n",
    "_,env = setup_experiment(env_config,kernel_config,likelihood_config,additional_layers=[])\n",
    "\n",
    "\n",
    "mdp = env.env\n",
    "# Approximator\n",
    "actor_input_shape = mdp.info.observation_space.shape\n",
    "actor_params = dict(network=ActorNetwork,\n",
    "                    n_features=n_features,\n",
    "                    input_shape=actor_input_shape,\n",
    "                    output_shape=mdp.info.action_space.shape)\n",
    "\n",
    "actor_optimizer = {'class': torch.optim.Adam,\n",
    "                   'params': {'lr': 1e-5}}\n",
    "\n",
    "critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n",
    "critic_params = dict(network=CriticNetwork,\n",
    "                     optimizer={'class': torch.optim.Adam,\n",
    "                                'params': {'lr': 1e-3}},\n",
    "                     loss=F.mse_loss,\n",
    "                     n_features=n_features,\n",
    "                     input_shape=critic_input_shape,\n",
    "                     output_shape=(1,))\n",
    "\n",
    "# Agent\n",
    "agent = DDPG(mdp.info, policy_class, policy_params,\n",
    "             actor_params, actor_optimizer, critic_params,\n",
    "             batch_size, initial_replay_size, max_replay_size,\n",
    "             tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "\n",
    "class ESQOptimizer(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,states,critic,actor_mlp,actor_params,\n",
    "                        sigma,params_per_step,n_workers=None):\n",
    "        \n",
    "        if n_workers is None : n_workers = mp.cpu_count()\n",
    "        \n",
    "        self.args = locals()\n",
    "        self.args.pop(\"self\")\n",
    "        \n",
    "        self.actor_params = actor_params\n",
    "        self.optimizer = torch.optim.Adam([actor_params])\n",
    "        \n",
    "         \n",
    "    def advantage_gradient(self,states,critic,actor_mlp,actor_params):\n",
    "        \n",
    "        noisy_actions = actor_mlp(actor_params,states).squeeze().T\n",
    "        noisy_q = critic(states,noisy_actions,output_tensor=True) ##  add absorbing flag\n",
    "        noisy_q = torch.sum(noisy_q)\n",
    "        noisy_grad = torch.autograd.grad(noisy_q,actor_params)\n",
    "        \n",
    "        return noisy_grad\n",
    "\n",
    "    def run_noisy_advantage(self,states,critic,actor_mlp,actor_params,\n",
    "                            sigma,seed):\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        eps = torch.randn_like(actor_params) * sigma\n",
    "        \n",
    "        actor_params1 = actor_params + eps\n",
    "        actor_params2 = actor_params - eps\n",
    "        \n",
    "        #grad1 = self.advantage_gradient(states,critic,actor_mlp,actor_params1)\n",
    "        #grad2 = self.advantage_gradient(states,critic,actor_mlp,actor_params2)\n",
    "        noisy_actions = actor_mlp(actor_params1,states).squeeze().T\n",
    "        noisy_q = critic(states,noisy_actions,output_tensor=True) ##  add absorbing flag\n",
    "        noisy_q = torch.sum(noisy_q)\n",
    "        noisy_grad = torch.autograd.grad(noisy_q,actor_params)[0] ## hotfix\n",
    "        \n",
    "        #return grad1+grad2\n",
    "        \n",
    "        return noisy_grad\n",
    "        \n",
    "    def run_parallel_advantage(self,states,critic,actor_mlp,actor_params,\n",
    "                               sigma,params_per_step,n_workers):\n",
    "        \n",
    "        \n",
    "    \n",
    "        args = [(states,critic,actor_mlp,actor_params,sigma,seed) for seed in range(params_per_step)]\n",
    "        \n",
    "        ctx = mp.get_context('fork')\n",
    "        \n",
    "        # Step 1: Init multiprocessing.Pool()\n",
    "\n",
    "        with ctx.Pool(n_workers) as pool:\n",
    "        \n",
    "            # Step 2:  Run processes (we might need to use mapreduce to avoid big memory usage)\n",
    "            grads = pool.starmap(self.run_noisy_advantage,args) ## list of [(reward*eps)]\n",
    "\n",
    "            # Step 3: Wait for workers to run then close pool\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def compute_grads(self):\n",
    "        \n",
    "        grads = self.run_parallel_advantage(**self.args)\n",
    "        \n",
    "        grads = torch.sum(torch.stack(grads),dim=0)\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()  \n",
    "        actor_grad = self.compute_grads()      \n",
    "        self.actor_params.grad = -actor_grad ## optimizer usually minimizes\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        print(self.actor_params)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo/src\n",
      "tensor([[0.3654, 0.7330, 0.4882, 0.2555, 0.8375, 0.0195, 0.2320, 0.2841, 0.1937,\n",
      "         0.6388, 0.5567, 0.4018, 0.2184, 0.4540, 0.5587, 0.3721, 0.8128, 0.8150]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3664, 0.7340, 0.4892, 0.2565, 0.8385, 0.0205, 0.2330, 0.2851, 0.1947,\n",
      "         0.6398, 0.5577, 0.4028, 0.2194, 0.4550, 0.5597, 0.3731, 0.8138, 0.8160]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3674, 0.7350, 0.4902, 0.2575, 0.8395, 0.0215, 0.2341, 0.2861, 0.1957,\n",
      "         0.6408, 0.5587, 0.4038, 0.2204, 0.4560, 0.5607, 0.3741, 0.8148, 0.8170]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3684, 0.7360, 0.4912, 0.2585, 0.8405, 0.0225, 0.2351, 0.2872, 0.1967,\n",
      "         0.6418, 0.5597, 0.4048, 0.2214, 0.4570, 0.5617, 0.3751, 0.8158, 0.8180]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3694, 0.7371, 0.4922, 0.2595, 0.8415, 0.0235, 0.2361, 0.2882, 0.1977,\n",
      "         0.6428, 0.5607, 0.4058, 0.2224, 0.4580, 0.5627, 0.3761, 0.8168, 0.8190]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3704, 0.7381, 0.4932, 0.2605, 0.8425, 0.0245, 0.2371, 0.2892, 0.1987,\n",
      "         0.6438, 0.5617, 0.4068, 0.2234, 0.4590, 0.5637, 0.3771, 0.8178, 0.8200]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3714, 0.7391, 0.4942, 0.2615, 0.8435, 0.0255, 0.2381, 0.2902, 0.1997,\n",
      "         0.6448, 0.5627, 0.4078, 0.2244, 0.4600, 0.5647, 0.3781, 0.8188, 0.8210]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3725, 0.7401, 0.4952, 0.2625, 0.8445, 0.0266, 0.2391, 0.2912, 0.2007,\n",
      "         0.6458, 0.5637, 0.4088, 0.2254, 0.4610, 0.5657, 0.3791, 0.8199, 0.8220]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3735, 0.7411, 0.4962, 0.2636, 0.8455, 0.0276, 0.2401, 0.2922, 0.2017,\n",
      "         0.6468, 0.5647, 0.4098, 0.2264, 0.4620, 0.5667, 0.3801, 0.8209, 0.8230]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3745, 0.7421, 0.4972, 0.2646, 0.8465, 0.0286, 0.2410, 0.2933, 0.2027,\n",
      "         0.6478, 0.5657, 0.4108, 0.2274, 0.4630, 0.5677, 0.3811, 0.8219, 0.8240]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/src\n",
    "#from src.optimizers.esq_pytorch import ESQOptimizer\n",
    "\n",
    "states = torch.rand(10,8)\n",
    "parameters = torch.rand(1,18,requires_grad=True)\n",
    "optimizer = ESQOptimizer(states=states,\n",
    "                         critic=agent._critic_approximator,\n",
    "                         actor_mlp =env.mlp,\n",
    "                         actor_params = parameters,\n",
    "                         sigma=1,\n",
    "                         params_per_step=4,\n",
    "                         n_workers=2)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    grads = optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def critic_step(transitions):\n",
    "\n",
    "    agent._replay_memory.add(transitions)\n",
    "\n",
    "    if agent._replay_memory.initialized:\n",
    "        \n",
    "            state, action, reward, next_state, absorbing, _ =\\\n",
    "                agent._replay_memory.get(agent._batch_size())\n",
    "\n",
    "            q_next = agent._next_q(next_state, absorbing)\n",
    "            q_target = reward + agent.mdp_info.gamma * q_next\n",
    "\n",
    "            agent._critic_approximator.fit(state, action, q_target,\n",
    "                                            **agent._critic_fit_params)\n",
    "            \n",
    "            agent._update_target(self._critic_approximator,\n",
    "                                self._target_critic_approximator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000018?line=0'>1</a>\u001b[0m grads[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8204, 0.8464, 0.9284, 1.4442, 0.9304, 0.9537, 0.9842, 0.9743, 1.5538,\n",
       "          1.6853, 1.6955, 2.4404, 1.8310, 1.9602, 2.0126, 1.7671, 1.8853, 3.5630]]),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = torch.rand(10,8)\n",
    "parameters = torch.rand(1,18,requires_grad=True)\n",
    "actions = env.mlp(parameters,states).squeeze().T\n",
    "q = agent._critic_approximator(states,actions,output_tensor=True)\n",
    "adv = torch.sum(q)\n",
    "torch.autograd.grad(adv,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mushroom_rl.approximators import Regressor\n",
    "# from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "# from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "\n",
    "# replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "# critic_approximator = Regressor(TorchApproximator,\n",
    "#                                               **critic_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = ESOptimizer(env,torch.zeros(env.mlp.len_params),sigma=1e-2,\n",
    "#                 params_per_step=50,episodes_per_param=1,n_workers=8)\n",
    "# # for i in range(1):\n",
    "    \n",
    "# #     optimizer.step()\n",
    "    \n",
    "# #     if i % 3 == 0:\n",
    "# #         avg_reward,_,transitions = env.run_many(optimizer.policy_params,5)\n",
    "# #         # print(f'avg_rewarad {avg_reward} ')\n",
    "# #         # print(f'policy_params : {optimizer.policy_params}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
