{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from mushroom_rl.core import Core\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat((state.float(), action.float()), dim=1)\n",
    "        features1 = F.relu(self._h1(state_action))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        q = self._h3(features2)\n",
    "\n",
    "        return torch.squeeze(q)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state):\n",
    "        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        a = self._h3(features2)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import DeepAC\n",
    "from mushroom_rl.policy import Policy\n",
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "from mushroom_rl.utils.parameters import Parameter, to_parameter\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class DDPG(DeepAC):\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient algorithm.\n",
    "    \"Continuous Control with Deep Reinforcement Learning\".\n",
    "    Lillicrap T. P. et al.. 2016.\n",
    "    \"\"\"\n",
    "    def __init__(self, mdp_info, policy_class, policy_params,\n",
    "                 actor_params, actor_optimizer, critic_params, batch_size,\n",
    "                 initial_replay_size, max_replay_size, tau, policy_delay=1,\n",
    "                 critic_fit_params=None, actor_predict_params=None, critic_predict_params=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            policy_class (Policy): class of the policy;\n",
    "            policy_params (dict): parameters of the policy to build;\n",
    "            actor_params (dict): parameters of the actor approximator to\n",
    "                build;\n",
    "            actor_optimizer (dict): parameters to specify the actor optimizer\n",
    "                algorithm;\n",
    "            critic_params (dict): parameters of the critic approximator to\n",
    "                build;\n",
    "            batch_size ([int, Parameter]): the number of samples in a batch;\n",
    "            initial_replay_size (int): the number of samples to collect before\n",
    "                starting the learning;\n",
    "            max_replay_size (int): the maximum number of samples in the replay\n",
    "                memory;\n",
    "            tau ((float, Parameter)): value of coefficient for soft updates;\n",
    "            policy_delay ([int, Parameter], 1): the number of updates of the critic after\n",
    "                which an actor update is implemented;\n",
    "            critic_fit_params (dict, None): parameters of the fitting algorithm\n",
    "                of the critic approximator;\n",
    "            actor_predict_params (dict, None): parameters for the prediction with the\n",
    "                actor approximator;\n",
    "            critic_predict_params (dict, None): parameters for the prediction with the\n",
    "                critic approximator.\n",
    "        \"\"\"\n",
    "        self._critic_fit_params = dict() if critic_fit_params is None else critic_fit_params\n",
    "        self._actor_predict_params = dict() if actor_predict_params is None else actor_predict_params\n",
    "        self._critic_predict_params = dict() if critic_predict_params is None else critic_predict_params\n",
    "\n",
    "        self._batch_size = to_parameter(batch_size)\n",
    "        self._tau = to_parameter(tau)\n",
    "        self._policy_delay = to_parameter(policy_delay)\n",
    "        self._fit_count = 0\n",
    "\n",
    "        self._replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "\n",
    "        target_critic_params = deepcopy(critic_params)\n",
    "        self._critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)\n",
    "        self._target_critic_approximator = Regressor(TorchApproximator,\n",
    "                                                     **target_critic_params)\n",
    "\n",
    "        target_actor_params = deepcopy(actor_params)\n",
    "        self._actor_approximator = Regressor(TorchApproximator,\n",
    "                                             **actor_params)\n",
    "        self._target_actor_approximator = Regressor(TorchApproximator,\n",
    "                                                    **target_actor_params)\n",
    "\n",
    "        self._init_target(self._critic_approximator,\n",
    "                          self._target_critic_approximator)\n",
    "        self._init_target(self._actor_approximator,\n",
    "                          self._target_actor_approximator)\n",
    "\n",
    "        policy = policy_class(self._actor_approximator, **policy_params)\n",
    "\n",
    "        policy_parameters = self._actor_approximator.model.network.parameters()\n",
    "\n",
    "        self._add_save_attr(\n",
    "            _critic_fit_params='pickle',\n",
    "            _critic_predict_params='pickle',\n",
    "            _actor_predict_params='pickle',\n",
    "            _batch_size='mushroom',\n",
    "            _tau='mushroom',\n",
    "            _policy_delay='mushroom',\n",
    "            _fit_count='primitive',\n",
    "            _replay_memory='mushroom',\n",
    "            _critic_approximator='mushroom',\n",
    "            _target_critic_approximator='mushroom',\n",
    "            _target_actor_approximator='mushroom'\n",
    "        )\n",
    "\n",
    "        super().__init__(mdp_info, policy, actor_optimizer, policy_parameters)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self._replay_memory.add(dataset)\n",
    "        if self._replay_memory.initialized:\n",
    "            state, action, reward, next_state, absorbing, _ =\\\n",
    "                self._replay_memory.get(self._batch_size())\n",
    "\n",
    "            q_next = self._next_q(next_state, absorbing)\n",
    "            q = reward + self.mdp_info.gamma * q_next\n",
    "\n",
    "            self._critic_approximator.fit(state, action, q,\n",
    "                                          **self._critic_fit_params)\n",
    "\n",
    "            \n",
    "            #################################################\n",
    "            ##### This will change for our actor gradient step\n",
    "            if self._fit_count % self._policy_delay() == 0:\n",
    "                loss = self._loss(state)\n",
    "                self._optimize_actor_parameters(loss)\n",
    "            \n",
    "            #################################################\n",
    "\n",
    "            self._update_target(self._critic_approximator,\n",
    "                                self._target_critic_approximator)\n",
    "            self._update_target(self._actor_approximator,\n",
    "                                self._target_actor_approximator)\n",
    "\n",
    "            self._fit_count += 1\n",
    "\n",
    "    def _loss(self, state):\n",
    "        action = self._actor_approximator(state, output_tensor=True, **self._actor_predict_params)\n",
    "        q = self._critic_approximator(state, action, output_tensor=True, **self._critic_predict_params)\n",
    "\n",
    "        return -q.mean()\n",
    "\n",
    "    def _next_q(self, next_state, absorbing):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            next_state (np.ndarray): the states where next action has to be\n",
    "                evaluated;\n",
    "            absorbing (np.ndarray): the absorbing flag for the states in\n",
    "                ``next_state``.\n",
    "        Returns:\n",
    "            Action-values returned by the critic for ``next_state`` and the\n",
    "            action returned by the actor.\n",
    "        \"\"\"\n",
    "        a = self._target_actor_approximator.predict(next_state, **self._actor_predict_params)\n",
    "\n",
    "        q = self._target_critic_approximator.predict(next_state, a, **self._critic_predict_params)\n",
    "        q *= 1 - absorbing\n",
    "\n",
    "        return q\n",
    "\n",
    "    def _post_load(self):\n",
    "        self._actor_approximator = self.policy._approximator\n",
    "        self._update_optimizer_parameters(self._actor_approximator.model.network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core (trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Core(object):\n",
    "    \"\"\"\n",
    "    Implements the functions to run a generic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent, mdp, callbacks_fit=None, callback_step=None,\n",
    "                 preprocessors=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            agent (Agent): the agent moving according to a policy;\n",
    "            mdp (Environment): the environment in which the agent moves;\n",
    "            callbacks_fit (list): list of callbacks to execute at the end of\n",
    "                each fit;\n",
    "            callback_step (Callback): callback to execute after each step;\n",
    "            preprocessors (list): list of state preprocessors to be\n",
    "                applied to state variables before feeding them to the\n",
    "                agent.\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.mdp = mdp\n",
    "        self.callbacks_fit = callbacks_fit if callbacks_fit is not None else list()\n",
    "        self.callback_step = callback_step if callback_step is not None else lambda x: None\n",
    "        self._preprocessors = preprocessors if preprocessors is not None else list()\n",
    "\n",
    "        self._state = None\n",
    "\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "        self._episode_steps = None\n",
    "        self._n_episodes = None\n",
    "        self._n_steps_per_fit = None\n",
    "        self._n_episodes_per_fit = None\n",
    "\n",
    "    def learn(self, n_steps=None, n_episodes=None, n_steps_per_fit=None,\n",
    "              n_episodes_per_fit=None, render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment and fits the policy\n",
    "        using the collected samples. The agent can be moved for a given number\n",
    "        of steps or a given number of episodes and, independently from this\n",
    "        choice, the policy can be fitted after a given number of steps or a\n",
    "        given number of episodes. By default, the environment is reset.\n",
    "        Args:\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            n_steps_per_fit (int, None): number of steps between each fit of the\n",
    "                policy;\n",
    "            n_episodes_per_fit (int, None): number of episodes between each fit\n",
    "                of the policy;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        assert (n_episodes_per_fit is not None and n_steps_per_fit is None)\\\n",
    "            or (n_episodes_per_fit is None and n_steps_per_fit is not None)\n",
    "\n",
    "        self._n_steps_per_fit = n_steps_per_fit\n",
    "        self._n_episodes_per_fit = n_episodes_per_fit\n",
    "\n",
    "        if n_steps_per_fit is not None:\n",
    "            fit_condition =\\\n",
    "                lambda: self._current_steps_counter >= self._n_steps_per_fit\n",
    "        else:\n",
    "            fit_condition = lambda: self._current_episodes_counter\\\n",
    "                                     >= self._n_episodes_per_fit\n",
    "\n",
    "        self._run(n_steps, n_episodes, fit_condition, render, quiet)\n",
    "\n",
    "    def evaluate(self, initial_states=None, n_steps=None, n_episodes=None,\n",
    "                 render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment using its policy.\n",
    "        The agent is moved for a provided number of steps, episodes, or from\n",
    "        a set of initial states for the whole episode. By default, the\n",
    "        environment is reset.\n",
    "        Args:\n",
    "            initial_states (np.ndarray, None): the starting states of each\n",
    "                episode;\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        fit_condition = lambda: False\n",
    "\n",
    "        return self._run(n_steps, n_episodes, fit_condition, render, quiet,\n",
    "                         initial_states)\n",
    "\n",
    "    def _run(self, n_steps, n_episodes, fit_condition, render, quiet,\n",
    "             initial_states=None):\n",
    "        assert n_episodes is not None and n_steps is None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is not None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is None and initial_states is not None\n",
    "\n",
    "        self._n_episodes = len(\n",
    "            initial_states) if initial_states is not None else n_episodes\n",
    "\n",
    "        if n_steps is not None:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_steps_counter < n_steps\n",
    "\n",
    "            steps_progress_bar = tqdm(total=n_steps,\n",
    "                                      dynamic_ncols=True, disable=quiet,\n",
    "                                      leave=False)\n",
    "            episodes_progress_bar = tqdm(disable=True)\n",
    "        else:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_episodes_counter < self._n_episodes\n",
    "\n",
    "            steps_progress_bar = tqdm(disable=True)\n",
    "            episodes_progress_bar = tqdm(total=self._n_episodes,\n",
    "                                         dynamic_ncols=True, disable=quiet,\n",
    "                                         leave=False)\n",
    "\n",
    "        return self._run_impl(move_condition, fit_condition, steps_progress_bar,\n",
    "                              episodes_progress_bar, render, initial_states)\n",
    "\n",
    "    def _run_impl(self, move_condition, fit_condition, steps_progress_bar,\n",
    "                  episodes_progress_bar, render, initial_states):\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "\n",
    "        dataset = list()\n",
    "        last = True\n",
    "        while move_condition():\n",
    "            if last:\n",
    "                self.reset(initial_states)\n",
    "\n",
    "            sample = self._step(render)\n",
    "\n",
    "            self.callback_step([sample])\n",
    "\n",
    "            self._total_steps_counter += 1\n",
    "            self._current_steps_counter += 1\n",
    "            steps_progress_bar.update(1)\n",
    "\n",
    "            if sample[-1]:\n",
    "                self._total_episodes_counter += 1\n",
    "                self._current_episodes_counter += 1\n",
    "                episodes_progress_bar.update(1)\n",
    "\n",
    "            dataset.append(sample)\n",
    "            if fit_condition():\n",
    "                self.agent.fit(dataset)\n",
    "                self._current_episodes_counter = 0\n",
    "                self._current_steps_counter = 0\n",
    "\n",
    "                for c in self.callbacks_fit:\n",
    "                    c(dataset)\n",
    "\n",
    "                dataset = list()\n",
    "\n",
    "            last = sample[-1]\n",
    "\n",
    "        self.agent.stop()\n",
    "        self.mdp.stop()\n",
    "\n",
    "        steps_progress_bar.close()\n",
    "        episodes_progress_bar.close()\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _step(self, render):\n",
    "        \"\"\"\n",
    "        Single step.\n",
    "        Args:\n",
    "            render (bool): whether to render or not.\n",
    "        Returns:\n",
    "            A tuple containing the previous state, the action sampled by the\n",
    "            agent, the reward obtained, the reached state, the absorbing flag\n",
    "            of the reached state and the last step flag.\n",
    "        \"\"\"\n",
    "        action = self.agent.draw_action(self._state)\n",
    "        next_state, reward, absorbing, _ = self.mdp.step(action)\n",
    "\n",
    "        self._episode_steps += 1\n",
    "\n",
    "        last = not(\n",
    "            self._episode_steps < self.mdp.info.horizon and not absorbing)\n",
    "\n",
    "        state = self._state\n",
    "        next_state = self._preprocess(next_state.copy())\n",
    "        self._state = next_state\n",
    "\n",
    "        return state, action, reward, next_state, absorbing, last\n",
    "\n",
    "    def reset(self, initial_states=None):\n",
    "        \"\"\"\n",
    "        Reset the state of the agent.\n",
    "        \"\"\"\n",
    "        if initial_states is None\\\n",
    "            or self._total_episodes_counter == self._n_episodes:\n",
    "            initial_state = None\n",
    "        else:\n",
    "            initial_state = initial_states[self._total_episodes_counter]\n",
    "\n",
    "        self._state = self._preprocess(self.mdp.reset(initial_state).copy())\n",
    "        self.agent.episode_start()\n",
    "        self.agent.next_action = None\n",
    "        self._episode_steps = 0\n",
    "\n",
    "    def _preprocess(self, state):\n",
    "        \"\"\"\n",
    "        Method to apply state preprocessors.\n",
    "        Args:\n",
    "            state (np.ndarray): the state to be preprocessed.\n",
    "        Returns:\n",
    "             The preprocessed state.\n",
    "        \"\"\"\n",
    "        for p in self._preprocessors:\n",
    "            state = p(state)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.environments.dm_control_env import DMControl\n",
    "from mushroom_rl.policy import DeterministicPolicy\n",
    "\n",
    "# MDP\n",
    "horizon = 500\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "mdp = DMControl('walker', 'stand', horizon, gamma)\n",
    "\n",
    "# Policy \n",
    "\n",
    "policy_class = DeterministicPolicy\n",
    "policy_params = dict()\n",
    "\n",
    "# Settings\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 5000\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "tau = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximator\n",
    "actor_input_shape = mdp.info.observation_space.shape\n",
    "actor_params = dict(network=ActorNetwork,\n",
    "                    n_features=n_features,\n",
    "                    input_shape=actor_input_shape,\n",
    "                    output_shape=mdp.info.action_space.shape)\n",
    "\n",
    "actor_optimizer = {'class': optim.Adam,\n",
    "                   'params': {'lr': 1e-5}}\n",
    "\n",
    "critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n",
    "critic_params = dict(network=CriticNetwork,\n",
    "                     optimizer={'class': optim.Adam,\n",
    "                                'params': {'lr': 1e-3}},\n",
    "                     loss=F.mse_loss,\n",
    "                     n_features=n_features,\n",
    "                     input_shape=critic_input_shape,\n",
    "                     output_shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "\n",
    "replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.76146949e-01,  6.30552070e-01,  3.51807054e-01,  9.36072538e-01,\n",
       "       -9.60701493e-01,  2.77583577e-01, -7.47872774e-01,  6.63842085e-01,\n",
       "        1.37830244e-01,  9.90455867e-01, -6.27902866e-01,  7.78291714e-01,\n",
       "       -9.33372077e-01,  3.58910248e-01,  1.29662781e+00, -2.45250000e-01,\n",
       "       -8.78463968e-17,  5.81756865e-16,  7.63833441e-16, -7.77156117e-17,\n",
       "       -7.28306304e-16,  5.86197757e-16,  8.88178420e-18,  9.94759830e-16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#action = np.zeros((6,))\n",
    "action = torch.zeros((6,))\n",
    "mdp.reset()\n",
    "next_state, reward, absorbing, _ = mdp.step(action)\n",
    "\n",
    "#[state, action, reward, next_state, absorbing, last] : DATAAAA\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_params = 0\n",
    "\n",
    "# for n_episodes :\n",
    "\n",
    "#     \n",
    "#     data = run_noisy_params \n",
    "    \n",
    "#     fit critic on data\n",
    "    \n",
    "#     compute actor gradients\n",
    "    \n",
    "#     actor_param = params + grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (101406557.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [18]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"n_eval_episodes\":\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "### Pseudo code for ES :\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"n_samples\":500,\n",
    "    \"n_workers\":n_workers,\n",
    "    \"noise_distribution\":PytorchDistrib,\n",
    "    \"n_eval_episodes\":\n",
    "}\n",
    "\n",
    "\n",
    "# in _step : one step would correspond to an Gradnum full step + Critic fit ==> ONE GRADIENT STEP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.environments.gym_env import Gym\n",
    "from mushroom_rl.utils.spaces import Discrete,Box\n",
    "\n",
    "\n",
    "env = Gym(\"Swimmer-v4\")\n",
    "env.info.action_space\n",
    "action_space = env.info.action_space\n",
    "\n",
    "if type(action_space) == Discrete:\n",
    "    ###output one action env will discretize\n",
    "    n_actions = 1 \n",
    "elif type(action_space) == Box:\n",
    "    n_actions = action_space.shape\n",
    "else : raise ValueError(f'Unkown action space{action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n",
      "MathLog.src.helpers : WARNING : MLP dimensions : [8, 20, 20, 20, 2]\n",
      "Using ard_num_dims = 1062\n",
      "avg_rewarad tensor([-0.0636]) \n",
      "policy_params : tensor([-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010,  0.0010])\n",
      "avg_rewarad tensor([-0.0285]) \n",
      "policy_params : tensor([-0.0040, -0.0040,  0.0004,  ..., -0.0038, -0.0039,  0.0040])\n",
      "avg_rewarad tensor([-0.0372]) \n",
      "policy_params : tensor([-0.0069, -0.0069,  0.0020,  ..., -0.0068, -0.0066,  0.0068])\n",
      "avg_rewarad tensor([-0.0360]) \n",
      "policy_params : tensor([-0.0099, -0.0099,  0.0044,  ..., -0.0097, -0.0094,  0.0095])\n",
      "avg_rewarad tensor([-0.0323]) \n",
      "policy_params : tensor([-0.0128, -0.0128,  0.0067,  ..., -0.0127, -0.0122,  0.0124])\n",
      "avg_rewarad tensor([-0.0357]) \n",
      "policy_params : tensor([-0.0157, -0.0157,  0.0091,  ..., -0.0155, -0.0151,  0.0154])\n",
      "avg_rewarad tensor([-0.0437]) \n",
      "policy_params : tensor([-0.0185, -0.0186,  0.0111,  ..., -0.0183, -0.0181,  0.0184])\n",
      "avg_rewarad tensor([-0.0335]) \n",
      "policy_params : tensor([-0.0212, -0.0215,  0.0137,  ..., -0.0212, -0.0209,  0.0215])\n",
      "avg_rewarad tensor([-0.0305]) \n",
      "policy_params : tensor([-0.0239, -0.0244,  0.0164,  ..., -0.0241, -0.0237,  0.0245])\n",
      "avg_rewarad tensor([-0.0540]) \n",
      "policy_params : tensor([-0.0265, -0.0272,  0.0191,  ..., -0.0271, -0.0265,  0.0277])\n",
      "avg_rewarad tensor([-0.0320]) \n",
      "policy_params : tensor([-0.0288, -0.0299,  0.0219,  ..., -0.0301, -0.0293,  0.0308])\n",
      "avg_rewarad tensor([-0.0461]) \n",
      "policy_params : tensor([-0.0310, -0.0325,  0.0248,  ..., -0.0333, -0.0321,  0.0341])\n",
      "avg_rewarad tensor([-0.0427]) \n",
      "policy_params : tensor([-0.0332, -0.0351,  0.0277,  ..., -0.0364, -0.0350,  0.0375])\n",
      "avg_rewarad tensor([-0.0372]) \n",
      "policy_params : tensor([-0.0352, -0.0377,  0.0307,  ..., -0.0395, -0.0378,  0.0408])\n",
      "avg_rewarad tensor([-0.0363]) \n",
      "policy_params : tensor([-0.0370, -0.0403,  0.0338,  ..., -0.0426, -0.0406,  0.0442])\n",
      "avg_rewarad tensor([-0.0389]) \n",
      "policy_params : tensor([-0.0386, -0.0429,  0.0367,  ..., -0.0459, -0.0433,  0.0476])\n",
      "avg_rewarad tensor([-0.0385]) \n",
      "policy_params : tensor([-0.0401, -0.0455,  0.0397,  ..., -0.0490, -0.0460,  0.0509])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-411:\n",
      "Process ForkPoolWorker-412:\n",
      "Process ForkPoolWorker-413:\n",
      "Process ForkPoolWorker-414:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-409:\n",
      "Process ForkPoolWorker-415:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-410:\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-416:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 82, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 89, in run\n",
      "    state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/Desktop/explo/src/environments/gym_env.py\", line 98, in step\n",
      "    obs, reward, absorbing, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 160, in step\n",
      "    self.do_simulation(action, self.frame_skip)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py\", line 182, in do_simulation\n",
      "    self._mujoco_bindings.mj_step(self.model, self.data)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 78, in run\n",
      "    with torch.no_grad():\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 82, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 366, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000014?line=13'>14</a>\u001b[0m optimizer \u001b[39m=\u001b[39m ESOptimizer(env,torch\u001b[39m.\u001b[39mzeros(env\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39mlen_params),sigma\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000014?line=14'>15</a>\u001b[0m                 params_per_step\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,episodes_per_param\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,n_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000014?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000014?line=19'>20</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000014?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000014?line=22'>23</a>\u001b[0m         avg_reward,_ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mrun_many(optimizer\u001b[39m.\u001b[39mpolicy_params,\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/explo/src/optimizers/es_pytorch.py:71\u001b[0m, in \u001b[0;36mESOptimizer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     policy_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradient(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_params,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigma,\n\u001b[1;32m     72\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams_per_step,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisodes_per_param,)\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()        \n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_params\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mpolicy_grad \u001b[39m## optimizer usually minimizes\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/explo/src/optimizers/es_pytorch.py:57\u001b[0m, in \u001b[0;36mESOptimizer.compute_gradient\u001b[0;34m(self, env, policy_params, sigma, params_per_step, episodes_per_param, n_workers)\u001b[0m\n\u001b[1;32m     54\u001b[0m pool \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mPool(n_workers)\n\u001b[1;32m     56\u001b[0m \u001b[39m# Step 2:  Run processes (we might need to use mapreduce to avoid big memory usage)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m weighted_noises \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mstarmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_noisy_params,args) \u001b[39m## list of [(reward*eps)]\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# Step 3: Wait for workers to run then close pool\u001b[39;00m\n\u001b[1;32m     60\u001b[0m pool\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 82, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 113, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 89, in run\n",
      "    state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 131, in __exit__\n",
      "    torch.set_grad_enabled(self.prev)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/gym_env.py\", line 98, in step\n",
      "    obs, reward, absorbing, info = self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 89, in run\n",
      "    state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 92, in forward\n",
      "    b_tmp = b.unsqueeze(-1).expand_as(w_tmp)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 215, in __init__\n",
      "    def __init__(self, mode: bool) -> None:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 89, in run\n",
      "    state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 92, in forward\n",
      "    b_tmp = b.unsqueeze(-1).expand_as(w_tmp)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 160, in step\n",
      "    self.do_simulation(action, self.frame_skip)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py\", line 182, in do_simulation\n",
      "    self._mujoco_bindings.mj_step(self.model, self.data)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environments/gym_env.py\", line 98, in step\n",
      "    obs, reward, absorbing, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/gym_env.py\", line 98, in step\n",
      "    obs, reward, absorbing, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 168, in step\n",
      "    ctrl_cost = self.control_cost(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 154, in control_cost\n",
      "    def control_cost(self, action):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 168, in step\n",
      "    ctrl_cost = self.control_cost(action)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/\n",
    "\n",
    "from src.optimizers.es_pytorch import ESOptimizer\n",
    "from src.helpers import setup_experiment\n",
    "from src.config import get_configs\n",
    "import torch\n",
    "\n",
    "env_name = \"Swimmer-v4\"\n",
    "kernel_name = \"rbf\"\n",
    "\n",
    "env_config,likelihood_config,kernel_config,optimizer_config,trainer_config = get_configs(env_name,kernel_name)\n",
    "_,env = setup_experiment(env_config,kernel_config,likelihood_config,additional_layers=[20,20,20])\n",
    "\n",
    "optimizer = ESOptimizer(env,torch.zeros(env.mlp.len_params),sigma=1e-2,\n",
    "                params_per_step=50,episodes_per_param=1,n_workers=8)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 3 == 0:\n",
    "        avg_reward,_ = env.run_many(optimizer.policy_params,5)\n",
    "        print(f'avg_rewarad {avg_reward} ')\n",
    "        print(f'policy_params : {optimizer.policy_params}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
