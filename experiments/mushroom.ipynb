{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from mushroom_rl.core import Core\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat((state.float(), action.float()), dim=1)\n",
    "        features1 = F.relu(self._h1(state_action))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        q = self._h3(features2)\n",
    "\n",
    "        return torch.squeeze(q)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state):\n",
    "        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        a = self._h3(features2)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import DeepAC\n",
    "from mushroom_rl.policy import Policy\n",
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "from mushroom_rl.utils.parameters import Parameter, to_parameter\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class DDPG(DeepAC):\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient algorithm.\n",
    "    \"Continuous Control with Deep Reinforcement Learning\".\n",
    "    Lillicrap T. P. et al.. 2016.\n",
    "    \"\"\"\n",
    "    def __init__(self, mdp_info, policy_class, policy_params,\n",
    "                 actor_params, actor_optimizer, critic_params, batch_size,\n",
    "                 initial_replay_size, max_replay_size, tau, policy_delay=1,\n",
    "                 critic_fit_params=None, actor_predict_params=None, critic_predict_params=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            policy_class (Policy): class of the policy;\n",
    "            policy_params (dict): parameters of the policy to build;\n",
    "            actor_params (dict): parameters of the actor approximator to\n",
    "                build;\n",
    "            actor_optimizer (dict): parameters to specify the actor optimizer\n",
    "                algorithm;\n",
    "            critic_params (dict): parameters of the critic approximator to\n",
    "                build;\n",
    "            batch_size ([int, Parameter]): the number of samples in a batch;\n",
    "            initial_replay_size (int): the number of samples to collect before\n",
    "                starting the learning;\n",
    "            max_replay_size (int): the maximum number of samples in the replay\n",
    "                memory;\n",
    "            tau ((float, Parameter)): value of coefficient for soft updates;\n",
    "            policy_delay ([int, Parameter], 1): the number of updates of the critic after\n",
    "                which an actor update is implemented;\n",
    "            critic_fit_params (dict, None): parameters of the fitting algorithm\n",
    "                of the critic approximator;\n",
    "            actor_predict_params (dict, None): parameters for the prediction with the\n",
    "                actor approximator;\n",
    "            critic_predict_params (dict, None): parameters for the prediction with the\n",
    "                critic approximator.\n",
    "        \"\"\"\n",
    "        self._critic_fit_params = dict() if critic_fit_params is None else critic_fit_params\n",
    "        self._actor_predict_params = dict() if actor_predict_params is None else actor_predict_params\n",
    "        self._critic_predict_params = dict() if critic_predict_params is None else critic_predict_params\n",
    "\n",
    "        self._batch_size = to_parameter(batch_size)\n",
    "        self._tau = to_parameter(tau)\n",
    "        self._policy_delay = to_parameter(policy_delay)\n",
    "        self._fit_count = 0\n",
    "\n",
    "        self._replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "\n",
    "        target_critic_params = deepcopy(critic_params)\n",
    "        self._critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)\n",
    "        self._target_critic_approximator = Regressor(TorchApproximator,\n",
    "                                                     **target_critic_params)\n",
    "\n",
    "        target_actor_params = deepcopy(actor_params)\n",
    "        self._actor_approximator = Regressor(TorchApproximator,\n",
    "                                             **actor_params)\n",
    "        self._target_actor_approximator = Regressor(TorchApproximator,\n",
    "                                                    **target_actor_params)\n",
    "\n",
    "        self._init_target(self._critic_approximator,\n",
    "                          self._target_critic_approximator)\n",
    "        self._init_target(self._actor_approximator,\n",
    "                          self._target_actor_approximator)\n",
    "\n",
    "        policy = policy_class(self._actor_approximator, **policy_params)\n",
    "\n",
    "        policy_parameters = self._actor_approximator.model.network.parameters()\n",
    "\n",
    "        self._add_save_attr(\n",
    "            _critic_fit_params='pickle',\n",
    "            _critic_predict_params='pickle',\n",
    "            _actor_predict_params='pickle',\n",
    "            _batch_size='mushroom',\n",
    "            _tau='mushroom',\n",
    "            _policy_delay='mushroom',\n",
    "            _fit_count='primitive',\n",
    "            _replay_memory='mushroom',\n",
    "            _critic_approximator='mushroom',\n",
    "            _target_critic_approximator='mushroom',\n",
    "            _target_actor_approximator='mushroom'\n",
    "        )\n",
    "\n",
    "        super().__init__(mdp_info, policy, actor_optimizer, policy_parameters)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self._replay_memory.add(dataset)\n",
    "        if self._replay_memory.initialized:\n",
    "            state, action, reward, next_state, absorbing, _ =\\\n",
    "                self._replay_memory.get(self._batch_size())\n",
    "\n",
    "            q_next = self._next_q(next_state, absorbing)\n",
    "            q = reward + self.mdp_info.gamma * q_next\n",
    "\n",
    "            self._critic_approximator.fit(state, action, q,\n",
    "                                          **self._critic_fit_params)\n",
    "\n",
    "            \n",
    "            #################################################\n",
    "            ##### This will change for our actor gradient step\n",
    "            if self._fit_count % self._policy_delay() == 0:\n",
    "                loss = self._loss(state)\n",
    "                self._optimize_actor_parameters(loss)\n",
    "            \n",
    "            #################################################\n",
    "\n",
    "            self._update_target(self._critic_approximator,\n",
    "                                self._target_critic_approximator)\n",
    "            self._update_target(self._actor_approximator,\n",
    "                                self._target_actor_approximator)\n",
    "\n",
    "            self._fit_count += 1\n",
    "\n",
    "    def _loss(self, state):\n",
    "        action = self._actor_approximator(state, output_tensor=True, **self._actor_predict_params)\n",
    "        q = self._critic_approximator(state, action, output_tensor=True, **self._critic_predict_params)\n",
    "\n",
    "        return -q.mean()\n",
    "\n",
    "    def _next_q(self, next_state, absorbing):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            next_state (np.ndarray): the states where next action has to be\n",
    "                evaluated;\n",
    "            absorbing (np.ndarray): the absorbing flag for the states in\n",
    "                ``next_state``.\n",
    "        Returns:\n",
    "            Action-values returned by the critic for ``next_state`` and the\n",
    "            action returned by the actor.\n",
    "        \"\"\"\n",
    "        a = self._target_actor_approximator.predict(next_state, **self._actor_predict_params)\n",
    "\n",
    "        q = self._target_critic_approximator.predict(next_state, a, **self._critic_predict_params)\n",
    "        q *= 1 - absorbing\n",
    "\n",
    "        return q\n",
    "\n",
    "    def _post_load(self):\n",
    "        self._actor_approximator = self.policy._approximator\n",
    "        self._update_optimizer_parameters(self._actor_approximator.model.network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core (trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Core(object):\n",
    "    \"\"\"\n",
    "    Implements the functions to run a generic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent, mdp, callbacks_fit=None, callback_step=None,\n",
    "                 preprocessors=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            agent (Agent): the agent moving according to a policy;\n",
    "            mdp (Environment): the environment in which the agent moves;\n",
    "            callbacks_fit (list): list of callbacks to execute at the end of\n",
    "                each fit;\n",
    "            callback_step (Callback): callback to execute after each step;\n",
    "            preprocessors (list): list of state preprocessors to be\n",
    "                applied to state variables before feeding them to the\n",
    "                agent.\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.mdp = mdp\n",
    "        self.callbacks_fit = callbacks_fit if callbacks_fit is not None else list()\n",
    "        self.callback_step = callback_step if callback_step is not None else lambda x: None\n",
    "        self._preprocessors = preprocessors if preprocessors is not None else list()\n",
    "\n",
    "        self._state = None\n",
    "\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "        self._episode_steps = None\n",
    "        self._n_episodes = None\n",
    "        self._n_steps_per_fit = None\n",
    "        self._n_episodes_per_fit = None\n",
    "\n",
    "    def learn(self, n_steps=None, n_episodes=None, n_steps_per_fit=None,\n",
    "              n_episodes_per_fit=None, render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment and fits the policy\n",
    "        using the collected samples. The agent can be moved for a given number\n",
    "        of steps or a given number of episodes and, independently from this\n",
    "        choice, the policy can be fitted after a given number of steps or a\n",
    "        given number of episodes. By default, the environment is reset.\n",
    "        Args:\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            n_steps_per_fit (int, None): number of steps between each fit of the\n",
    "                policy;\n",
    "            n_episodes_per_fit (int, None): number of episodes between each fit\n",
    "                of the policy;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        assert (n_episodes_per_fit is not None and n_steps_per_fit is None)\\\n",
    "            or (n_episodes_per_fit is None and n_steps_per_fit is not None)\n",
    "\n",
    "        self._n_steps_per_fit = n_steps_per_fit\n",
    "        self._n_episodes_per_fit = n_episodes_per_fit\n",
    "\n",
    "        if n_steps_per_fit is not None:\n",
    "            fit_condition =\\\n",
    "                lambda: self._current_steps_counter >= self._n_steps_per_fit\n",
    "        else:\n",
    "            fit_condition = lambda: self._current_episodes_counter\\\n",
    "                                     >= self._n_episodes_per_fit\n",
    "\n",
    "        self._run(n_steps, n_episodes, fit_condition, render, quiet)\n",
    "\n",
    "    def evaluate(self, initial_states=None, n_steps=None, n_episodes=None,\n",
    "                 render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment using its policy.\n",
    "        The agent is moved for a provided number of steps, episodes, or from\n",
    "        a set of initial states for the whole episode. By default, the\n",
    "        environment is reset.\n",
    "        Args:\n",
    "            initial_states (np.ndarray, None): the starting states of each\n",
    "                episode;\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        fit_condition = lambda: False\n",
    "\n",
    "        return self._run(n_steps, n_episodes, fit_condition, render, quiet,\n",
    "                         initial_states)\n",
    "\n",
    "    def _run(self, n_steps, n_episodes, fit_condition, render, quiet,\n",
    "             initial_states=None):\n",
    "        assert n_episodes is not None and n_steps is None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is not None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is None and initial_states is not None\n",
    "\n",
    "        self._n_episodes = len(\n",
    "            initial_states) if initial_states is not None else n_episodes\n",
    "\n",
    "        if n_steps is not None:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_steps_counter < n_steps\n",
    "\n",
    "            steps_progress_bar = tqdm(total=n_steps,\n",
    "                                      dynamic_ncols=True, disable=quiet,\n",
    "                                      leave=False)\n",
    "            episodes_progress_bar = tqdm(disable=True)\n",
    "        else:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_episodes_counter < self._n_episodes\n",
    "\n",
    "            steps_progress_bar = tqdm(disable=True)\n",
    "            episodes_progress_bar = tqdm(total=self._n_episodes,\n",
    "                                         dynamic_ncols=True, disable=quiet,\n",
    "                                         leave=False)\n",
    "\n",
    "        return self._run_impl(move_condition, fit_condition, steps_progress_bar,\n",
    "                              episodes_progress_bar, render, initial_states)\n",
    "\n",
    "    def _run_impl(self, move_condition, fit_condition, steps_progress_bar,\n",
    "                  episodes_progress_bar, render, initial_states):\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "\n",
    "        dataset = list()\n",
    "        last = True\n",
    "        while move_condition():\n",
    "            if last:\n",
    "                self.reset(initial_states)\n",
    "\n",
    "            sample = self._step(render)\n",
    "\n",
    "            self.callback_step([sample])\n",
    "\n",
    "            self._total_steps_counter += 1\n",
    "            self._current_steps_counter += 1\n",
    "            steps_progress_bar.update(1)\n",
    "\n",
    "            if sample[-1]:\n",
    "                self._total_episodes_counter += 1\n",
    "                self._current_episodes_counter += 1\n",
    "                episodes_progress_bar.update(1)\n",
    "\n",
    "            dataset.append(sample)\n",
    "            if fit_condition():\n",
    "                self.agent.fit(dataset)\n",
    "                self._current_episodes_counter = 0\n",
    "                self._current_steps_counter = 0\n",
    "\n",
    "                for c in self.callbacks_fit:\n",
    "                    c(dataset)\n",
    "\n",
    "                dataset = list()\n",
    "\n",
    "            last = sample[-1]\n",
    "\n",
    "        self.agent.stop()\n",
    "        self.mdp.stop()\n",
    "\n",
    "        steps_progress_bar.close()\n",
    "        episodes_progress_bar.close()\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _step(self, render):\n",
    "        \"\"\"\n",
    "        Single step.\n",
    "        Args:\n",
    "            render (bool): whether to render or not.\n",
    "        Returns:\n",
    "            A tuple containing the previous state, the action sampled by the\n",
    "            agent, the reward obtained, the reached state, the absorbing flag\n",
    "            of the reached state and the last step flag.\n",
    "        \"\"\"\n",
    "        action = self.agent.draw_action(self._state)\n",
    "        next_state, reward, absorbing, _ = self.mdp.step(action)\n",
    "\n",
    "        self._episode_steps += 1\n",
    "\n",
    "        last = not(\n",
    "            self._episode_steps < self.mdp.info.horizon and not absorbing)\n",
    "\n",
    "        state = self._state\n",
    "        next_state = self._preprocess(next_state.copy())\n",
    "        self._state = next_state\n",
    "\n",
    "        return state, action, reward, next_state, absorbing, last\n",
    "\n",
    "    def reset(self, initial_states=None):\n",
    "        \"\"\"\n",
    "        Reset the state of the agent.\n",
    "        \"\"\"\n",
    "        if initial_states is None\\\n",
    "            or self._total_episodes_counter == self._n_episodes:\n",
    "            initial_state = None\n",
    "        else:\n",
    "            initial_state = initial_states[self._total_episodes_counter]\n",
    "\n",
    "        self._state = self._preprocess(self.mdp.reset(initial_state).copy())\n",
    "        self.agent.episode_start()\n",
    "        self.agent.next_action = None\n",
    "        self._episode_steps = 0\n",
    "\n",
    "    def _preprocess(self, state):\n",
    "        \"\"\"\n",
    "        Method to apply state preprocessors.\n",
    "        Args:\n",
    "            state (np.ndarray): the state to be preprocessed.\n",
    "        Returns:\n",
    "             The preprocessed state.\n",
    "        \"\"\"\n",
    "        for p in self._preprocessors:\n",
    "            state = p(state)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.environments.dm_control_env import DMControl\n",
    "from mushroom_rl.policy import DeterministicPolicy\n",
    "\n",
    "# MDP\n",
    "horizon = 500\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "mdp = DMControl('walker', 'stand', horizon, gamma)\n",
    "\n",
    "# Policy \n",
    "\n",
    "policy_class = DeterministicPolicy\n",
    "policy_params = dict()\n",
    "\n",
    "# Settings\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 5000\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "tau = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximator\n",
    "actor_input_shape = mdp.info.observation_space.shape\n",
    "actor_params = dict(network=ActorNetwork,\n",
    "                    n_features=n_features,\n",
    "                    input_shape=actor_input_shape,\n",
    "                    output_shape=mdp.info.action_space.shape)\n",
    "\n",
    "actor_optimizer = {'class': optim.Adam,\n",
    "                   'params': {'lr': 1e-5}}\n",
    "\n",
    "critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n",
    "critic_params = dict(network=CriticNetwork,\n",
    "                     optimizer={'class': optim.Adam,\n",
    "                                'params': {'lr': 1e-3}},\n",
    "                     loss=F.mse_loss,\n",
    "                     n_features=n_features,\n",
    "                     input_shape=critic_input_shape,\n",
    "                     output_shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "\n",
    "replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.50650117e-02, -9.98482771e-01, -9.99610400e-01, -2.79114287e-02,\n",
       "        -2.20969164e-01, -9.75280795e-01,  9.36846706e-02, -9.95601920e-01,\n",
       "        -9.76586156e-01, -2.15126662e-01,  5.35446917e-01, -8.44568884e-01,\n",
       "        -1.23511226e-01, -9.92343175e-01,  1.29662781e+00, -2.45250000e-01,\n",
       "         6.31439345e-18, -8.79296636e-16, -6.38378239e-16, -6.30606678e-16,\n",
       "        -1.46993528e-15, -9.68114477e-16, -2.84217094e-16,  5.75095527e-16]),\n",
       " 0.8818831264574494,\n",
       " False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "action = np.zeros((6,))\n",
    "mdp.reset()\n",
    "next_state, reward, absorbing, _ = mdp.step(action)\n",
    "\n",
    "#[state, action, reward, next_state, absorbing, last] : DATAAAA\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_params = 0\n",
    "\n",
    "# for n_episodes :\n",
    "\n",
    "#     \n",
    "#     data = run_noisy_params \n",
    "    \n",
    "#     fit critic on data\n",
    "    \n",
    "#     compute actor gradients\n",
    "    \n",
    "#     actor_param = params + grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pseudo code for ES :\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"n_samples\":500,\n",
    "    \"n_workers\":n_workers,\n",
    "    \"noise_distribution\":PytorchDistrib,\n",
    "    \"n_eval_episodes\":\n",
    "}\n",
    "\n",
    "\n",
    "# in _step : one step would correspond to an Gradnum full step + Critic fit ==> ONE GRADIENT STEP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MathLog.src.helpers : WARNING : MLP dimensions : [8, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
      "  logger.warn(\n",
      "/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ard_num_dims = 18\n",
      "avg_rewarad tensor([0.0386]) \n",
      "policy_params : tensor([ 0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
      "        -0.0010,  0.0010, -0.0010,  0.0010,  0.0010,  0.0010,  0.0010, -0.0010,\n",
      "         0.0010, -0.0010])\n",
      "avg_rewarad tensor([0.0908]) \n",
      "policy_params : tensor([ 0.0040,  0.0038, -0.0040, -0.0040, -0.0035, -0.0040, -0.0039,  0.0040,\n",
      "        -0.0036,  0.0040, -0.0040,  0.0013,  0.0040,  0.0040,  0.0040, -0.0033,\n",
      "         0.0040, -0.0040])\n",
      "avg_rewarad tensor([0.0285]) \n",
      "policy_params : tensor([ 0.0070,  0.0067, -0.0070, -0.0070, -0.0059, -0.0068, -0.0069,  0.0070,\n",
      "        -0.0064,  0.0070, -0.0069,  0.0018,  0.0070,  0.0070,  0.0070, -0.0031,\n",
      "         0.0070, -0.0070])\n",
      "avg_rewarad tensor([0.0295]) \n",
      "policy_params : tensor([ 0.0099,  0.0096, -0.0099, -0.0100, -0.0075, -0.0097, -0.0098,  0.0100,\n",
      "        -0.0092,  0.0100, -0.0097,  0.0012,  0.0100,  0.0100,  0.0100, -0.0033,\n",
      "         0.0100, -0.0099])\n",
      "avg_rewarad tensor([0.1023]) \n",
      "policy_params : tensor([ 1.2985e-02,  1.2589e-02, -1.2910e-02, -1.3043e-02, -9.2301e-03,\n",
      "        -1.2375e-02, -1.2484e-02,  1.3011e-02, -1.2042e-02,  1.2924e-02,\n",
      "        -1.2579e-02, -9.2198e-05,  1.3082e-02,  1.3135e-02,  1.3006e-02,\n",
      "        -2.9560e-03,  1.2988e-02, -1.2849e-02])\n",
      "avg_rewarad tensor([0.1427]) \n",
      "policy_params : tensor([ 0.0160,  0.0156, -0.0159, -0.0161, -0.0112, -0.0150, -0.0151,  0.0160,\n",
      "        -0.0149,  0.0159, -0.0155, -0.0002,  0.0161,  0.0162,  0.0160, -0.0022,\n",
      "         0.0160, -0.0158])\n",
      "avg_rewarad tensor([0.0915]) \n",
      "policy_params : tensor([ 1.8994e-02,  1.8596e-02, -1.8930e-02, -1.9078e-02, -1.3046e-02,\n",
      "        -1.7593e-02, -1.7570e-02,  1.8997e-02, -1.7757e-02,  1.8844e-02,\n",
      "        -1.8394e-02, -4.0285e-05,  1.9020e-02,  1.9279e-02,  1.9060e-02,\n",
      "        -1.9813e-03,  1.9039e-02, -1.8743e-02])\n",
      "avg_rewarad tensor([0.0312]) \n",
      "policy_params : tensor([ 0.0220,  0.0217, -0.0220, -0.0222, -0.0146, -0.0201, -0.0200,  0.0220,\n",
      "        -0.0205,  0.0218, -0.0213, -0.0002,  0.0220,  0.0224,  0.0221, -0.0015,\n",
      "         0.0221, -0.0217])\n",
      "avg_rewarad tensor([0.0399]) \n",
      "policy_params : tensor([ 0.0250,  0.0248, -0.0250, -0.0252, -0.0164, -0.0225, -0.0224,  0.0249,\n",
      "        -0.0234,  0.0248, -0.0242,  0.0005,  0.0249,  0.0256,  0.0252, -0.0012,\n",
      "         0.0251, -0.0247])\n",
      "avg_rewarad tensor([0.0633]) \n",
      "policy_params : tensor([ 0.0281,  0.0279, -0.0279, -0.0283, -0.0178, -0.0250, -0.0248,  0.0278,\n",
      "        -0.0264,  0.0277, -0.0272,  0.0014,  0.0278,  0.0288,  0.0283, -0.0010,\n",
      "         0.0281, -0.0276])\n",
      "avg_rewarad tensor([0.0968]) \n",
      "policy_params : tensor([ 0.0311,  0.0311, -0.0309, -0.0314, -0.0191, -0.0275, -0.0267,  0.0308,\n",
      "        -0.0294,  0.0306, -0.0302,  0.0028,  0.0307,  0.0319,  0.0314, -0.0007,\n",
      "         0.0312, -0.0306])\n",
      "avg_rewarad tensor([0.1544]) \n",
      "policy_params : tensor([ 0.0342,  0.0341, -0.0338, -0.0343, -0.0202, -0.0300, -0.0286,  0.0338,\n",
      "        -0.0325,  0.0336, -0.0331,  0.0045,  0.0335,  0.0351,  0.0344, -0.0002,\n",
      "         0.0342, -0.0335])\n",
      "avg_rewarad tensor([0.1588]) \n",
      "policy_params : tensor([ 0.0372,  0.0372, -0.0366, -0.0373, -0.0210, -0.0326, -0.0304,  0.0368,\n",
      "        -0.0356,  0.0365, -0.0361,  0.0064,  0.0364,  0.0382,  0.0374,  0.0003,\n",
      "         0.0373, -0.0364])\n",
      "avg_rewarad tensor([0.1468]) \n",
      "policy_params : tensor([ 0.0404,  0.0403, -0.0395, -0.0403, -0.0217, -0.0352, -0.0324,  0.0398,\n",
      "        -0.0386,  0.0394, -0.0391,  0.0085,  0.0393,  0.0414,  0.0404,  0.0006,\n",
      "         0.0403, -0.0393])\n",
      "avg_rewarad tensor([0.0885]) \n",
      "policy_params : tensor([ 0.0434,  0.0433, -0.0423, -0.0433, -0.0220, -0.0377, -0.0342,  0.0428,\n",
      "        -0.0416,  0.0424, -0.0421,  0.0103,  0.0422,  0.0446,  0.0435,  0.0017,\n",
      "         0.0434, -0.0423])\n",
      "avg_rewarad tensor([0.0501]) \n",
      "policy_params : tensor([ 0.0465,  0.0463, -0.0451, -0.0462, -0.0221, -0.0402, -0.0354,  0.0458,\n",
      "        -0.0447,  0.0453, -0.0452,  0.0120,  0.0450,  0.0478,  0.0465,  0.0035,\n",
      "         0.0464, -0.0451])\n",
      "avg_rewarad tensor([0.0730]) \n",
      "policy_params : tensor([ 0.0496,  0.0493, -0.0480, -0.0493, -0.0228, -0.0427, -0.0367,  0.0488,\n",
      "        -0.0474,  0.0483, -0.0482,  0.0132,  0.0479,  0.0509,  0.0495,  0.0060,\n",
      "         0.0494, -0.0480])\n",
      "avg_rewarad tensor([0.0751]) \n",
      "policy_params : tensor([ 0.0527,  0.0523, -0.0508, -0.0522, -0.0238, -0.0452, -0.0383,  0.0517,\n",
      "        -0.0498,  0.0514, -0.0513,  0.0144,  0.0508,  0.0540,  0.0525,  0.0088,\n",
      "         0.0524, -0.0510])\n",
      "avg_rewarad tensor([0.1115]) \n",
      "policy_params : tensor([ 0.0558,  0.0553, -0.0537, -0.0553, -0.0251, -0.0477, -0.0396,  0.0547,\n",
      "        -0.0522,  0.0544, -0.0544,  0.0153,  0.0536,  0.0572,  0.0556,  0.0113,\n",
      "         0.0554, -0.0540])\n",
      "avg_rewarad tensor([0.1744]) \n",
      "policy_params : tensor([ 0.0590,  0.0584, -0.0566, -0.0584, -0.0258, -0.0501, -0.0408,  0.0576,\n",
      "        -0.0548,  0.0573, -0.0574,  0.0159,  0.0565,  0.0603,  0.0587,  0.0135,\n",
      "         0.0584, -0.0569])\n",
      "avg_rewarad tensor([0.1890]) \n",
      "policy_params : tensor([ 0.0622,  0.0614, -0.0596, -0.0616, -0.0258, -0.0524, -0.0423,  0.0606,\n",
      "        -0.0574,  0.0603, -0.0604,  0.0166,  0.0594,  0.0634,  0.0618,  0.0156,\n",
      "         0.0615, -0.0598])\n",
      "avg_rewarad tensor([0.1821]) \n",
      "policy_params : tensor([ 0.0653,  0.0643, -0.0625, -0.0646, -0.0265, -0.0547, -0.0439,  0.0635,\n",
      "        -0.0598,  0.0633, -0.0635,  0.0177,  0.0623,  0.0664,  0.0649,  0.0178,\n",
      "         0.0645, -0.0628])\n",
      "avg_rewarad tensor([0.1356]) \n",
      "policy_params : tensor([ 0.0685,  0.0673, -0.0654, -0.0676, -0.0278, -0.0571, -0.0451,  0.0664,\n",
      "        -0.0625,  0.0663, -0.0667,  0.0194,  0.0651,  0.0694,  0.0679,  0.0205,\n",
      "         0.0675, -0.0658])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-545:\n",
      "Process ForkPoolWorker-546:\n",
      "Process ForkPoolWorker-548:\n",
      "Process ForkPoolWorker-550:\n",
      "Process ForkPoolWorker-551:\n",
      "Process ForkPoolWorker-552:\n",
      "Process ForkPoolWorker-547:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-549:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000012?line=13'>14</a>\u001b[0m optimizer \u001b[39m=\u001b[39m ESOptimizer(env,torch\u001b[39m.\u001b[39mzeros(\u001b[39m18\u001b[39m),sigma\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000012?line=14'>15</a>\u001b[0m                 params_per_step\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,episodes_per_param\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,n_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000012?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000012?line=19'>20</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000012?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000012?line=22'>23</a>\u001b[0m         avg_reward,_ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mrun_many(optimizer\u001b[39m.\u001b[39mpolicy_params,\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/explo/src/optimizers/es_pytorch.py:71\u001b[0m, in \u001b[0;36mESOptimizer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     policy_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradient(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_params,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigma,\n\u001b[1;32m     72\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams_per_step,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisodes_per_param,)\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()        \n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_params\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mpolicy_grad \u001b[39m##watch out for the sign\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/explo/src/optimizers/es_pytorch.py:57\u001b[0m, in \u001b[0;36mESOptimizer.compute_gradient\u001b[0;34m(self, env, policy_params, sigma, params_per_step, episodes_per_param, n_workers)\u001b[0m\n\u001b[1;32m     54\u001b[0m pool \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mPool(n_workers)\n\u001b[1;32m     56\u001b[0m \u001b[39m# Step 2:  Run processes (we might need to use mapreduce to avoid big memory usage)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m weighted_noises \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mstarmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_noisy_params,args) \u001b[39m## list of [(reward*eps)]\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# Step 3: Wait for workers to run then close pool\u001b[39;00m\n\u001b[1;32m     60\u001b[0m pool\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 88, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 95, in run\n",
      "    state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,states2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 84, in run\n",
      "    with torch.no_grad():\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 88, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,states1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 131, in __exit__\n",
      "    torch.set_grad_enabled(self.prev)\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 88, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 88, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 160, in step\n",
      "    self.do_simulation(action, self.frame_skip)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 84, in forward\n",
      "    weights,biases = self.create_weights(params)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 88, in run\n",
      "    action = self.mlp(params,states[t].unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py\", line 182, in do_simulation\n",
      "    self._mujoco_bindings.mj_step(self.model, self.data)\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 99, in forward\n",
      "    logger.debug(f'MLP : actions {outputs.shape}')\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 120, in run_many\n",
      "    reward,states = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 61, in create_weights\n",
      "    weight = params[...,start:end].reshape(*params.shape[:-1],out_size,in_size)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/logging/__init__.py\", line 1455, in debug\n",
      "    def debug(self, msg, *args, **kwargs):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 216, in __init__\n",
      "    self.prev = torch.is_grad_enabled()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 84, in forward\n",
      "    weights,biases = self.create_weights(params)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environment.py\", line 95, in run\n",
      "    state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 61, in create_weights\n",
      "    weight = params[...,start:end].reshape(*params.shape[:-1],out_size,in_size)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 99, in forward\n",
      "    logger.debug(f'MLP : actions {outputs.shape}')\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/logging/__init__.py\", line 1455, in debug\n",
      "    def debug(self, msg, *args, **kwargs):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 160, in step\n",
      "    self.do_simulation(action, self.frame_skip)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py\", line 182, in do_simulation\n",
      "    self._mujoco_bindings.mj_step(self.model, self.data)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/\n",
    "\n",
    "from src.optimizers.es_pytorch import ESOptimizer\n",
    "from src.helpers import setup_experiment\n",
    "from src.config import get_configs\n",
    "import torch\n",
    "\n",
    "env_name = \"Swimmer-v4\"\n",
    "kernel_name = \"rbf\"\n",
    "\n",
    "env_config,likelihood_config,kernel_config,optimizer_config,trainer_config = get_configs(env_name,kernel_name)\n",
    "_,env = setup_experiment(env_config,kernel_config,likelihood_config,additional_layers=[])\n",
    "\n",
    "optimizer = ESOptimizer(env,torch.zeros(18),sigma=1e-2,\n",
    "                params_per_step=50,episodes_per_param=1,n_workers=8)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 3 == 0:\n",
    "        avg_reward,_ = env.run_many(optimizer.policy_params,5)\n",
    "        print(f'avg_rewarad {avg_reward} ')\n",
    "        print(f'policy_params : {optimizer.policy_params}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
