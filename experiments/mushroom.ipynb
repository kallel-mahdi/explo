{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "The Jupyter notebook server failed to launch in time. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/\n",
    "\n",
    "from mushroom_rl.environments.dm_control_env import DMControl\n",
    "from mushroom_rl.policy import DeterministicPolicy\n",
    "from src.ddpg import DDPG\n",
    "from src.helpers import setup_experiment\n",
    "from src.config import get_configs\n",
    "import torch\n",
    "from src.approximators.actor import ActorNetwork\n",
    "from src.approximators.critic import CriticNetwork\n",
    "\n",
    "\n",
    "# MDP\n",
    "horizon = 500\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "#mdp = DMControl('walker', 'stand', horizon, gamma)\n",
    "\n",
    "\n",
    "# Settings\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 10000\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "tau = .001\n",
    "\n",
    "\n",
    "from os import path\n",
    "import logging\n",
    "log_file_path = path.join(\"/home/q123/Desktop/explo/logging.conf\")\n",
    "logging.config.fileConfig(log_file_path)\n",
    "\n",
    "log_file_path\n",
    "\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_step(agent,transitions,n_epochs=2):\n",
    "    \n",
    "    agent._replay_memory.add(transitions)\n",
    "\n",
    "    state, action, reward, next_state, absorbing, _ =\\\n",
    "        agent._replay_memory.get(agent._batch_size())\n",
    "\n",
    "    q_next = agent._next_q(next_state, absorbing)\n",
    "    q_target = reward + agent.mdp_info.gamma * q_next\n",
    "\n",
    "    agent._critic_approximator.fit(state, action, q_target,n_epochs=n_epochs,\n",
    "                                    **agent._critic_fit_params)\n",
    "    \n",
    "    agent._update_target(agent._critic_approximator,\n",
    "                        agent._target_critic_approximator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo/src\n",
      "MathLog.src.helpers : WARNING : MLP dimensions : [8, 2]\n",
      "Using ard_num_dims = 18\n",
      "avg_reward tensor([-0.0531])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0628])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0645])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0505])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0483])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0462])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0634])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0520])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0425])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0493])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0596])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0609])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0612])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0585])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0515])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0506])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0605])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0514])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0500])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0617])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0513])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0523])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0525])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0594])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0582])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0498])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0630])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0640])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0526])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0563])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0637])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0604])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0566])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0651])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0570])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0477])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0596])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0592])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0455])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0513])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0528])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0483])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0459])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0635])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0551])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0439])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0498])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0614])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0521])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0499])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0463])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0506])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0619])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0610])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0524])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0592])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0541])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0501])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0576])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0573])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0424])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0447])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0444])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0614])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0590])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0631])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0467])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0517])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0578])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0620])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0659])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0500])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0603])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0565])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0670])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0513])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0544])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0676])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0441])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0649])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0555])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0499])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0614])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0587])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0632])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0552])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0559])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0535])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0588])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0573])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0414])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0508])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0509])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0580])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0431])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0650])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0574])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0574])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0583])\n",
      "done training critic\n",
      "avg_reward tensor([-0.0622])\n",
      "done training critic\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo/src\n",
    "\n",
    "from src.optimizers.esq_pytorch import ESQOptimizer\n",
    "\n",
    "\n",
    "env_name = \"Swimmer-v4\"\n",
    "kernel_name = \"rbf\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    ## Setup environment\n",
    "    env_config,likelihood_config,kernel_config,optimizer_config,trainer_config = get_configs(env_name,kernel_name)\n",
    "    _,env = setup_experiment(env_config,kernel_config,likelihood_config,additional_layers=[])\n",
    "\n",
    "\n",
    "    # Setup DDPG\n",
    "    mdp = env.env\n",
    "\n",
    "    policy_class = DeterministicPolicy\n",
    "    policy_params = dict()\n",
    "\n",
    "    actor_input_shape = mdp.info.observation_space.shape\n",
    "    actor_params = dict(network=ActorNetwork,\n",
    "                        n_features=n_features,\n",
    "                        input_shape=actor_input_shape,\n",
    "                        output_shape=mdp.info.action_space.shape)\n",
    "\n",
    "    actor_optimizer = {'class': torch.optim.Adam,\n",
    "                    'params': {'lr': 1e-4}}\n",
    "\n",
    "    critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n",
    "    critic_params = dict(network=CriticNetwork,\n",
    "                        optimizer={'class': torch.optim.Adam,\n",
    "                                    'params': {'lr': 1e-3}},\n",
    "                        loss=F.mse_loss,\n",
    "                        n_features=n_features,\n",
    "                        input_shape=critic_input_shape,\n",
    "                        output_shape=(1,))\n",
    "\n",
    "    agent = DDPG(mdp.info, policy_class,policy_params,\n",
    "                actor_params, actor_optimizer, \n",
    "                critic_params,\n",
    "                batch_size, initial_replay_size, max_replay_size,\n",
    "                tau)\n",
    "\n",
    "    esq_optimizer = ESQOptimizer(\n",
    "                            critic=agent._critic_approximator.model.network,\n",
    "                            actor = agent._actor_approximator.model.network,\n",
    "                            sigma=1e-1,\n",
    "                            params_per_step=40,\n",
    "                            n_workers=8)\n",
    " \n",
    "    for i in range(100):\n",
    "        \n",
    "        avg_reward,states,transitions = env.run_many(agent._actor_approximator,5)\n",
    "        print(\"avg_reward\",avg_reward)  \n",
    "            \n",
    "        critic_step(agent,transitions)\n",
    "        \n",
    "        states_batch,_,_,_,_,_ = agent._replay_memory.get(agent._batch_size())\n",
    "        \n",
    "        print(\"done training critic\")\n",
    "        grads = esq_optimizer.step(torch.tensor(states_batch)) ## fit critic\n",
    "        #grads = esq_optimizer.step(states) ## fit critic\n",
    "        \n",
    "        #print(agent._critic_approximator.model.network.parameters())\n",
    "        \n",
    "    #     #print(optimizer.actor.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mushroom_rl.approximators import Regressor\n",
    "# from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "# from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "\n",
    "# replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "# critic_approximator = Regressor(TorchApproximator,\n",
    "#                                               **critic_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
