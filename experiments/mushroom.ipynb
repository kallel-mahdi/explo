{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from mushroom_rl.core import Core\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat((state.float(), action.float()), dim=1)\n",
    "        features1 = F.relu(self._h1(state_action))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        q = self._h3(features2)\n",
    "\n",
    "        return torch.squeeze(q)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state):\n",
    "        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        a = self._h3(features2)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import DeepAC\n",
    "from mushroom_rl.policy import Policy\n",
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "from mushroom_rl.utils.parameters import Parameter, to_parameter\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class DDPG(DeepAC):\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient algorithm.\n",
    "    \"Continuous Control with Deep Reinforcement Learning\".\n",
    "    Lillicrap T. P. et al.. 2016.\n",
    "    \"\"\"\n",
    "    def __init__(self, mdp_info, policy_class, policy_params,\n",
    "                 actor_params, actor_optimizer, critic_params, batch_size,\n",
    "                 initial_replay_size, max_replay_size, tau, policy_delay=1,\n",
    "                 critic_fit_params=None, actor_predict_params=None, critic_predict_params=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            policy_class (Policy): class of the policy;\n",
    "            policy_params (dict): parameters of the policy to build;\n",
    "            actor_params (dict): parameters of the actor approximator to\n",
    "                build;\n",
    "            actor_optimizer (dict): parameters to specify the actor optimizer\n",
    "                algorithm;\n",
    "            critic_params (dict): parameters of the critic approximator to\n",
    "                build;\n",
    "            batch_size ([int, Parameter]): the number of samples in a batch;\n",
    "            initial_replay_size (int): the number of samples to collect before\n",
    "                starting the learning;\n",
    "            max_replay_size (int): the maximum number of samples in the replay\n",
    "                memory;\n",
    "            tau ((float, Parameter)): value of coefficient for soft updates;\n",
    "            policy_delay ([int, Parameter], 1): the number of updates of the critic after\n",
    "                which an actor update is implemented;\n",
    "            critic_fit_params (dict, None): parameters of the fitting algorithm\n",
    "                of the critic approximator;\n",
    "            actor_predict_params (dict, None): parameters for the prediction with the\n",
    "                actor approximator;\n",
    "            critic_predict_params (dict, None): parameters for the prediction with the\n",
    "                critic approximator.\n",
    "        \"\"\"\n",
    "        self._critic_fit_params = dict() if critic_fit_params is None else critic_fit_params\n",
    "        self._actor_predict_params = dict() if actor_predict_params is None else actor_predict_params\n",
    "        self._critic_predict_params = dict() if critic_predict_params is None else critic_predict_params\n",
    "\n",
    "        self._batch_size = to_parameter(batch_size)\n",
    "        self._tau = to_parameter(tau)\n",
    "        self._policy_delay = to_parameter(policy_delay)\n",
    "        self._fit_count = 0\n",
    "\n",
    "        self._replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "\n",
    "        target_critic_params = deepcopy(critic_params)\n",
    "        self._critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)\n",
    "        self._target_critic_approximator = Regressor(TorchApproximator,\n",
    "                                                     **target_critic_params)\n",
    "\n",
    "        target_actor_params = deepcopy(actor_params)\n",
    "        self._actor_approximator = Regressor(TorchApproximator,\n",
    "                                             **actor_params)\n",
    "        self._target_actor_approximator = Regressor(TorchApproximator,\n",
    "                                                    **target_actor_params)\n",
    "\n",
    "        self._init_target(self._critic_approximator,\n",
    "                          self._target_critic_approximator)\n",
    "        self._init_target(self._actor_approximator,\n",
    "                          self._target_actor_approximator)\n",
    "\n",
    "        policy = policy_class(self._actor_approximator, **policy_params)\n",
    "\n",
    "        policy_parameters = self._actor_approximator.model.network.parameters()\n",
    "\n",
    "        self._add_save_attr(\n",
    "            _critic_fit_params='pickle',\n",
    "            _critic_predict_params='pickle',\n",
    "            _actor_predict_params='pickle',\n",
    "            _batch_size='mushroom',\n",
    "            _tau='mushroom',\n",
    "            _policy_delay='mushroom',\n",
    "            _fit_count='primitive',\n",
    "            _replay_memory='mushroom',\n",
    "            _critic_approximator='mushroom',\n",
    "            _target_critic_approximator='mushroom',\n",
    "            _target_actor_approximator='mushroom'\n",
    "        )\n",
    "\n",
    "        super().__init__(mdp_info, policy, actor_optimizer, policy_parameters)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self._replay_memory.add(dataset)\n",
    "        if self._replay_memory.initialized:\n",
    "            state, action, reward, next_state, absorbing, _ =\\\n",
    "                self._replay_memory.get(self._batch_size())\n",
    "\n",
    "            q_next = self._next_q(next_state, absorbing)\n",
    "            q = reward + self.mdp_info.gamma * q_next\n",
    "\n",
    "            self._critic_approximator.fit(state, action, q,\n",
    "                                          **self._critic_fit_params)\n",
    "\n",
    "            \n",
    "            #################################################\n",
    "            ##### This will change for our actor gradient step\n",
    "            if self._fit_count % self._policy_delay() == 0:\n",
    "                loss = self._loss(state)\n",
    "                self._optimize_actor_parameters(loss)\n",
    "            \n",
    "            #################################################\n",
    "\n",
    "            self._update_target(self._critic_approximator,\n",
    "                                self._target_critic_approximator)\n",
    "            self._update_target(self._actor_approximator,\n",
    "                                self._target_actor_approximator)\n",
    "\n",
    "            self._fit_count += 1\n",
    "\n",
    "    def _loss(self, state):\n",
    "        action = self._actor_approximator(state, output_tensor=True, **self._actor_predict_params)\n",
    "        q = self._critic_approximator(state, action, output_tensor=True, **self._critic_predict_params)\n",
    "\n",
    "        return -q.mean()\n",
    "\n",
    "    def _next_q(self, next_state, absorbing):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            next_state (np.ndarray): the states where next action has to be\n",
    "                evaluated;\n",
    "            absorbing (np.ndarray): the absorbing flag for the states in\n",
    "                ``next_state``.\n",
    "        Returns:\n",
    "            Action-values returned by the critic for ``next_state`` and the\n",
    "            action returned by the actor.\n",
    "        \"\"\"\n",
    "        a = self._target_actor_approximator.predict(next_state, **self._actor_predict_params)\n",
    "\n",
    "        q = self._target_critic_approximator.predict(next_state, a, **self._critic_predict_params)\n",
    "        q *= 1 - absorbing\n",
    "\n",
    "        return q\n",
    "\n",
    "    def _post_load(self):\n",
    "        self._actor_approximator = self.policy._approximator\n",
    "        self._update_optimizer_parameters(self._actor_approximator.model.network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core (trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Core(object):\n",
    "    \"\"\"\n",
    "    Implements the functions to run a generic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent, mdp, callbacks_fit=None, callback_step=None,\n",
    "                 preprocessors=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            agent (Agent): the agent moving according to a policy;\n",
    "            mdp (Environment): the environment in which the agent moves;\n",
    "            callbacks_fit (list): list of callbacks to execute at the end of\n",
    "                each fit;\n",
    "            callback_step (Callback): callback to execute after each step;\n",
    "            preprocessors (list): list of state preprocessors to be\n",
    "                applied to state variables before feeding them to the\n",
    "                agent.\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.mdp = mdp\n",
    "        self.callbacks_fit = callbacks_fit if callbacks_fit is not None else list()\n",
    "        self.callback_step = callback_step if callback_step is not None else lambda x: None\n",
    "        self._preprocessors = preprocessors if preprocessors is not None else list()\n",
    "\n",
    "        self._state = None\n",
    "\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "        self._episode_steps = None\n",
    "        self._n_episodes = None\n",
    "        self._n_steps_per_fit = None\n",
    "        self._n_episodes_per_fit = None\n",
    "\n",
    "    def learn(self, n_steps=None, n_episodes=None, n_steps_per_fit=None,\n",
    "              n_episodes_per_fit=None, render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment and fits the policy\n",
    "        using the collected samples. The agent can be moved for a given number\n",
    "        of steps or a given number of episodes and, independently from this\n",
    "        choice, the policy can be fitted after a given number of steps or a\n",
    "        given number of episodes. By default, the environment is reset.\n",
    "        Args:\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            n_steps_per_fit (int, None): number of steps between each fit of the\n",
    "                policy;\n",
    "            n_episodes_per_fit (int, None): number of episodes between each fit\n",
    "                of the policy;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        assert (n_episodes_per_fit is not None and n_steps_per_fit is None)\\\n",
    "            or (n_episodes_per_fit is None and n_steps_per_fit is not None)\n",
    "\n",
    "        self._n_steps_per_fit = n_steps_per_fit\n",
    "        self._n_episodes_per_fit = n_episodes_per_fit\n",
    "\n",
    "        if n_steps_per_fit is not None:\n",
    "            fit_condition =\\\n",
    "                lambda: self._current_steps_counter >= self._n_steps_per_fit\n",
    "        else:\n",
    "            fit_condition = lambda: self._current_episodes_counter\\\n",
    "                                     >= self._n_episodes_per_fit\n",
    "\n",
    "        self._run(n_steps, n_episodes, fit_condition, render, quiet)\n",
    "\n",
    "    def evaluate(self, initial_states=None, n_steps=None, n_episodes=None,\n",
    "                 render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment using its policy.\n",
    "        The agent is moved for a provided number of steps, episodes, or from\n",
    "        a set of initial states for the whole episode. By default, the\n",
    "        environment is reset.\n",
    "        Args:\n",
    "            initial_states (np.ndarray, None): the starting states of each\n",
    "                episode;\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        fit_condition = lambda: False\n",
    "\n",
    "        return self._run(n_steps, n_episodes, fit_condition, render, quiet,\n",
    "                         initial_states)\n",
    "\n",
    "    def _run(self, n_steps, n_episodes, fit_condition, render, quiet,\n",
    "             initial_states=None):\n",
    "        assert n_episodes is not None and n_steps is None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is not None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is None and initial_states is not None\n",
    "\n",
    "        self._n_episodes = len(\n",
    "            initial_states) if initial_states is not None else n_episodes\n",
    "\n",
    "        if n_steps is not None:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_steps_counter < n_steps\n",
    "\n",
    "            steps_progress_bar = tqdm(total=n_steps,\n",
    "                                      dynamic_ncols=True, disable=quiet,\n",
    "                                      leave=False)\n",
    "            episodes_progress_bar = tqdm(disable=True)\n",
    "        else:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_episodes_counter < self._n_episodes\n",
    "\n",
    "            steps_progress_bar = tqdm(disable=True)\n",
    "            episodes_progress_bar = tqdm(total=self._n_episodes,\n",
    "                                         dynamic_ncols=True, disable=quiet,\n",
    "                                         leave=False)\n",
    "\n",
    "        return self._run_impl(move_condition, fit_condition, steps_progress_bar,\n",
    "                              episodes_progress_bar, render, initial_states)\n",
    "\n",
    "    def _run_impl(self, move_condition, fit_condition, steps_progress_bar,\n",
    "                  episodes_progress_bar, render, initial_states):\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "\n",
    "        dataset = list()\n",
    "        last = True\n",
    "        while move_condition():\n",
    "            if last:\n",
    "                self.reset(initial_states)\n",
    "\n",
    "            sample = self._step(render)\n",
    "\n",
    "            self.callback_step([sample])\n",
    "\n",
    "            self._total_steps_counter += 1\n",
    "            self._current_steps_counter += 1\n",
    "            steps_progress_bar.update(1)\n",
    "\n",
    "            if sample[-1]:\n",
    "                self._total_episodes_counter += 1\n",
    "                self._current_episodes_counter += 1\n",
    "                episodes_progress_bar.update(1)\n",
    "\n",
    "            dataset.append(sample)\n",
    "            if fit_condition():\n",
    "                self.agent.fit(dataset)\n",
    "                self._current_episodes_counter = 0\n",
    "                self._current_steps_counter = 0\n",
    "\n",
    "                for c in self.callbacks_fit:\n",
    "                    c(dataset)\n",
    "\n",
    "                dataset = list()\n",
    "\n",
    "            last = sample[-1]\n",
    "\n",
    "        self.agent.stop()\n",
    "        self.mdp.stop()\n",
    "\n",
    "        steps_progress_bar.close()\n",
    "        episodes_progress_bar.close()\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _step(self, render):\n",
    "        \"\"\"\n",
    "        Single step.\n",
    "        Args:\n",
    "            render (bool): whether to render or not.\n",
    "        Returns:\n",
    "            A tuple containing the previous state, the action sampled by the\n",
    "            agent, the reward obtained, the reached state, the absorbing flag\n",
    "            of the reached state and the last step flag.\n",
    "        \"\"\"\n",
    "        action = self.agent.draw_action(self._state)\n",
    "        next_state, reward, absorbing, _ = self.mdp.step(action)\n",
    "\n",
    "        self._episode_steps += 1\n",
    "\n",
    "        last = not(\n",
    "            self._episode_steps < self.mdp.info.horizon and not absorbing)\n",
    "\n",
    "        state = self._state\n",
    "        next_state = self._preprocess(next_state.copy())\n",
    "        self._state = next_state\n",
    "\n",
    "        return state, action, reward, next_state, absorbing, last\n",
    "\n",
    "    def reset(self, initial_states=None):\n",
    "        \"\"\"\n",
    "        Reset the state of the agent.\n",
    "        \"\"\"\n",
    "        if initial_states is None\\\n",
    "            or self._total_episodes_counter == self._n_episodes:\n",
    "            initial_state = None\n",
    "        else:\n",
    "            initial_state = initial_states[self._total_episodes_counter]\n",
    "\n",
    "        self._state = self._preprocess(self.mdp.reset(initial_state).copy())\n",
    "        self.agent.episode_start()\n",
    "        self.agent.next_action = None\n",
    "        self._episode_steps = 0\n",
    "\n",
    "    def _preprocess(self, state):\n",
    "        \"\"\"\n",
    "        Method to apply state preprocessors.\n",
    "        Args:\n",
    "            state (np.ndarray): the state to be preprocessed.\n",
    "        Returns:\n",
    "             The preprocessed state.\n",
    "        \"\"\"\n",
    "        for p in self._preprocessors:\n",
    "            state = p(state)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.environments.dm_control_env import DMControl\n",
    "from mushroom_rl.policy import DeterministicPolicy\n",
    "\n",
    "# MDP\n",
    "horizon = 500\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "mdp = DMControl('walker', 'stand', horizon, gamma)\n",
    "\n",
    "# Policy \n",
    "\n",
    "policy_class = DeterministicPolicy\n",
    "policy_params = dict()\n",
    "\n",
    "# Settings\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 5000\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "tau = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MyCore(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        dataset = list()\n",
    "        \n",
    "        for i in range(self.n_episodes):\n",
    "           \n",
    "            sample = self.step()\n",
    "            dataset.append(sample) ##samples [state, action, reward, next_state, absorbing, last]\n",
    "            \n",
    "            if fit_condition():\n",
    "                self.agent.fit(dataset)\n",
    "                \n",
    "                dataset = list()\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _step(self, render):\n",
    "        \"\"\"\n",
    "        Single step, in our case this corresponds to running ESOptimizer\n",
    "        \n",
    "        Our optimizer should be returning lists of (state, action, reward, next_state, absorbing, last)\n",
    "        \n",
    "        Basically copy code from here\n",
    "        \"\"\"\n",
    "        \n",
    "        state, action, reward, next_state, absorbing, _ = self.optimizer.step()\n",
    "\n",
    "        return state, action, reward, next_state, absorbing, last\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximator\n",
    "actor_input_shape = mdp.info.observation_space.shape\n",
    "actor_params = dict(network=ActorNetwork,\n",
    "                    n_features=n_features,\n",
    "                    input_shape=actor_input_shape,\n",
    "                    output_shape=mdp.info.action_space.shape)\n",
    "\n",
    "actor_optimizer = {'class': optim.Adam,\n",
    "                   'params': {'lr': 1e-5}}\n",
    "\n",
    "critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n",
    "critic_params = dict(network=CriticNetwork,\n",
    "                     optimizer={'class': optim.Adam,\n",
    "                                'params': {'lr': 1e-3}},\n",
    "                     loss=F.mse_loss,\n",
    "                     n_features=n_features,\n",
    "                     input_shape=critic_input_shape,\n",
    "                     output_shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "\n",
    "replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n",
      "MathLog.src.helpers : WARNING : MLP dimensions : [8, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo/src/environments/objective.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rslt = torch.tensor(state, dtype=torch.float32)\n",
      "/home/q123/Desktop/explo/src/environments/objective.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states.append(torch.tensor(self.manipulate_state(state)))\n",
      "/home/q123/Desktop/explo/src/environments/objective.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions.append(torch.tensor(action))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ard_num_dims = 18\n",
      "avg_rewarad tensor([0.0410]) \n",
      "policy_params : tensor([ 0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
      "        -0.0010,  0.0010, -0.0010, -0.0010,  0.0010,  0.0010,  0.0010, -0.0010,\n",
      "         0.0010, -0.0010])\n",
      "avg_rewarad tensor([0.0790]) \n",
      "policy_params : tensor([ 0.0040,  0.0040, -0.0040, -0.0040, -0.0029, -0.0040, -0.0039,  0.0040,\n",
      "        -0.0038,  0.0040, -0.0038, -0.0036,  0.0040,  0.0040,  0.0040, -0.0033,\n",
      "         0.0040, -0.0040])\n",
      "avg_rewarad tensor([0.0304]) \n",
      "policy_params : tensor([ 0.0070,  0.0070, -0.0070, -0.0069, -0.0051, -0.0070, -0.0067,  0.0070,\n",
      "        -0.0065,  0.0070, -0.0067, -0.0060,  0.0069,  0.0069,  0.0070, -0.0057,\n",
      "         0.0070, -0.0070])\n",
      "avg_rewarad tensor([0.0245]) \n",
      "policy_params : tensor([ 0.0099,  0.0099, -0.0100, -0.0099, -0.0071, -0.0100, -0.0096,  0.0100,\n",
      "        -0.0094,  0.0100, -0.0096, -0.0076,  0.0098,  0.0099,  0.0100, -0.0071,\n",
      "         0.0100, -0.0100])\n",
      "avg_rewarad tensor([0.0550]) \n",
      "policy_params : tensor([ 0.0129,  0.0128, -0.0130, -0.0129, -0.0088, -0.0127, -0.0126,  0.0130,\n",
      "        -0.0124,  0.0130, -0.0125, -0.0087,  0.0128,  0.0129,  0.0130, -0.0087,\n",
      "         0.0130, -0.0130])\n",
      "avg_rewarad tensor([0.1366]) \n",
      "policy_params : tensor([ 0.0159,  0.0156, -0.0159, -0.0158, -0.0103, -0.0153, -0.0154,  0.0160,\n",
      "        -0.0151,  0.0160, -0.0155, -0.0099,  0.0157,  0.0159,  0.0160, -0.0095,\n",
      "         0.0161, -0.0159])\n",
      "avg_rewarad tensor([0.1332]) \n",
      "policy_params : tensor([ 0.0189,  0.0185, -0.0189, -0.0187, -0.0117, -0.0179, -0.0183,  0.0190,\n",
      "        -0.0180,  0.0190, -0.0186, -0.0109,  0.0186,  0.0190,  0.0190, -0.0103,\n",
      "         0.0191, -0.0189])\n",
      "avg_rewarad tensor([0.0808]) \n",
      "policy_params : tensor([ 0.0219,  0.0213, -0.0219, -0.0217, -0.0127, -0.0205, -0.0212,  0.0220,\n",
      "        -0.0209,  0.0220, -0.0216, -0.0116,  0.0216,  0.0222,  0.0221, -0.0103,\n",
      "         0.0221, -0.0219])\n",
      "avg_rewarad tensor([0.0258]) \n",
      "policy_params : tensor([ 0.0249,  0.0241, -0.0248, -0.0246, -0.0134, -0.0231, -0.0238,  0.0251,\n",
      "        -0.0237,  0.0249, -0.0245, -0.0127,  0.0246,  0.0253,  0.0252, -0.0098,\n",
      "         0.0252, -0.0248])\n",
      "avg_rewarad tensor([0.0486]) \n",
      "policy_params : tensor([ 0.0279,  0.0268, -0.0278, -0.0276, -0.0147, -0.0257, -0.0266,  0.0280,\n",
      "        -0.0264,  0.0280, -0.0275, -0.0132,  0.0276,  0.0285,  0.0282, -0.0086,\n",
      "         0.0282, -0.0278])\n",
      "avg_rewarad tensor([0.0582]) \n",
      "policy_params : tensor([ 0.0309,  0.0295, -0.0309, -0.0305, -0.0159, -0.0282, -0.0294,  0.0310,\n",
      "        -0.0288,  0.0310, -0.0305, -0.0138,  0.0307,  0.0316,  0.0313, -0.0075,\n",
      "         0.0313, -0.0308])\n",
      "avg_rewarad tensor([0.1311]) \n",
      "policy_params : tensor([ 0.0339,  0.0323, -0.0339, -0.0335, -0.0172, -0.0306, -0.0318,  0.0340,\n",
      "        -0.0312,  0.0340, -0.0335, -0.0141,  0.0337,  0.0347,  0.0343, -0.0060,\n",
      "         0.0343, -0.0338])\n",
      "avg_rewarad tensor([0.1659]) \n",
      "policy_params : tensor([ 0.0369,  0.0352, -0.0369, -0.0364, -0.0185, -0.0330, -0.0336,  0.0369,\n",
      "        -0.0337,  0.0370, -0.0365, -0.0136,  0.0367,  0.0378,  0.0373, -0.0043,\n",
      "         0.0373, -0.0368])\n",
      "avg_rewarad tensor([0.1653]) \n",
      "policy_params : tensor([ 0.0400,  0.0382, -0.0399, -0.0393, -0.0199, -0.0355, -0.0352,  0.0399,\n",
      "        -0.0363,  0.0399, -0.0396, -0.0134,  0.0397,  0.0409,  0.0403, -0.0023,\n",
      "         0.0403, -0.0398])\n",
      "avg_rewarad tensor([0.1322]) \n",
      "policy_params : tensor([ 0.0430,  0.0411, -0.0428, -0.0422, -0.0212, -0.0379, -0.0366,  0.0428,\n",
      "        -0.0391,  0.0429, -0.0426, -0.0128,  0.0426,  0.0439,  0.0433, -0.0002,\n",
      "         0.0434, -0.0427])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-362:\n",
      "Process ForkPoolWorker-365:\n",
      "Process ForkPoolWorker-363:\n",
      "Process ForkPoolWorker-368:\n",
      "Process ForkPoolWorker-364:\n",
      "Process ForkPoolWorker-367:\n",
      "Process ForkPoolWorker-361:\n",
      "Process ForkPoolWorker-366:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/mushroom.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000011?line=13'>14</a>\u001b[0m optimizer \u001b[39m=\u001b[39m ESOptimizer(env,torch\u001b[39m.\u001b[39mzeros(env\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39mlen_params),sigma\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000011?line=14'>15</a>\u001b[0m                 params_per_step\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,episodes_per_param\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,n_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000011?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000011?line=19'>20</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000011?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/mushroom.ipynb#ch0000011?line=22'>23</a>\u001b[0m         avg_reward,_,_ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mrun_many(optimizer\u001b[39m.\u001b[39mpolicy_params,\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/explo/src/optimizers/es_pytorch.py:71\u001b[0m, in \u001b[0;36mESOptimizer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     policy_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradient(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_params,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigma,\n\u001b[1;32m     72\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams_per_step,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisodes_per_param,)\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()        \n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_params\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mpolicy_grad \u001b[39m## optimizer usually minimizes\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/explo/src/optimizers/es_pytorch.py:57\u001b[0m, in \u001b[0;36mESOptimizer.compute_gradient\u001b[0;34m(self, env, policy_params, sigma, params_per_step, episodes_per_param, n_workers)\u001b[0m\n\u001b[1;32m     54\u001b[0m pool \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mPool(n_workers)\n\u001b[1;32m     56\u001b[0m \u001b[39m# Step 2:  Run processes (we might need to use mapreduce to avoid big memory usage)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m weighted_noises \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mstarmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_noisy_params,args) \u001b[39m## list of [(reward*eps)]\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# Step 3: Wait for workers to run then close pool\u001b[39;00m\n\u001b[1;32m     60\u001b[0m pool\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/boptim/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,_,transitions1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,_,transitions2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,_,transitions1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,_,transitions1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,_,transitions1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 92, in run\n",
      "    next_state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 78, in run\n",
      "    state = torch.Tensor(next_state)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 85, in run\n",
      "    action = self.mlp(params,state.unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 94, in run\n",
      "    states.append(torch.tensor(self.manipulate_state(state)))\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 38, in run_noisy_params\n",
      "    rewards2,_,transitions2 = tmp_env.run_many(params2,episodes_per_param)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1105, in _call_impl\n",
      "    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 190, in manipulate_state\n",
      "    rslt = torch.tensor(state, dtype=torch.float32)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 85, in run\n",
      "    action = self.mlp(params,state.unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 84, in forward\n",
      "    weights,biases = self.create_weights(params)\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 65, in create_weights\n",
      "    bias = params[...,end:end+out_size].reshape(*params.shape[:-1],out_size)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/q123/Desktop/explo/src/environments/gym_env.py\", line 98, in step\n",
      "    obs, reward, absorbing, info = self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,_,transitions1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "  File \"/home/q123/Desktop/explo/src/optimizers/es_pytorch.py\", line 37, in run_noisy_params\n",
      "    rewards1,_,transitions1 = tmp_env.run_many(params1,episodes_per_param)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 92, in run\n",
      "    next_state, reward_tmp, done, _ = self.env.step(action.detach().numpy())\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 122, in run_many\n",
      "    reward,states,transitions = self.run(params)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 94, in run\n",
      "    states.append(torch.tensor(self.manipulate_state(state)))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environments/gym_env.py\", line 98, in step\n",
      "    obs, reward, absorbing, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 49, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 160, in step\n",
      "    self.do_simulation(action, self.frame_skip)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py\", line 182, in do_simulation\n",
      "    self._mujoco_bindings.mj_step(self.model, self.data)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/environments/objective.py\", line 85, in run\n",
      "    action = self.mlp(params,state.unsqueeze(0)).squeeze()\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 37, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/wrappers/env_checker.py\", line 41, in step\n",
      "    return self.env.step(action)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/swimmer_v4.py\", line 160, in step\n",
      "    self.do_simulation(action, self.frame_skip)\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py\", line 182, in do_simulation\n",
      "    self._mujoco_bindings.mj_step(self.model, self.data)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/q123/Desktop/explo/src/policy.py\", line 99, in forward\n",
      "    logger.debug(f'MLP : actions {outputs.shape}')\n",
      "  File \"/home/q123/miniconda3/envs/boptim/lib/python3.10/logging/__init__.py\", line 1455, in debug\n",
      "    def debug(self, msg, *args, **kwargs):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd /home/q123/Desktop/explo/\n",
    "\n",
    "from src.optimizers.es_pytorch import ESOptimizer\n",
    "from src.helpers import setup_experiment\n",
    "from src.config import get_configs\n",
    "import torch\n",
    "\n",
    "env_name = \"Swimmer-v4\"\n",
    "kernel_name = \"rbf\"\n",
    "\n",
    "env_config,likelihood_config,kernel_config,optimizer_config,trainer_config = get_configs(env_name,kernel_name)\n",
    "_,env = setup_experiment(env_config,kernel_config,likelihood_config,additional_layers=[])\n",
    "\n",
    "optimizer = ESOptimizer(env,torch.zeros(env.mlp.len_params),sigma=1e-2,\n",
    "                params_per_step=50,episodes_per_param=1,n_workers=8)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 3 == 0:\n",
    "        avg_reward,_,_ = env.run_many(optimizer.policy_params,5)\n",
    "        print(f'avg_rewarad {avg_reward} ')\n",
    "        print(f'policy_params : {optimizer.policy_params}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
