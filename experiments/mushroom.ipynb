{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from mushroom_rl.core import Core\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat((state.float(), action.float()), dim=1)\n",
    "        features1 = F.relu(self._h1(state_action))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        q = self._h3(features2)\n",
    "\n",
    "        return torch.squeeze(q)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state):\n",
    "        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        a = self._h3(features2)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic.deep_actor_critic import DeepAC\n",
    "from mushroom_rl.policy import Policy\n",
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "from mushroom_rl.utils.parameters import Parameter, to_parameter\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class DDPG(DeepAC):\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient algorithm.\n",
    "    \"Continuous Control with Deep Reinforcement Learning\".\n",
    "    Lillicrap T. P. et al.. 2016.\n",
    "    \"\"\"\n",
    "    def __init__(self, mdp_info, policy_class, policy_params,\n",
    "                 actor_params, actor_optimizer, critic_params, batch_size,\n",
    "                 initial_replay_size, max_replay_size, tau, policy_delay=1,\n",
    "                 critic_fit_params=None, actor_predict_params=None, critic_predict_params=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            policy_class (Policy): class of the policy;\n",
    "            policy_params (dict): parameters of the policy to build;\n",
    "            actor_params (dict): parameters of the actor approximator to\n",
    "                build;\n",
    "            actor_optimizer (dict): parameters to specify the actor optimizer\n",
    "                algorithm;\n",
    "            critic_params (dict): parameters of the critic approximator to\n",
    "                build;\n",
    "            batch_size ([int, Parameter]): the number of samples in a batch;\n",
    "            initial_replay_size (int): the number of samples to collect before\n",
    "                starting the learning;\n",
    "            max_replay_size (int): the maximum number of samples in the replay\n",
    "                memory;\n",
    "            tau ((float, Parameter)): value of coefficient for soft updates;\n",
    "            policy_delay ([int, Parameter], 1): the number of updates of the critic after\n",
    "                which an actor update is implemented;\n",
    "            critic_fit_params (dict, None): parameters of the fitting algorithm\n",
    "                of the critic approximator;\n",
    "            actor_predict_params (dict, None): parameters for the prediction with the\n",
    "                actor approximator;\n",
    "            critic_predict_params (dict, None): parameters for the prediction with the\n",
    "                critic approximator.\n",
    "        \"\"\"\n",
    "        self._critic_fit_params = dict() if critic_fit_params is None else critic_fit_params\n",
    "        self._actor_predict_params = dict() if actor_predict_params is None else actor_predict_params\n",
    "        self._critic_predict_params = dict() if critic_predict_params is None else critic_predict_params\n",
    "\n",
    "        self._batch_size = to_parameter(batch_size)\n",
    "        self._tau = to_parameter(tau)\n",
    "        self._policy_delay = to_parameter(policy_delay)\n",
    "        self._fit_count = 0\n",
    "\n",
    "        self._replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "\n",
    "        target_critic_params = deepcopy(critic_params)\n",
    "        self._critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)\n",
    "        self._target_critic_approximator = Regressor(TorchApproximator,\n",
    "                                                     **target_critic_params)\n",
    "\n",
    "        target_actor_params = deepcopy(actor_params)\n",
    "        self._actor_approximator = Regressor(TorchApproximator,\n",
    "                                             **actor_params)\n",
    "        self._target_actor_approximator = Regressor(TorchApproximator,\n",
    "                                                    **target_actor_params)\n",
    "\n",
    "        self._init_target(self._critic_approximator,\n",
    "                          self._target_critic_approximator)\n",
    "        self._init_target(self._actor_approximator,\n",
    "                          self._target_actor_approximator)\n",
    "\n",
    "        policy = policy_class(self._actor_approximator, **policy_params)\n",
    "\n",
    "        policy_parameters = self._actor_approximator.model.network.parameters()\n",
    "\n",
    "        self._add_save_attr(\n",
    "            _critic_fit_params='pickle',\n",
    "            _critic_predict_params='pickle',\n",
    "            _actor_predict_params='pickle',\n",
    "            _batch_size='mushroom',\n",
    "            _tau='mushroom',\n",
    "            _policy_delay='mushroom',\n",
    "            _fit_count='primitive',\n",
    "            _replay_memory='mushroom',\n",
    "            _critic_approximator='mushroom',\n",
    "            _target_critic_approximator='mushroom',\n",
    "            _target_actor_approximator='mushroom'\n",
    "        )\n",
    "\n",
    "        super().__init__(mdp_info, policy, actor_optimizer, policy_parameters)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self._replay_memory.add(dataset)\n",
    "        if self._replay_memory.initialized:\n",
    "            state, action, reward, next_state, absorbing, _ =\\\n",
    "                self._replay_memory.get(self._batch_size())\n",
    "\n",
    "            q_next = self._next_q(next_state, absorbing)\n",
    "            q = reward + self.mdp_info.gamma * q_next\n",
    "\n",
    "            self._critic_approximator.fit(state, action, q,\n",
    "                                          **self._critic_fit_params)\n",
    "\n",
    "            \n",
    "            #################################################\n",
    "            ##### This will change for our actor gradient step\n",
    "            if self._fit_count % self._policy_delay() == 0:\n",
    "                loss = self._loss(state)\n",
    "                self._optimize_actor_parameters(loss)\n",
    "            \n",
    "            #################################################\n",
    "\n",
    "            self._update_target(self._critic_approximator,\n",
    "                                self._target_critic_approximator)\n",
    "            self._update_target(self._actor_approximator,\n",
    "                                self._target_actor_approximator)\n",
    "\n",
    "            self._fit_count += 1\n",
    "\n",
    "    def _loss(self, state):\n",
    "        action = self._actor_approximator(state, output_tensor=True, **self._actor_predict_params)\n",
    "        q = self._critic_approximator(state, action, output_tensor=True, **self._critic_predict_params)\n",
    "\n",
    "        return -q.mean()\n",
    "\n",
    "    def _next_q(self, next_state, absorbing):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            next_state (np.ndarray): the states where next action has to be\n",
    "                evaluated;\n",
    "            absorbing (np.ndarray): the absorbing flag for the states in\n",
    "                ``next_state``.\n",
    "        Returns:\n",
    "            Action-values returned by the critic for ``next_state`` and the\n",
    "            action returned by the actor.\n",
    "        \"\"\"\n",
    "        a = self._target_actor_approximator.predict(next_state, **self._actor_predict_params)\n",
    "\n",
    "        q = self._target_critic_approximator.predict(next_state, a, **self._critic_predict_params)\n",
    "        q *= 1 - absorbing\n",
    "\n",
    "        return q\n",
    "\n",
    "    def _post_load(self):\n",
    "        self._actor_approximator = self.policy._approximator\n",
    "        self._update_optimizer_parameters(self._actor_approximator.model.network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core (trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Core(object):\n",
    "    \"\"\"\n",
    "    Implements the functions to run a generic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent, mdp, callbacks_fit=None, callback_step=None,\n",
    "                 preprocessors=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Args:\n",
    "            agent (Agent): the agent moving according to a policy;\n",
    "            mdp (Environment): the environment in which the agent moves;\n",
    "            callbacks_fit (list): list of callbacks to execute at the end of\n",
    "                each fit;\n",
    "            callback_step (Callback): callback to execute after each step;\n",
    "            preprocessors (list): list of state preprocessors to be\n",
    "                applied to state variables before feeding them to the\n",
    "                agent.\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.mdp = mdp\n",
    "        self.callbacks_fit = callbacks_fit if callbacks_fit is not None else list()\n",
    "        self.callback_step = callback_step if callback_step is not None else lambda x: None\n",
    "        self._preprocessors = preprocessors if preprocessors is not None else list()\n",
    "\n",
    "        self._state = None\n",
    "\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "        self._episode_steps = None\n",
    "        self._n_episodes = None\n",
    "        self._n_steps_per_fit = None\n",
    "        self._n_episodes_per_fit = None\n",
    "\n",
    "    def learn(self, n_steps=None, n_episodes=None, n_steps_per_fit=None,\n",
    "              n_episodes_per_fit=None, render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment and fits the policy\n",
    "        using the collected samples. The agent can be moved for a given number\n",
    "        of steps or a given number of episodes and, independently from this\n",
    "        choice, the policy can be fitted after a given number of steps or a\n",
    "        given number of episodes. By default, the environment is reset.\n",
    "        Args:\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            n_steps_per_fit (int, None): number of steps between each fit of the\n",
    "                policy;\n",
    "            n_episodes_per_fit (int, None): number of episodes between each fit\n",
    "                of the policy;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        assert (n_episodes_per_fit is not None and n_steps_per_fit is None)\\\n",
    "            or (n_episodes_per_fit is None and n_steps_per_fit is not None)\n",
    "\n",
    "        self._n_steps_per_fit = n_steps_per_fit\n",
    "        self._n_episodes_per_fit = n_episodes_per_fit\n",
    "\n",
    "        if n_steps_per_fit is not None:\n",
    "            fit_condition =\\\n",
    "                lambda: self._current_steps_counter >= self._n_steps_per_fit\n",
    "        else:\n",
    "            fit_condition = lambda: self._current_episodes_counter\\\n",
    "                                     >= self._n_episodes_per_fit\n",
    "\n",
    "        self._run(n_steps, n_episodes, fit_condition, render, quiet)\n",
    "\n",
    "    def evaluate(self, initial_states=None, n_steps=None, n_episodes=None,\n",
    "                 render=False, quiet=False):\n",
    "        \"\"\"\n",
    "        This function moves the agent in the environment using its policy.\n",
    "        The agent is moved for a provided number of steps, episodes, or from\n",
    "        a set of initial states for the whole episode. By default, the\n",
    "        environment is reset.\n",
    "        Args:\n",
    "            initial_states (np.ndarray, None): the starting states of each\n",
    "                episode;\n",
    "            n_steps (int, None): number of steps to move the agent;\n",
    "            n_episodes (int, None): number of episodes to move the agent;\n",
    "            render (bool, False): whether to render the environment or not;\n",
    "            quiet (bool, False): whether to show the progress bar or not.\n",
    "        \"\"\"\n",
    "        fit_condition = lambda: False\n",
    "\n",
    "        return self._run(n_steps, n_episodes, fit_condition, render, quiet,\n",
    "                         initial_states)\n",
    "\n",
    "    def _run(self, n_steps, n_episodes, fit_condition, render, quiet,\n",
    "             initial_states=None):\n",
    "        assert n_episodes is not None and n_steps is None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is not None and initial_states is None\\\n",
    "            or n_episodes is None and n_steps is None and initial_states is not None\n",
    "\n",
    "        self._n_episodes = len(\n",
    "            initial_states) if initial_states is not None else n_episodes\n",
    "\n",
    "        if n_steps is not None:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_steps_counter < n_steps\n",
    "\n",
    "            steps_progress_bar = tqdm(total=n_steps,\n",
    "                                      dynamic_ncols=True, disable=quiet,\n",
    "                                      leave=False)\n",
    "            episodes_progress_bar = tqdm(disable=True)\n",
    "        else:\n",
    "            move_condition =\\\n",
    "                lambda: self._total_episodes_counter < self._n_episodes\n",
    "\n",
    "            steps_progress_bar = tqdm(disable=True)\n",
    "            episodes_progress_bar = tqdm(total=self._n_episodes,\n",
    "                                         dynamic_ncols=True, disable=quiet,\n",
    "                                         leave=False)\n",
    "\n",
    "        return self._run_impl(move_condition, fit_condition, steps_progress_bar,\n",
    "                              episodes_progress_bar, render, initial_states)\n",
    "\n",
    "    def _run_impl(self, move_condition, fit_condition, steps_progress_bar,\n",
    "                  episodes_progress_bar, render, initial_states):\n",
    "        self._total_episodes_counter = 0\n",
    "        self._total_steps_counter = 0\n",
    "        self._current_episodes_counter = 0\n",
    "        self._current_steps_counter = 0\n",
    "\n",
    "        dataset = list()\n",
    "        last = True\n",
    "        while move_condition():\n",
    "            if last:\n",
    "                self.reset(initial_states)\n",
    "\n",
    "            sample = self._step(render)\n",
    "\n",
    "            self.callback_step([sample])\n",
    "\n",
    "            self._total_steps_counter += 1\n",
    "            self._current_steps_counter += 1\n",
    "            steps_progress_bar.update(1)\n",
    "\n",
    "            if sample[-1]:\n",
    "                self._total_episodes_counter += 1\n",
    "                self._current_episodes_counter += 1\n",
    "                episodes_progress_bar.update(1)\n",
    "\n",
    "            dataset.append(sample)\n",
    "            if fit_condition():\n",
    "                self.agent.fit(dataset)\n",
    "                self._current_episodes_counter = 0\n",
    "                self._current_steps_counter = 0\n",
    "\n",
    "                for c in self.callbacks_fit:\n",
    "                    c(dataset)\n",
    "\n",
    "                dataset = list()\n",
    "\n",
    "            last = sample[-1]\n",
    "\n",
    "        self.agent.stop()\n",
    "        self.mdp.stop()\n",
    "\n",
    "        steps_progress_bar.close()\n",
    "        episodes_progress_bar.close()\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _step(self, render):\n",
    "        \"\"\"\n",
    "        Single step.\n",
    "        Args:\n",
    "            render (bool): whether to render or not.\n",
    "        Returns:\n",
    "            A tuple containing the previous state, the action sampled by the\n",
    "            agent, the reward obtained, the reached state, the absorbing flag\n",
    "            of the reached state and the last step flag.\n",
    "        \"\"\"\n",
    "        action = self.agent.draw_action(self._state)\n",
    "        next_state, reward, absorbing, _ = self.mdp.step(action)\n",
    "\n",
    "        self._episode_steps += 1\n",
    "\n",
    "        last = not(\n",
    "            self._episode_steps < self.mdp.info.horizon and not absorbing)\n",
    "\n",
    "        state = self._state\n",
    "        next_state = self._preprocess(next_state.copy())\n",
    "        self._state = next_state\n",
    "\n",
    "        return state, action, reward, next_state, absorbing, last\n",
    "\n",
    "    def reset(self, initial_states=None):\n",
    "        \"\"\"\n",
    "        Reset the state of the agent.\n",
    "        \"\"\"\n",
    "        if initial_states is None\\\n",
    "            or self._total_episodes_counter == self._n_episodes:\n",
    "            initial_state = None\n",
    "        else:\n",
    "            initial_state = initial_states[self._total_episodes_counter]\n",
    "\n",
    "        self._state = self._preprocess(self.mdp.reset(initial_state).copy())\n",
    "        self.agent.episode_start()\n",
    "        self.agent.next_action = None\n",
    "        self._episode_steps = 0\n",
    "\n",
    "    def _preprocess(self, state):\n",
    "        \"\"\"\n",
    "        Method to apply state preprocessors.\n",
    "        Args:\n",
    "            state (np.ndarray): the state to be preprocessed.\n",
    "        Returns:\n",
    "             The preprocessed state.\n",
    "        \"\"\"\n",
    "        for p in self._preprocessors:\n",
    "            state = p(state)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.environments.dm_control_env import DMControl\n",
    "from mushroom_rl.policy import DeterministicPolicy\n",
    "\n",
    "# MDP\n",
    "horizon = 500\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "mdp = DMControl('walker', 'stand', horizon, gamma)\n",
    "\n",
    "# Policy \n",
    "\n",
    "policy_class = DeterministicPolicy\n",
    "policy_params = dict()\n",
    "\n",
    "# Settings\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 5000\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "tau = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximator\n",
    "actor_input_shape = mdp.info.observation_space.shape\n",
    "actor_params = dict(network=ActorNetwork,\n",
    "                    n_features=n_features,\n",
    "                    input_shape=actor_input_shape,\n",
    "                    output_shape=mdp.info.action_space.shape)\n",
    "\n",
    "actor_optimizer = {'class': optim.Adam,\n",
    "                   'params': {'lr': 1e-5}}\n",
    "\n",
    "critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n",
    "critic_params = dict(network=CriticNetwork,\n",
    "                     optimizer={'class': optim.Adam,\n",
    "                                'params': {'lr': 1e-3}},\n",
    "                     loss=F.mse_loss,\n",
    "                     n_features=n_features,\n",
    "                     input_shape=critic_input_shape,\n",
    "                     output_shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.approximators import Regressor\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.replay_memory import ReplayMemory\n",
    "\n",
    "replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n",
    "critic_approximator = Regressor(TorchApproximator,\n",
    "                                              **critic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.50650117e-02, -9.98482771e-01, -9.99610400e-01, -2.79114287e-02,\n",
       "        -2.20969164e-01, -9.75280795e-01,  9.36846706e-02, -9.95601920e-01,\n",
       "        -9.76586156e-01, -2.15126662e-01,  5.35446917e-01, -8.44568884e-01,\n",
       "        -1.23511226e-01, -9.92343175e-01,  1.29662781e+00, -2.45250000e-01,\n",
       "         6.31439345e-18, -8.79296636e-16, -6.38378239e-16, -6.30606678e-16,\n",
       "        -1.46993528e-15, -9.68114477e-16, -2.84217094e-16,  5.75095527e-16]),\n",
       " 0.8818831264574494,\n",
       " False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "action = np.zeros((6,))\n",
    "mdp.reset()\n",
    "next_state, reward, absorbing, _ = mdp.step(action)\n",
    "\n",
    "#[state, action, reward, next_state, absorbing, last] : DATAAAA\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_params = 0\n",
    "\n",
    "# for n_episodes :\n",
    "    \n",
    "#     data = run_params + noisy_params \n",
    "    \n",
    "#     fit critic on data\n",
    "    \n",
    "#     compute actor gradients\n",
    "    \n",
    "#     actor_param = params + grads\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('boptim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8389904c907846b71296796d17b1509d31543c622799a32225d90d0bb5700220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
