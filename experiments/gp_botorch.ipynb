{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo\n",
    "\n",
    "### local imports \n",
    "from src.environment import EnvironmentObjective\n",
    "from src.vanillagp import step\n",
    "from src.policy import MLP\n",
    "\n",
    "### botorch\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "### general imports\n",
    "import torch\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr -11.268491744995117 max 13.784440040588379\n",
      "curr -18.64009666442871 max 27.62029457092285\n",
      "curr 24.820655822753906 max 27.62029457092285\n",
      "curr 7.225327014923096 max 27.62029457092285\n",
      "curr -6.598489284515381 max 27.62029457092285\n",
      "curr -28.516984939575195 max 32.97597885131836\n",
      "curr 34.18520736694336 max 122.03939819335938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/vanillagp.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/vanillagp.ipynb#ch0000001?line=21'>22</a>\u001b[0m max_iter \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/vanillagp.ipynb#ch0000001?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/vanillagp.ipynb#ch0000001?line=25'>26</a>\u001b[0m   step(model,objective_env)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/vanillagp.ipynb#ch0000001?line=27'>28</a>\u001b[0m   \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/vanillagp.ipynb#ch0000001?line=29'>30</a>\u001b[0m     best_val \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain_targets\u001b[39m.\u001b[39mmax()\n",
      "File \u001b[0;32m~/Desktop/explo/src/vanillagp.py:33\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(model, objective_env)\u001b[0m\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=21'>22</a>\u001b[0m EI \u001b[39m=\u001b[39m ExpectedImprovement(model\u001b[39m=\u001b[39mmodel, best_f\u001b[39m=\u001b[39mbest_value)\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=23'>24</a>\u001b[0m new_x, _ \u001b[39m=\u001b[39m optimize_acqf(\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=24'>25</a>\u001b[0m   acq_function\u001b[39m=\u001b[39mEI,\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=25'>26</a>\u001b[0m   bounds\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor([[\u001b[39m0.0\u001b[39m] \u001b[39m*\u001b[39m len_params, [\u001b[39m1.0\u001b[39m] \u001b[39m*\u001b[39m len_params]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=29'>30</a>\u001b[0m   options\u001b[39m=\u001b[39m{},\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=30'>31</a>\u001b[0m )\n\u001b[0;32m---> <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=32'>33</a>\u001b[0m new_y \u001b[39m=\u001b[39m objective_env(new_x)\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=34'>35</a>\u001b[0m \u001b[39m### Update training points.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/vanillagp.py?line=35'>36</a>\u001b[0m train_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([model\u001b[39m.\u001b[39mtrain_inputs[\u001b[39m0\u001b[39m], new_x])\n",
      "File \u001b[0;32m~/Desktop/explo/src/environment.py:68\u001b[0m, in \u001b[0;36mEnvironmentObjective.__call__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     <a href='file:///home/q123/Desktop/explo/src/environment.py?line=66'>67</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, params: torch\u001b[39m.\u001b[39mTensor) :\n\u001b[0;32m---> <a href='file:///home/q123/Desktop/explo/src/environment.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(params)\n",
      "File \u001b[0;32m~/Desktop/explo/src/environment.py:121\u001b[0m, in \u001b[0;36mEnvironmentObjective.run\u001b[0;34m(self, params, render, test)\u001b[0m\n\u001b[1;32m    <a href='file:///home/q123/Desktop/explo/src/environment.py?line=118'>119</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps):  \u001b[39m# rollout\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/q123/Desktop/explo/src/environment.py?line=119'>120</a>\u001b[0m     actions[t] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy(states[t], params)\n\u001b[0;32m--> <a href='file:///home/q123/Desktop/explo/src/environment.py?line=120'>121</a>\u001b[0m     state, rewards[t], done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(actions[t]\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m    <a href='file:///home/q123/Desktop/explo/src/environment.py?line=121'>122</a>\u001b[0m     states[t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanipulate_state(state)\n\u001b[1;32m    <a href='file:///home/q123/Desktop/explo/src/environment.py?line=122'>123</a>\u001b[0m     r \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanipulate_reward(\n\u001b[1;32m    <a href='file:///home/q123/Desktop/explo/src/environment.py?line=123'>124</a>\u001b[0m         rewards[t], actions[t], states[t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], done\n\u001b[1;32m    <a href='file:///home/q123/Desktop/explo/src/environment.py?line=124'>125</a>\u001b[0m     )  \u001b[39m# Define as stochastic gradient ascent.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=16'>17</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=17'>18</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/time_limit.py?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=11'>12</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=12'>13</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py:162\u001b[0m, in \u001b[0;36mSwimmerEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=157'>158</a>\u001b[0m forward_reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_reward_weight \u001b[39m*\u001b[39m x_velocity\n\u001b[1;32m    <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=159'>160</a>\u001b[0m ctrl_cost \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol_cost(action)\n\u001b[0;32m--> <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=161'>162</a>\u001b[0m observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_obs()\n\u001b[1;32m    <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=162'>163</a>\u001b[0m reward \u001b[39m=\u001b[39m forward_reward \u001b[39m-\u001b[39m ctrl_cost\n\u001b[1;32m    <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=163'>164</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py:185\u001b[0m, in \u001b[0;36mSwimmerEnv._get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=181'>182</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclude_current_positions_from_observation:\n\u001b[1;32m    <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=182'>183</a>\u001b[0m     position \u001b[39m=\u001b[39m position[\u001b[39m2\u001b[39m:]\n\u001b[0;32m--> <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=184'>185</a>\u001b[0m observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate([position, velocity])\u001b[39m.\u001b[39mravel()\n\u001b[1;32m    <a href='file:///home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/gym/envs/mujoco/swimmer_v3.py?line=185'>186</a>\u001b[0m \u001b[39mreturn\u001b[39;00m observation\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### initialize policy\n",
    "mlp = MLP(*[8,2])\n",
    "\n",
    "# Initialize environment\n",
    "\n",
    "objective_env = EnvironmentObjective(\n",
    "  env=gym.make(\"Swimmer-v3\"),\n",
    "  policy=mlp,\n",
    "  manipulate_state=None,\n",
    "  manipulate_reward=None,\n",
    ")\n",
    "\n",
    "### initialize train_x, train_y\n",
    "train_x = torch.rand(1,mlp.len_params) ## [n_trials,n_params]\n",
    "train_y = [objective_env.run(p) for p in train_x]\n",
    "train_y = torch.Tensor(train_y).reshape(-1,1)  ## [n_trials,1]\n",
    "\n",
    "### initialize model\n",
    "model = SingleTaskGP(train_X=train_x, train_Y=train_y)\n",
    "\n",
    "### now we loop :\n",
    "max_iter = 200\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "  step(model,objective_env)\n",
    "\n",
    "  if i % 10 == 0:\n",
    "\n",
    "    best_val = model.train_targets.max()\n",
    "    curr_val = model.train_targets[-1]\n",
    "    print(f'curr {curr_val} max {best_val}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
