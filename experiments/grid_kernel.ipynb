{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo\n",
    "\n",
    "### local imports \n",
    "from src.environment import EnvironmentObjective\n",
    "from src.optim import step\n",
    "from src.policy import MLP\n",
    "\n",
    "### botorch\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "\n",
    "### gpytorch \n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.kernels import RBFKernel,ScaleKernel,Kernel\n",
    "from gpytorch.models import ExactGP\n",
    "\n",
    "### general imports\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "### Logging \n",
    "import logging\n",
    "logger = logging.getLogger('__main__')\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Toy kernel for warningging\n",
    "\n",
    "class MyKernel(gpytorch.kernels.RBFKernel):\n",
    "   \n",
    "    def forward(self,x1,x2,**params):\n",
    "        \n",
    "        logger.warning(f'x1 {x1.shape} / x2 {x2.shape}')\n",
    "        kernel = super().forward(x1,x2,**params)\n",
    "        logger.warning(f'pair kernel {kernel.shape}')\n",
    "        return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGP(ExactGP,GPyTorchModel):\n",
    "    \n",
    "    _num_outputs = 1\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_x, train_y,train_s, likelihood,\n",
    "                 kernel=None,mlp=None):\n",
    "        \n",
    "        ExactGP.__init__(self,train_x, train_y, likelihood)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        if kernel is None:\n",
    "            self.covar_module = MyKernel()\n",
    "        else :\n",
    "            self.covar_module = kernel(mlp,train_s)\n",
    "            \n",
    "            \n",
    "        ### necessary attribute for gpytorch to function\n",
    "        #self.num_outputs = 1\n",
    "    \n",
    "    def update_train_data(self,new_x, new_y,new_s,strict=False):\n",
    "        \n",
    "        train_x = torch.cat([self.train_inputs[0], new_x])\n",
    "        train_y = torch.cat([self.train_targets, new_y])\n",
    "        ExactGP.set_train_data(self,inputs=train_x,targets=train_y,strict=strict)\n",
    "        \n",
    "        \n",
    "        ### update state kernels with new states\n",
    "        if isinstance(self.covar_module,StateKernel):\n",
    "            self.covar_module.update(new_s)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateKernel(gpytorch.kernels.Kernel):\n",
    "    \n",
    "    \"\"\"Abstract class for a kernel that uses state action pairs metric\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,mlp,train_s):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.update_states(train_s)\n",
    "        self.mlp = mlp\n",
    "        ### set rbf_module with appropriate ard dims\n",
    "        num_states = 200\n",
    "        self.rbf_module = ScaleKernel(RBFKernel())\n",
    "        \n",
    "\n",
    "    def test_policy(self,params_batch,states):\n",
    "        \n",
    "        logger.warning('mlp :params_batch.shape{params_batch.shape}')\n",
    "        actions = self.mlp(params_batch,states)\n",
    "        logger.warning('mlp :actions.shape{actions.shape}')\n",
    "        # first_dims = params_batch.shape[:-1]\n",
    "        # last_dims = actions.shape[-2:]\n",
    "        # actions = actions.reshape(*first_dims,*last_dims)\n",
    "        actions = torch.flatten(actions,start_dim=-2)\n",
    "        logger.warning('reshape :actions.shape{actions.shape}')\n",
    "        return actions\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self,x1,x2,**params):\n",
    "        \n",
    "        logger.warning(f'x1 {x1.shape} / x2 {x2.shape}')\n",
    "        \n",
    "        #Evaluate current parameters\n",
    "        actions1 = self.test_policy(x1,self.states)\n",
    "        actions2 = self.test_policy(x2,self.states)\n",
    "        logger.warning(f'actions1 {actions1.shape} actions2 {actions2.shape} ')\n",
    "        # Compute pairwise pairwise kernel \n",
    "        kernel = self.rbf_module(actions1, actions2, **params)\n",
    "        logger.warning(f'pair kernel {kernel.shape}')\n",
    "        \n",
    "        return kernel \n",
    "    \n",
    "    def update(self,new_s):\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "        \n",
    "class GridKernel(StateKernel):\n",
    "    \n",
    "    \n",
    "    def get_grid(self,low,high,samples_per_dim):\n",
    "        \n",
    "        \n",
    "        state_dims = low.shape[0]\n",
    "        points = [torch.linspace(low[i],high[i],samples_per_dim) \n",
    "                    for i in range(state_dims)]\n",
    "        grid = torch.meshgrid(*points)\n",
    "        grid = torch.stack(grid)\n",
    "        grid = torch.flatten(grid,start_dim=1).T ## [n_states,state_dim]\n",
    "        \n",
    "        logger.warning(f' grid shape {grid.shape}')\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def update_states(self,new_s):\n",
    "        \n",
    "        self.high,_= torch.max(new_s,dim=0)\n",
    "        self.low,_= torch.min(new_s,dim=0)\n",
    "        self.states = self.get_grid(self.low,self.high,\n",
    "                                    samples_per_dim=5)\n",
    "        \n",
    "        #print(f'observation box : \\n low {self.low} \\n high :{self.high} \\n grid shape {self.states.shape}')\n",
    "    \n",
    "    def update(self,new_s):\n",
    "        \n",
    "        \n",
    "        tmp_buff = torch.cat([self.states, new_s])\n",
    "        high,_= torch.max(tmp_buff,dim=0)\n",
    "        low,_= torch.min(tmp_buff,dim=0)\n",
    "        \n",
    "        logger.warning(f'BUffer shape {tmp_buff.shape}')\n",
    "        \n",
    "        ### update only if be\n",
    "        if any(high>self.high) or any(low<self.low):\n",
    "            self.update_states(tmp_buff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pendulum\n",
    "mlp = MLP([3,1],add_bias=True) ## pendulum\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "### Swimmer \n",
    "#mlp = MLP([8,2],add_bias=True) ##swimmer\n",
    "#env = gym.make(\"Swimmer-v3\")\n",
    "\n",
    "### Inverted pendulum\n",
    "\n",
    "# mlp = MLP([4,1],add_bias=True) ##swimmer\n",
    "# env = gym.make(\"InvertedPendulum-v2\")\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "\n",
    "objective_env = EnvironmentObjective(\n",
    "  env=env,\n",
    "  mlp=mlp,\n",
    "  manipulate_state=None,\n",
    "  manipulate_reward=None,\n",
    ")\n",
    "\n",
    "### initialize train_x, train_y\n",
    "train_x = torch.rand(100,mlp.len_params) ## [n_trials,n_params]\n",
    "train_data = [objective_env.run(p) for p in train_x]\n",
    "train_y = torch.Tensor([d[0] for d in train_data]).reshape(-1)  ## [n_trials,1]\n",
    "train_s = torch.stack( [d[1] for d in train_data])  ## [n_trials,max_len,state_dim]\n",
    "train_s = torch.flatten(train_s,start_dim=0,end_dim=1) ## [n_trials*max_len,state_dim]\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = MyGP(train_x, train_y,train_s,likelihood,\n",
    "                    kernel=GridKernel,mlp=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "likelihood.noise_covar.raw_noise tensor([0.])\n",
      "mean_module.constant tensor([0.])\n",
      "covar_module.rbf_module.raw_outputscale tensor(0.)\n",
      "covar_module.rbf_module.base_kernel.raw_lengthscale tensor([[0.]])\n",
      "##############################\n",
      "##############################\n",
      "likelihood.noise_covar.raw_noise tensor([6169.4604])\n",
      "mean_module.constant tensor([-1564.2587])\n",
      "covar_module.rbf_module.raw_outputscale tensor(3972.7710)\n",
      "covar_module.rbf_module.base_kernel.raw_lengthscale tensor([[-39.9550]])\n",
      "##############################\n",
      "current -1671.0499267578125 / max -1040.72265625 /batch_mean -1548.0745849609375 /batch_max -1548.0745849609375 \n",
      "##############################\n",
      "likelihood.noise_covar.raw_noise tensor([11078.7637])\n",
      "mean_module.constant tensor([-1562.0969])\n",
      "covar_module.rbf_module.raw_outputscale tensor(8882.0742)\n",
      "covar_module.rbf_module.base_kernel.raw_lengthscale tensor([[-39.9550]])\n",
      "##############################\n",
      "current -1087.8255615234375 / max -1040.72265625 /batch_mean -1567.0399169921875 /batch_max -1567.0399169921875 \n",
      "##############################\n",
      "likelihood.noise_covar.raw_noise tensor([11078.7607])\n",
      "mean_module.constant tensor([-1563.0969])\n",
      "covar_module.rbf_module.raw_outputscale tensor(8882.0713)\n",
      "covar_module.rbf_module.base_kernel.raw_lengthscale tensor([[-39.9550]])\n",
      "##############################\n",
      "current -1513.2918701171875 / max -1040.72265625 /batch_mean -1568.8319091796875 /batch_max -1568.8319091796875 \n",
      "##############################\n",
      "likelihood.noise_covar.raw_noise tensor([11078.8535])\n",
      "mean_module.constant tensor([-1558.5253])\n",
      "covar_module.rbf_module.raw_outputscale tensor(8882.1641)\n",
      "covar_module.rbf_module.base_kernel.raw_lengthscale tensor([[-39.9550]])\n",
      "##############################\n",
      "current -1596.2222900390625 / max -1040.72265625 /batch_mean -1881.4676513671875 /batch_max -1881.4676513671875 \n"
     ]
    }
   ],
   "source": [
    "### now we loop :\n",
    "max_iter = 5\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "  step(model,objective_env)\n",
    "\n",
    "  if i % 1 == 0 and i>=1:\n",
    "  \n",
    "\n",
    "    max = model.train_targets.max()\n",
    "    batch_mean = model.train_targets[i-1:i].mean()\n",
    "    batch_max = model.train_targets[i-1:i].max()\n",
    "    curr = model.train_targets[-1]\n",
    "    print(f'current {curr} / max {max} /batch_mean {batch_mean} /batch_max {batch_max} ')\n",
    "\n",
    "    #print(f'model.train_inputs.shape{model.train_inputs[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually fitting GP (maximizing likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_iter = 100 \n",
    "\n",
    "# # Find optimal model hyperparameters\n",
    "# model.train()\n",
    "# likelihood.train()\n",
    "\n",
    "# # Use the adam optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.25)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# # \"Loss\" for GPs - the marginal log likelihood\n",
    "# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# for i in range(3):\n",
    "#     # Zero gradients from previous iteration\n",
    "#     optimizer.zero_grad()\n",
    "#     # Output from model\n",
    "#     output = model(train_x)\n",
    "#     # Calc loss and backprop gradients\n",
    "#     loss = -mll(output, train_y)\n",
    "#     logger.warning(f'Loss {loss.shape}')\n",
    "#     loss.backward()\n",
    "#     print('Iter %d/%d - Loss: %.3f noise: %.3f' % \n",
    "#         (\n",
    "#         i + 1, training_iter, loss.item(),\n",
    "#         model.likelihood.noise.item())\n",
    "#         )\n",
    "#     optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
