{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo\n",
    "import torch\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1494.0669, 1482.1525, 1446.1427, 1494.0669, 1482.1525, 1446.1427,\n",
       "           1494.0669, 1482.1525, 1446.1427, 1474.1210, 1474.1210, 1474.1210,\n",
       "           1474.1210, 1474.1210, 1474.1210, 1474.1210, 1474.1210, 1474.1210]]]),)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Linear,Sequential,Identity\n",
    "from typing import Tuple, List, Callable, Union, Optional\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "class MyMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "                self,\n",
    "                params,\n",
    "                Ls: List[int],\n",
    "                add_bias: bool = False,\n",
    "                nonlinearity: Optional[Callable] = None,\n",
    "                ):\n",
    "        \n",
    "        \"\"\"Inits MLP with the provided weights \n",
    "        Note the MLP can support batches of weights \"\"\"\n",
    "        \n",
    "        super(MyMLP, self).__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.weight_sizes  = [(in_size,out_size)\n",
    "                                for in_size, out_size in zip(Ls[:-1], Ls[1:])]\n",
    "        self.len_params = sum(\n",
    "            [\n",
    "                (in_size + 1 * add_bias) * out_size\n",
    "                for in_size, out_size in zip(Ls[:-1], Ls[1:])\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def create_weights(self,params):\n",
    "        \n",
    "        weights = []\n",
    "        start,end = (0,0)\n",
    "        \n",
    "        for (in_size,out_size) in self.weight_sizes:\n",
    "            \n",
    "            start = end\n",
    "            end   = start  + (in_size * out_size)\n",
    "            end   = start + in_size * out_size        \n",
    "            \n",
    "            weight = params[...,start:end].reshape(out_size,in_size)\n",
    "            weights.append(weight.T) ## add transpose or dim error\n",
    "            \n",
    "        return weights\n",
    "        \n",
    "        \n",
    "    def forward(self,params):\n",
    "\n",
    "        weights = self.create_weights(self.params)\n",
    "        \n",
    "        output = states\n",
    "        for w in weights:\n",
    "            output = output @ w\n",
    "        \n",
    "        return output\n",
    "\n",
    "params =torch.ones(1,1,18,requires_grad=True)\n",
    "\n",
    "mlp = MyMLP(params,[3,3,3])\n",
    "states = torch.rand(1000,3)\n",
    "outputs = torch.sum(mlp(states))\n",
    "print(mlp(states).shape)\n",
    "\n",
    "gradient = torch.autograd.grad(outputs=outputs,inputs=params)\n",
    "gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1458.7617, 1469.2220, 1530.4148, 1458.7617, 1469.2220, 1530.4148,\n",
       "           1458.7617, 1469.2220, 1530.4148, 1486.1328, 1486.1328, 1486.1328,\n",
       "           1486.1328, 1486.1328, 1486.1328, 1486.1328, 1486.1328, 1486.1328]]]),)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Linear,Sequential,Identity\n",
    "from typing import Tuple, List, Callable, Union, Optional\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "class MyMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "                self,\n",
    "                params,\n",
    "                Ls: List[int],\n",
    "                add_bias: bool = False,\n",
    "                nonlinearity: Optional[Callable] = None,\n",
    "                ):\n",
    "        \n",
    "        \"\"\"Inits MLP with the provided weights \n",
    "        Note the MLP can support batches of weights \"\"\"\n",
    "        \n",
    "        super(MyMLP, self).__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.weight_sizes  = [(in_size,out_size)\n",
    "                                for in_size, out_size in zip(Ls[:-1], Ls[1:])]\n",
    "        self.len_params = sum(\n",
    "            [\n",
    "                (in_size + 1 * add_bias) * out_size\n",
    "                for in_size, out_size in zip(Ls[:-1], Ls[1:])\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def create_weights(self,params):\n",
    "        \n",
    "        weights = []\n",
    "        start,end = (0,0)\n",
    "        \n",
    "        for (in_size,out_size) in self.weight_sizes:\n",
    "            \n",
    "            start = end\n",
    "            end   = start  + (in_size * out_size)\n",
    "            end   = start + in_size * out_size        \n",
    "            \n",
    "            weight = params[...,start:end].reshape(out_size,in_size)\n",
    "            weights.append(weight.T) ## add transpose or dim error\n",
    "            \n",
    "        return weights\n",
    "        \n",
    "        \n",
    "    def forward(self,params,states):\n",
    "\n",
    "        weights = self.create_weights(self.params)\n",
    "        \n",
    "        output = states\n",
    "        for w in weights:\n",
    "            output = output @ w\n",
    "        \n",
    "        return output\n",
    "\n",
    "params =torch.ones(1,1,18,requires_grad=True)\n",
    "\n",
    "mlp = MyMLP(params,[3,3,3])\n",
    "states = torch.rand(1000,3)\n",
    "outputs = torch.sum(mlp(params,states))\n",
    "\n",
    "######Â²\n",
    "gradient = torch.autograd.grad(outputs=outputs,inputs=params)\n",
    "gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.nn.Parameter(torch.ones(16,requires_grad=True))\n",
    "states = torch.rand(10,8)\n",
    "mlp = MLP(test_x,[8,2])\n",
    "# for p in mlp.parameters():\n",
    "#     print(p)\n",
    "a = torch.sum(mlp(states))\n",
    "gradient = torch.autograd.grad(outputs=a,inputs=test_x)\n",
    "gradient"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
