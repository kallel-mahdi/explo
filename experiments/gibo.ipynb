{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def one_step_cholesky(top_left, K_Xθ, K_θθ, A_inv) :\n",
    "    \n",
    "    '''Update the Cholesky factor when the matrix is extended.\n",
    "\n",
    "    Note: See thesis appendix A.2 for notation of args and further information.\n",
    "\n",
    "    Args:\n",
    "        top_left: Cholesky factor L11 of old matrix A11.\n",
    "        K_Xθ: Upper right bock matrix A12 of new matrix A.\n",
    "        K_θθ: Lower right block matrix A22 of new matrix A.\n",
    "        A_inv: Inverse of old matrix A11.\n",
    "\n",
    "    Returns:\n",
    "        New cholesky factor S of new matrix A.\n",
    "    '''\n",
    "    # Solve with A \\ b: A @ x = b, x = A^(-1) @ b,\n",
    "    # top_right = L11^T \\ A12 = L11^T  \\ K_Xθ, top_right = (L11^T)^(-1) @ K_Xθ,\n",
    "    # Use: (L11^(-1))^T = L11 @ A11^(-1).\n",
    "    # Hint: could also be solved with torch.cholesky_solve (in theory faster).\n",
    "    top_right = top_left @ (A_inv @ K_Xθ)\n",
    "    bot_left = torch.zeros_like(top_right).transpose(-1, -2)\n",
    "    bot_right = torch.cholesky(\n",
    "        K_θθ - top_right.transpose(-1, -2) @ top_right, upper=True\n",
    "    )\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.cat([top_left, top_right], dim=-1),\n",
    "            torch.cat([bot_left, bot_right], dim=-1),\n",
    "        ],\n",
    "        dim=-2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import botorch\n",
    "\n",
    "\n",
    "\n",
    "class GradientInformation(botorch.acquisition.AnalyticAcquisitionFunction):\n",
    "    '''Acquisition function to sample points for gradient information.\n",
    "\n",
    "    Attributes:\n",
    "        model: Gaussian process model that supplies the Jacobian (e.g. DerivativeExactGPSEModel).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model):\n",
    "        '''Inits acquisition function with model.'''\n",
    "        super().__init__(model)\n",
    "\n",
    "    def update_theta_i(self, theta_i: torch.Tensor):\n",
    "      \n",
    "        '''Updates the current parameters.\n",
    "\n",
    "        This leads to an update of K_xX_dx.\n",
    "\n",
    "        Args:\n",
    "            theta_i: New parameters.\n",
    "        '''\n",
    "        if not torch.is_tensor(theta_i):\n",
    "            theta_i = torch.tensor(theta_i)\n",
    "        self.theta_i = theta_i\n",
    "        self.update_K_xX_dx()\n",
    "\n",
    "    def update_K_xX_dx(self):\n",
    "        '''When new x is given update K_xX_dx.'''\n",
    "        # Pre-compute large part of K_xX_dx.\n",
    "        X = self.model.train_inputs[0]\n",
    "        x = self.theta_i.view(-1, self.model.D)\n",
    "        self.K_xX_dx_part = self._get_KxX_dx(x, X)\n",
    "\n",
    "    def _get_KxX_dx(self, x, X) :\n",
    "        \n",
    "        \n",
    "        '''Computes the analytic derivative of the kernel K(x,X) w.r.t. x.\n",
    "\n",
    "        Args:\n",
    "            x: (n x D) Test points.\n",
    "\n",
    "        Returns:\n",
    "            (n x D) The derivative of K(x,X) w.r.t. x.\n",
    "        '''\n",
    "        N = X.shape[0]\n",
    "        n = x.shape[0]\n",
    "        K_xX = self.model.covar_module(x, X).evaluate()\n",
    "        lengthscale = self.model.covar_module.base_kernel.lengthscale.detach()\n",
    "        return (\n",
    "            -torch.eye(self.model.D, device=X.device)\n",
    "            / lengthscale\n",
    "            @ (\n",
    "                (x.view(n, 1, self.model.D) - X.view(1, N, self.model.D))\n",
    "                * K_xX.view(n, N, 1)\n",
    "            ).transpose(1, 2)\n",
    "        )\n",
    "\n",
    "    @botorch.utils.transforms.t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, thetas: torch.Tensor) :\n",
    "        \n",
    "        '''Evaluate the acquisition function on the candidate set thetas.\n",
    "\n",
    "        Args:\n",
    "            thetas: A (b) x D-dim Tensor of (b) batches with a d-dim theta points each.\n",
    "\n",
    "        Returns:\n",
    "            A (b)-dim Tensor of acquisition function values at the given theta points.\n",
    "        '''\n",
    "        sigma_n = self.model.likelihood.noise_covar.noise\n",
    "        D = self.model.D\n",
    "        X = self.model.train_inputs[0]\n",
    "        x = self.theta_i.view(-1, D)\n",
    "\n",
    "        variances = []\n",
    "        for theta in thetas:\n",
    "            theta = theta.view(-1, D)\n",
    "            # Compute K_Xθ, K_θθ (do not forget to add noise).\n",
    "            K_Xθ = self.model.covar_module(X, theta).evaluate()\n",
    "            K_θθ = self.model.covar_module(theta).evaluate() + sigma_n * torch.eye(\n",
    "                K_Xθ.shape[-1]\n",
    "            ).to(theta)\n",
    "\n",
    "            # Get Cholesky factor.\n",
    "            L = one_step_cholesky(\n",
    "                top_left=self.model.get_L_lower().transpose(-1, -2),\n",
    "                K_Xθ=K_Xθ,\n",
    "                K_θθ=K_θθ,\n",
    "                A_inv=self.model.get_KXX_inv(),\n",
    "            )\n",
    "\n",
    "            # Get K_XX_inv.\n",
    "            K_XX_inv = torch.cholesky_inverse(L, upper=True)\n",
    "\n",
    "            # get K_xX_dx\n",
    "            K_xθ_dx = self._get_KxX_dx(x, theta)\n",
    "            K_xX_dx = torch.cat([self.K_xX_dx_part, K_xθ_dx], dim=-1)\n",
    "\n",
    "            # Compute_variance.\n",
    "            variance_d = -K_xX_dx @ K_XX_inv @ K_xX_dx.transpose(1, 2)\n",
    "            variances.append(torch.trace(variance_d.view(D, D)).view(1))\n",
    "\n",
    "        return -torch.cat(variances, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import botorch\n",
    "\n",
    "\n",
    "class ExactGPSEModel(gpytorch.models.ExactGP, botorch.models.gpytorch.GPyTorchModel):\n",
    "   \n",
    "\n",
    "    _num_outputs = 1  # To inform GPyTorchModel API.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x: torch.Tensor,\n",
    "        train_y: torch.Tensor,\n",
    "        lengthscale_constraint=None,\n",
    "        lengthscale_hyperprior=None,\n",
    "        outputscale_constraint=None,\n",
    "        outputscale_hyperprior=None,\n",
    "        noise_constraint=None,\n",
    "        noise_hyperprior=None,\n",
    "        ard_num_dims=None,\n",
    "        prior_mean=0,\n",
    "    ):\n",
    "        \"\"\"Inits GP model with data and a Gaussian likelihood.\"\"\"\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "            noise_constraint=noise_constraint, noise_prior=noise_hyperprior\n",
    "        )\n",
    "        if train_y is not None:\n",
    "            train_y = train_y.squeeze(-1)\n",
    "        super(ExactGPSEModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        if prior_mean != 0:\n",
    "            self.mean_module.initialize(constant=prior_mean)\n",
    "            self.mean_module.constant.requires_grad = False\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=ard_num_dims,\n",
    "                lengthscale_prior=lengthscale_hyperprior,\n",
    "                lengthscale_constraint=lengthscale_constraint,\n",
    "            ),\n",
    "            outputscale_prior=outputscale_hyperprior,\n",
    "            outputscale_constraint=outputscale_constraint,\n",
    "        )\n",
    "        # Initialize lengthscale and outputscale to mean of priors.\n",
    "        if lengthscale_hyperprior is not None:\n",
    "            self.covar_module.base_kernel.lengthscale = lengthscale_hyperprior.mean\n",
    "        if outputscale_hyperprior is not None:\n",
    "            self.covar_module.outputscale = outputscale_hyperprior.mean\n",
    "\n",
    "        self.D = train_x.shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "    \n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class DerivativeExactGPSEModel(ExactGPSEModel):\n",
    "\n",
    "\n",
    "    def __init__(self,train_x,train_y):\n",
    "       \n",
    "        \"\"\"Inits GP model with data and a Gaussian likelihood.\"\"\"\n",
    "\n",
    "        \n",
    "        super(DerivativeExactGPSEModel, self).__init__(train_x,train_y)\n",
    "        self.D = train_x.shape[1]\n",
    "        self.N = train_x.shape[0]\n",
    "        self.N_max = 1e5\n",
    "\n",
    "    def append_train_data(self,new_x,new_y):\n",
    "\n",
    "          \n",
    "          train_x = torch.cat([self.train_inputs[0], new_x.reshape(1,-1)])\n",
    "          train_y = torch.cat([self.train_targets, new_y])\n",
    "          self.set_train_data(inputs=train_x, targets=train_y, strict=False)\n",
    "          self.N = train_x.shape[0]\n",
    "\n",
    "    def get_L_lower(self):\n",
    "        \"\"\"Get Cholesky decomposition L, where L is a lower triangular matrix.\n",
    "\n",
    "        Returns:\n",
    "            Cholesky decomposition L.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.prediction_strategy.lik_train_train_covar.root_decomposition()\n",
    "            .root.evaluate()\n",
    "            .detach()\n",
    "        )\n",
    "\n",
    "    def get_KXX_inv(self):\n",
    "        \"\"\"Get the inverse matrix of K(X,X).\n",
    "\n",
    "        Returns:\n",
    "            The inverse of K(X,X).\n",
    "        \"\"\"\n",
    "        L_inv_upper = self.prediction_strategy.covar_cache.detach()\n",
    "        return L_inv_upper @ L_inv_upper.transpose(0, 1)\n",
    "\n",
    "    def get_KXX_inv_old(self):\n",
    "        \"\"\"Get the inverse matrix of K(X,X).\n",
    "\n",
    "        Not as efficient as get_KXX_inv.\n",
    "\n",
    "        Returns:\n",
    "            The inverse of K(X,X).\n",
    "        \"\"\"\n",
    "        X = self.train_inputs[0]\n",
    "        sigma_n = self.likelihood.noise_covar.noise.detach()\n",
    "        return torch.inverse(\n",
    "            self.covar_module(X).evaluate() + torch.eye(X.shape[0]) * sigma_n\n",
    "        )\n",
    "\n",
    "    def _get_KxX_dx(self, x):\n",
    "        \"\"\"Computes the analytic derivative of the kernel K(x,X) w.r.t. x.\n",
    "\n",
    "        Args:\n",
    "            x: (n x D) Test points.\n",
    "\n",
    "        Returns:\n",
    "            (n x D) The derivative of K(x,X) w.r.t. x.\n",
    "        \"\"\"\n",
    "        X = self.train_inputs[0]\n",
    "        n = x.shape[0]\n",
    "        K_xX = self.covar_module(x, X).evaluate()\n",
    "        lengthscale = self.covar_module.base_kernel.lengthscale.detach()\n",
    "        return (\n",
    "            -torch.eye(self.D, device=x.device)\n",
    "            / lengthscale ** 2\n",
    "            @ (\n",
    "                (x.view(n, 1, self.D) - X.view(1, self.N, self.D))\n",
    "                * K_xX.view(n, self.N, 1)\n",
    "            ).transpose(1, 2)\n",
    "        )\n",
    "\n",
    "    def _get_Kxx_dx2(self):\n",
    "        \"\"\"Computes the analytic second derivative of the kernel K(x,x) w.r.t. x.\n",
    "\n",
    "        Args:\n",
    "            x: (n x D) Test points.\n",
    "\n",
    "        Returns:\n",
    "            (n x D x D) The second derivative of K(x,x) w.r.t. x.\n",
    "        \"\"\"\n",
    "        lengthscale = self.covar_module.base_kernel.lengthscale.detach()\n",
    "        sigma_f = self.covar_module.outputscale.detach()\n",
    "        return (\n",
    "            torch.eye(self.D, device=lengthscale.device) / lengthscale ** 2\n",
    "        ) * sigma_f\n",
    "\n",
    "    def posterior_derivative(self, x):\n",
    "        \"\"\"Computes the posterior of the derivative of the GP w.r.t. the given test\n",
    "        points x.\n",
    "\n",
    "        Args:\n",
    "            x: (n x D) Test points.\n",
    "\n",
    "        Returns:\n",
    "            A GPyTorchPosterior.\n",
    "        \"\"\"\n",
    "        if self.prediction_strategy is None:\n",
    "            self.posterior(x)  # Call this to update prediction strategy of GPyTorch.\n",
    "        K_xX_dx = self._get_KxX_dx(x)\n",
    "        mean_d = K_xX_dx @ self.get_KXX_inv() @ self.train_targets\n",
    "        variance_d = (\n",
    "            self._get_Kxx_dx2() - K_xX_dx @ self.get_KXX_inv() @ K_xX_dx.transpose(1, 2)\n",
    "        )\n",
    "        variance_d = variance_d.clamp_min(1e-9)\n",
    "\n",
    "        return mean_d, variance_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_acqf_custom_bo(\n",
    "    acq_func: botorch.acquisition.AcquisitionFunction,\n",
    "    bounds: torch.Tensor    \n",
    "):\n",
    "    '''Function to optimize the GradientInformation acquisition function for custom Bayesian optimization.'''\n",
    "\n",
    " \n",
    "    candidates, acq_value = botorch.optim.optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=1,  # Analytic acquisition function.\n",
    "        num_restarts=20,\n",
    "        raw_samples=100,\n",
    "        options={'nonnegative': True, 'batch_limit': 5},\n",
    "        return_best_only=True,\n",
    "        sequential=False,\n",
    "    )\n",
    "    # Observe new values.\n",
    "    new_x = candidates.detach()\n",
    "    return new_x, acq_value\n",
    "\n",
    "\n",
    "class GIBO():\n",
    "    \n",
    "    def __init__(self,model,\n",
    "                 verbose= False,\n",
    "                 normalize_gradient=True,standard_deviation_scaling=False):\n",
    "\n",
    "      max_samples_per_iteration = 3\n",
    "      delta = 0.1\n",
    "      acquisition_fcn = GradientInformation(model)\n",
    "      optimize_acqf =  optimize_acqf_custom_bo\n",
    "      optimizer_torch = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "      params_tmp = model.train_inputs[0][-1]\n",
    "      params_history_list = [params_tmp.clone()]\n",
    "      D = len_params\n",
    "      self.__dict__.update(locals())\n",
    "\n",
    "    def step(self,model,objective_env):\n",
    "\n",
    "        params = self.params_history_list[-1].reshape(1,-1)\n",
    "        self.params = params\n",
    "        model.posterior(params)  ## hotfix\n",
    " \n",
    "        # Evaluate current parameters\n",
    "        f_params = objective_env(params)\n",
    "        # Update training points\n",
    "        model.append_train_data(params, f_params)\n",
    "        self.acquisition_fcn.update_theta_i(params)\n",
    "        \n",
    "        # Stay local around current parameters.\n",
    "        \n",
    "        bounds = torch.tensor([[-self.delta], [self.delta]]) + params\n",
    "        \n",
    "        # Only optimize model hyperparameters if N >= N_max.\n",
    "        if (model.N >= model.N_max): \n",
    "\n",
    "            # Adjust hyperparameters\n",
    "            mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "            fit_gpytorch_model(mll)\n",
    "            model.posterior(params)  ## hotfix\n",
    "\n",
    "        \n",
    "        ## Optimize gradient information locally\n",
    "        for i in range(self.max_samples_per_iteration):\n",
    "\n",
    "            model.posterior(params)  ## hotfix\n",
    "\n",
    "\n",
    "            # Optimize acquistion function and get new observation.\n",
    "            new_x, acq_value = self.optimize_acqf(self.acquisition_fcn, bounds)            \n",
    "            new_y = objective_env(new_x)\n",
    "            # Update training points.\n",
    "            self.model.append_train_data(new_x, new_y)\n",
    "\n",
    "            model.posterior(params)  ## hotfix\n",
    "            self.acquisition_fcn.update_K_xX_dx()\n",
    "\n",
    "        ## compute gradients manually\n",
    "        with torch.no_grad():\n",
    "          \n",
    "            self.optimizer_torch.zero_grad()\n",
    "            mean_d, variance_d = model.posterior_derivative(params)\n",
    "            params_grad = -mean_d.view(1, self.D)\n",
    "\n",
    "            if self.normalize_gradient:\n",
    "                lengthscale = model.covar_module.base_kernel.lengthscale.detach()\n",
    "                params_grad = torch.nn.functional.normalize(params_grad) * lengthscale\n",
    "\n",
    "            if self.standard_deviation_scaling:\n",
    "                params_grad = params_grad / torch.diag(variance_d.view(self.D, self.D))\n",
    "\n",
    "            params.grad = params_grad  # set gradients\n",
    "            self.optimizer_torch.step()      \n",
    "            self.params_history_list.append(self.params.clone())\n",
    "\n",
    "        # if self.verbose:\n",
    "        #     posterior = model.posterior(self.params)\n",
    "        #     print(\n",
    "        #         f\"theta_t:  predicted mean {posterior.mvn.mean.item(): .2f} / variance {posterior.mvn.variance.item(): .2f}.\"\n",
    "        #     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'len_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/q123/Desktop/explo/experiments/gibo.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/gibo.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39m### initialize train_x, train_y\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/gibo.ipynb#ch0000004?line=2'>3</a>\u001b[0m train_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m,len_params) \u001b[39m## [n_trials,n_params]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/gibo.ipynb#ch0000004?line=3'>4</a>\u001b[0m train_y \u001b[39m=\u001b[39m [objective_env\u001b[39m.\u001b[39mrun(p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m train_x]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/q123/Desktop/explo/experiments/gibo.ipynb#ch0000004?line=4'>5</a>\u001b[0m train_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(train_y)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)  \u001b[39m## [n_trials,1]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'len_params' is not defined"
     ]
    }
   ],
   "source": [
    "### initialize train_x, train_y\n",
    "\n",
    "train_x = torch.rand(3,len_params) ## [n_trials,n_params]\n",
    "train_y = [objective_env.run(p) for p in train_x]\n",
    "train_y = torch.Tensor(train_y).reshape(-1,1)  ## [n_trials,1]\n",
    "\n",
    "### initialize model\n",
    "\n",
    "model = DerivativeExactGPSEModel(train_x, train_y)\n",
    "optimizer = GIBO(model)\n",
    "\n",
    "### now we loop :\n",
    "max_iter = 100\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "  optimizer.step(model,objective_env)\n",
    "  \n",
    "  if i % 5 == 0:\n",
    "\n",
    "    best_val = model.train_targets.max()\n",
    "    curr_val = model.train_targets[-1]    \n",
    "    print(f'curr {curr_val} max {best_val}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor([0])\n",
    "a = torch.Tensor([1,2,3])\n",
    "torch.cat((a,b))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
