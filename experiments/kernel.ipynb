{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/q123/Desktop/explo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q123/miniconda3/envs/explo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd /home/q123/Desktop/explo\n",
    "\n",
    "### local imports \n",
    "from src.environment import EnvironmentObjective\n",
    "from src.vanillagp import step\n",
    "from src.policy import MLP\n",
    "\n",
    "### botorch\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "### general imports\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "### Logging \n",
    "import logging\n",
    "logger = logging.getLogger('output shapes')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Toy kernel for debugging\n",
    "\n",
    "class MyKernel(gpytorch.kernels.RBFKernel):\n",
    "   \n",
    "    def forward(self,x1,x2,**params):\n",
    "        \n",
    "        logger.debug(f'x1 {x1.shape}')\n",
    "        kernel = super().forward(x1,x2,**params)\n",
    "        logger.debug(f'pair kernel {kernel.shape}')\n",
    "        return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridKernel(gpytorch.kernels.Kernel):\n",
    "    \n",
    "    def __init__(self,mlp,actions_metric,\n",
    "                 states,states_w=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if states_w is None:\n",
    "            states_w = torch.ones(states.size(0))\n",
    "            \n",
    "        rbf_module =  gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "        ## save params to instance\n",
    "        self.__dict__.update(locals())\n",
    "    \n",
    "    def compute_actions(self,states,params_batch):\n",
    "        \n",
    "        rslt = [self.mlp(states,p).squeeze() \n",
    "                for p in params_batch.squeeze()]\n",
    "        \n",
    "        \n",
    "        #### WARNING THIS MIGHT BE A SOURCE OF ERROR\n",
    "        first_dims = params_batch.size()[:-1]\n",
    "        last_dim = rslt[0].size(-1)\n",
    "        rslt = torch.stack(rslt).reshape(*first_dims,last_dim) ## hotfix\n",
    "        ###############################################\"\"\"\"\n",
    "        \n",
    "        return rslt\n",
    "            \n",
    "\n",
    "    def forward(self,x1,x2,**params):\n",
    "        \n",
    "        logger.debug(f'x1 {x1.shape}')\n",
    "        states,states_w = self.states,self.states_w\n",
    "    \n",
    "        #Evaluate current parameters\n",
    "        actions1 = self.compute_actions(states,x1).squeeze()\n",
    "        actions2 = self.compute_actions(states,x2).squeeze()\n",
    "        logger.debug(f'actions size {actions1.shape} ')\n",
    "        \n",
    "        \n",
    "        # Compute pairwise pairwise kernel \n",
    "        #distances = self.covar_dist(actions1, actions2, **params)\n",
    "        kernel = self.rbf_module(actions1, actions2, **params)\n",
    "        logger.debug(f'pair kernel {kernel.shape}')\n",
    "        \n",
    "        return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class GridGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,\n",
    "                 mlp,actions_metric,states):\n",
    "        \n",
    "        \n",
    "        super(GridGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.covar_module = GridKernel(mlp,actions_metric,states)\n",
    "        #self.covar_module = MyKernel()\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize policy\n",
    "mlp = MLP(*[3,1])\n",
    "mlp.requires_grad = False\n",
    "\n",
    "# Initialize environment\n",
    "\n",
    "objective_env = EnvironmentObjective(\n",
    "  env=gym.make(\"Pendulum-v1\"),\n",
    "  policy=mlp,\n",
    "  manipulate_state=None,\n",
    "  manipulate_reward=None,\n",
    ")\n",
    "\n",
    "### initialize train_x, train_y\n",
    "train_x = torch.rand(100,mlp.len_params) ## [n_trials,n_params]\n",
    "train_y = [objective_env.run(p) for p in train_x]\n",
    "train_y = torch.Tensor(train_y).reshape(-1)  ## [n_trials,1]\n",
    "\n",
    "# initialize likelihood and model\n",
    "\n",
    "states = objective_env.get_grid()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GridGPModel(train_x, train_y, likelihood,\n",
    "                    mlp,torch.linalg.norm,states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 844349.688 noise: 0.693\n",
      "Iter 2/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 3/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 4/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 5/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 6/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 7/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 8/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 9/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 10/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 11/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 12/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 13/100 - Loss: 16.964 noise: 76119.500\n",
      "Iter 14/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 15/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 16/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 17/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 18/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 19/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 20/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 21/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 22/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 23/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 24/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 25/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 26/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 27/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 28/100 - Loss: 16.963 noise: 76119.500\n",
      "Iter 29/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 30/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 31/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 32/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 33/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 34/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 35/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 36/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 37/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 38/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 39/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 40/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 41/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 42/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 43/100 - Loss: 16.962 noise: 76119.500\n",
      "Iter 44/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 45/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 46/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 47/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 48/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 49/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 50/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 51/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 52/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 53/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 54/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 55/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 56/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 57/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 58/100 - Loss: 16.961 noise: 76119.500\n",
      "Iter 59/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 60/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 61/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 62/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 63/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 64/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 65/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 66/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 67/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 68/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 69/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 70/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 71/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 72/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 73/100 - Loss: 16.960 noise: 76119.500\n",
      "Iter 74/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 75/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 76/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 77/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 78/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 79/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 80/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 81/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 82/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 83/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 84/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 85/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 86/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 87/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 88/100 - Loss: 16.959 noise: 76119.500\n",
      "Iter 89/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 90/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 91/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 92/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 93/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 94/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 95/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 96/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 97/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 98/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 99/100 - Loss: 16.958 noise: 76119.500\n",
      "Iter 100/100 - Loss: 16.958 noise: 76119.500\n"
     ]
    }
   ],
   "source": [
    "training_iter = 100 \n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.25)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(100):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    logger.debug(f'Loss {loss.shape}')\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f noise: %.3f' % \n",
    "        (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.likelihood.noise.item())\n",
    "        )\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_grid = objective_env.get_grid()\n",
    "# kernel = GridKernel(mlp,torch.linalg.norm,states_grid)\n",
    "# kernel.forward(torch.rand(3),torch.rand(3))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b54cb4d83655428105eabb77a9cd1898504607119e0ebf088afaf3437f4d048"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('explo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
